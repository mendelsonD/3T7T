{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a75c12be",
   "metadata": {},
   "source": [
    "# Check that hemispheres are:\n",
    "# 1. not identical\n",
    "# 2. correclty assigned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96068824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import nibabel as nib\n",
    "os.chdir(\"/host/verges/tank/data/daniel/01_3T7T/z/code/analyses/\")\n",
    "import tTsTGrpUtils as tsutil\n",
    "import utils_plots as up\n",
    "import importlib\n",
    "import datetime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835fe68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin(n_vertices, n_bins):\n",
    "    import numpy as np\n",
    "    return np.array_split(np.arange(n_vertices), n_bins)\n",
    "\n",
    "def describeMapDif(x, y, logger, investigate = False, n_bins = 20, zeroVal = 1e-4):\n",
    "    \"\"\"\n",
    "    Describe the difference in df of same size (eg., maps with v vertices by n participants)\n",
    "    between two dataframes x and y (eg., left and right hemisphere maps).\n",
    "\n",
    "\n",
    "    investigate: bool\n",
    "        If True, then if the mean difference within any bin i more than 1 std from the overall mean, \n",
    "        then split this bin into sub-bins and print include those results\n",
    "    \"\"\"\n",
    "    # print summary stats\n",
    "    logger.info(\"\\tDifference L-R summary stats:\")\n",
    "    binned_stats = [] # init output\n",
    "\n",
    "    zeroVal_fmt = f\"{zeroVal:.0e}\"\n",
    "    subbins = max(1, n_bins // 4)\n",
    "    \n",
    "    d = x - y\n",
    "    d_vals = d.values # convert to array\n",
    "    all_flat = d_vals.ravel()    \n",
    "    n_above_all = int(np.count_nonzero(np.logical_and(~np.isnan(all_flat), all_flat > zeroVal)))\n",
    "    #  Summary stats across all vertices\n",
    "    all_stats = pd.Series({\n",
    "        'name': 'all',\n",
    "        'n_vertices': d_vals.shape[1],\n",
    "        'n_subjects': d_vals.shape[0],\n",
    "        'n_totVals': all_flat.size,\n",
    "        'mean': np.nanmean(all_flat),\n",
    "        'std': np.nanstd(all_flat),\n",
    "        'min': np.nanmin(all_flat),\n",
    "        '25%': np.nanpercentile(all_flat, 25),\n",
    "        '50%': np.nanpercentile(all_flat, 50),\n",
    "        '75%': np.nanpercentile(all_flat, 75),\n",
    "        'max': np.nanmax(all_flat),\n",
    "        f'n_above{zeroVal_fmt}': n_above_all,\n",
    "        f'%_above{zeroVal_fmt}': (n_above_all / all_flat.size) * 100\n",
    "    })\n",
    "\n",
    "    binned_stats.append(all_stats)\n",
    "    subbin_stats = []\n",
    "\n",
    "    n_subj, n_vertices = d.shape\n",
    "    idx_grps = bin(n_vertices = n_vertices, n_bins=n_bins)\n",
    "    for bin_idx, idxs in enumerate(idx_grps, start=1):\n",
    "        bin_values = d_vals[:, idxs]\n",
    "        flat_bin = bin_values.ravel()\n",
    "        n_above = int(np.count_nonzero(np.logical_and(~np.isnan(flat_bin), flat_bin > zeroVal)))\n",
    "        # Calculate stats directly with numpy (much faster)\n",
    "        bin_stats = pd.Series({\n",
    "            'name': f'bin_{bin_idx}',\n",
    "            'n_vertices': bin_values.shape[1],\n",
    "            'n_subjects': bin_values.shape[0],\n",
    "            'n_totVals': flat_bin.size,\n",
    "            'mean': np.nanmean(flat_bin),\n",
    "            'std': np.nanstd(flat_bin),\n",
    "            'min': np.nanmin(flat_bin),\n",
    "            '25%': np.nanpercentile(flat_bin, 25),\n",
    "            '50%': np.nanpercentile(flat_bin, 50),\n",
    "            '75%': np.nanpercentile(flat_bin, 75),\n",
    "            'max': np.nanmax(flat_bin),\n",
    "            f'n_above{zeroVal_fmt}': n_above,\n",
    "            f'%_above{zeroVal_fmt}': (n_above / flat_bin.size) * 100\n",
    "        })\n",
    "        binned_stats.append(bin_stats)\n",
    "\n",
    "        if investigate:\n",
    "            if bin_stats[f'%_above{zeroVal_fmt}'] >= 5:\n",
    "                # Further investigate this bin by splitting into sub-bins\n",
    "                idx_subgrps = bin(n_vertices = bin_values.shape[1], n_bins=subbins)\n",
    "                for subbin_idx, sub_idxs in enumerate(idx_subgrps, start=1):\n",
    "                    subbin_vals = bin_values[:,sub_idxs]\n",
    "                    flat_sub = subbin_vals.ravel()\n",
    "                    \n",
    "                    n_above_sub = int(np.count_nonzero(np.logical_and(~np.isnan(flat_sub), flat_sub > zeroVal)))\n",
    "                    count = subbin_vals.size\n",
    "\n",
    "                    if count == 0:\n",
    "                        sub_bin_stats = pd.Series({\n",
    "                            'name': f'bin_{bin_idx}-{subbin_idx}',\n",
    "                            'n_vertices': subbin_vals.shape[1],\n",
    "                            'n_subjects': subbin_vals.shape[0],\n",
    "                            'n_totVals': count,\n",
    "                            'mean': np.nan,\n",
    "                            'std': np.nan,\n",
    "                            'min': np.nan,\n",
    "                            '25%': np.nan,\n",
    "                            '50%': np.nan,\n",
    "                            '75%': np.nan,\n",
    "                            'max': np.nan,\n",
    "                            f'n_above{zeroVal_fmt}': count,\n",
    "                            f'%_above{zeroVal_fmt}': np.nan\n",
    "                        })\n",
    "                    else:\n",
    "                        sub_bin_stats = pd.Series({\n",
    "                            'name': f'bin_{bin_idx}-{subbin_idx}',\n",
    "                            'n_vertices': subbin_vals.shape[1],\n",
    "                            'n_subjects': subbin_vals.shape[0],\n",
    "                            'n_totVals': count,\n",
    "                            'mean': np.nanmean(flat_sub),\n",
    "                            'std': np.nanstd(flat_sub),\n",
    "                            'min': np.nanmin(flat_sub),\n",
    "                            '25%': np.nanpercentile(flat_sub, 25),\n",
    "                            '50%': np.nanpercentile(flat_sub, 50),\n",
    "                            '75%': np.nanpercentile(flat_sub, 75),\n",
    "                            'max': np.nanmax(flat_sub),\n",
    "                            f'n_above{zeroVal_fmt}': n_above_sub,\n",
    "                            f'%_above{zeroVal_fmt}': (n_above_sub / count) * 100\n",
    "                        })\n",
    "                    subbin_stats.append(sub_bin_stats)\n",
    "    \n",
    "    # Combine all bin statistics into a single DataFrame\n",
    "    binned_df = pd.concat(binned_stats, axis=1)\n",
    "    logger.info(binned_df.round(3).to_string())\n",
    "    \n",
    "    if investigate and subbin_stats:\n",
    "        logger.info(\"\\n\\tInvestigated Sub-bins stats:\")\n",
    "        subbin_df = pd.concat(subbin_stats, axis=1)\n",
    "        logger.info(subbin_df.round(3).to_string())\n",
    "\n",
    "def parse_map_colname(s):\n",
    "    \"\"\"\n",
    "    Return dict with keys: region, hemi, surf, label, feature, smoothing (may be None)\n",
    "    \"\"\"\n",
    "    import re\n",
    "    ft_wrong = False\n",
    "    # regex tuned for patterns like:\n",
    "    # ctx_hemi-R_surf-fsLR-32k_label-white_T1map_smth-10mm\n",
    "    # or zb_ctx_hemi-L_surf-fsLR-5k_label-pial_feature-T1map_smooth-5mm\n",
    "    \n",
    "    pat = re.compile(\n",
    "        r'^(?P<region>[^_]+)_hemi-(?P<hemi>[LR])_surf-(?P<surf>[^_]+)_label-(?P<label>[^_]+)'\n",
    "        r'(?:_(?:feature-)?(?P<feature>[^_]+))?(?:_(?:smth|smooth)-(?P<smoothing>\\d+)mm)?$'\n",
    "    )\n",
    "\n",
    "    m = pat.match(s)\n",
    "    if m:\n",
    "        \n",
    "        out = m.groupdict()\n",
    "        \n",
    "        if 'smth' in out['feature'] or 'smooth' in out['feature']:\n",
    "            ft_wrong = True\n",
    "            smth_kernel = out['feature'].split('-')[1]\n",
    "            if smth_kernel.endswith('mm'):\n",
    "                smth_kernel = smth_kernel.replace('mm','')\n",
    "            out['smoothing'] = smth_kernel\n",
    "            \n",
    "        # ensure feature extracted (sometimes appears before label or without \"feature-\")\n",
    "        if out['label'] == 'thickness':\n",
    "            out['feature'] = 'thickness'\n",
    "            out['label'] = 'midthickness'\n",
    "        elif out['feature'] is None or ft_wrong:\n",
    "            # try to get trailing token after label (e.g. ..._label-white_T1map_smth-10mm)\n",
    "            parts = s.split('_')\n",
    "            if len(parts) >= 5:\n",
    "                out['feature'] = parts[4].replace('feature-', '')\n",
    "        return out\n",
    "\n",
    "    # fallback: structured split (best-effort)\n",
    "    parts = s.split('_')\n",
    "    out = {'region': None, 'hemi': None, 'surf': None, 'label': None, 'feature': None, 'smoothing': None}\n",
    "    try:\n",
    "        out['region'] = parts[0]\n",
    "        out['hemi'] = parts[1].split('-', 1)[1]\n",
    "        out['surf'] = parts[2].split('-', 1)[1]\n",
    "        # label token may be 'label-XXX' or 'label-XXX_feature-YYY'\n",
    "        lab_tok = parts[3]\n",
    "        out['label'] = lab_tok.split('-', 1)[1]\n",
    "        \n",
    "        if out['label'] == 'thickness':\n",
    "            out['feature'] = 'thickness'\n",
    "        else:\n",
    "            # Find the feature between 'label-{label}_' and the next '_'\n",
    "            label_pattern = f\"label-{out['label']}_\"\n",
    "            if label_pattern in s:\n",
    "                after_label = s.split(label_pattern, 1)[1]\n",
    "            # Get everything before the next '_' or end of string\n",
    "            feature_part = after_label.split('_')[0]\n",
    "            # Remove any prefix like 'feature-'\n",
    "            out['feature'] = feature_part.replace('feature-', '')\n",
    "            \n",
    "        # Fix smoothing search - look for smoothing tokens in the original string\n",
    "        for part in parts:\n",
    "            if 'smth-' in part or 'smooth-' in part:\n",
    "                if 'smth-' in part:\n",
    "                    smoothing_part = part.split('smth-')[1]\n",
    "                else:\n",
    "                    smoothing_part = part.split('smooth-')[1]\n",
    "                if smoothing_part.endswith('mm'):\n",
    "                    out['smoothing'] = smoothing_part.replace('mm','')  # Remove 'mm' suffix\n",
    "                break            \n",
    "        \n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return out\n",
    "\n",
    "def correspZBCol(tTsTCol_l, tTsTCol_r):\n",
    "    \"\"\"\n",
    "    Take name of 3T7T smoothed map path column and return corresponding zb smoothed map path column.\n",
    "    \"\"\"\n",
    "    d_l = parse_map_colname(tTsTCol_l)\n",
    "    d_r = parse_map_colname(tTsTCol_r)\n",
    "    \n",
    "    #print(tsutil.print_dict(d_l, return_txt = True))\n",
    "    #print(tsutil.print_dict(d_r, return_txt = True))\n",
    "    \n",
    "    assert d_l['region'] == d_r['region'], \"Regions do not match\"\n",
    "    \n",
    "    zb_base_l = f\"zb_{d_l['region']}_hemi-{d_l['hemi']}\"\n",
    "    zb_base_r = f\"zb_{d_r['region']}_hemi-{d_r['hemi']}\"\n",
    "    \n",
    "    if d_l['region'] == 'hipp':\n",
    "        zb_l = f\"{zb_base_l}_surf-{d_l['surf']}_label-{d_l['label']}_feature-{d_l['feature']}_smooth-{d_l['smoothing']}mm\"\n",
    "        zb_r = f\"{zb_base_r}_surf-{d_r['surf']}_label-{d_r['label']}_feature-{d_r['feature']}_smooth-{d_r['smoothing']}mm\"\n",
    "    else:\n",
    "        zb_l = f\"{zb_base_l}_surf-{d_l['surf']}_label-{d_l['label']}_feature-{d_l['feature']}_smooth-{d_l['smoothing']}mm\"\n",
    "        zb_r = f\"{zb_base_r}_surf-{d_r['surf']}_label-{d_r['label']}_feature-{d_r['feature']}_smooth-{d_r['smoothing']}mm\"\n",
    "    return zb_l, zb_r\n",
    "\n",
    "importlib.reload(tsutil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748b6e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_pth = \"/host/verges/tank/data/daniel/3T7T/z/outputs/04d_dl_maps_26Nov2025-153407.pkl\"\n",
    "dl = tsutil.loadPickle(dl_pth)\n",
    "\n",
    "log_pth = f\"/host/verges/tank/data/daniel/01_3T7T/z/outputs/debug/logs/mapDifs_{datetime.datetime.now().strftime('%d%b%Y-%H%M%S')}\"\n",
    "logger = tsutil._get_file_logger(__name__, log_file_path=log_pth)\n",
    "print(f\"Logging to: {log_pth}\")\n",
    "tsutil.print_dict(dl)\n",
    "\n",
    "for i, itm in enumerate(dl):\n",
    "    print(f\"{i}/{len(dl)}...\")\n",
    "    logger.info(f\"{'-'*100}\\n{tsutil.printItemMetadata(itm, return_txt=True)}\") \n",
    "    mps = tsutil.loadPickle(itm['df_maps'])\n",
    "    mps_L, mps_R = tsutil.splitHemis(mps, rmv_lbl=True)\n",
    "    describeMapDif(mps_L, mps_R, logger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a6fe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print name of functions in tsutil\n",
    "funcs = [f for f in dir(tsutil) if callable(getattr(tsutil, f)) and not f.startswith(\"_\")]\n",
    "print(\"Functions in tTsTGrpUtils:\")\n",
    "for f in funcs:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398740c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare to zbrains\n",
    "df_pths_zb = '/host/verges/tank/data/daniel/01_3T7T/z/outputs/debug/04c_dfPths_dsMaps_withZb_27Nov2025-1015.csv' # has paths to 3T7T smoothed maps and zbsmoothed maps\n",
    "\n",
    "# for rows with analogous paths to 3T7T and zb maps, load both and compute difference\n",
    "df_pths = pd.read_csv(df_pths_zb)\n",
    "cols_L, cols_R = tsutil.get_mapCols(df_pths.columns, verbose=False)\n",
    "\n",
    "zb_cols_l = [c for c in cols_L if 'zb_' in c]\n",
    "zb_cols_r = [c for c in cols_R if 'zb_' in c]\n",
    "\n",
    "# remove all elements in zb_cols_l from cols_L\n",
    "cols_L = [c for c in cols_L if c not in zb_cols_l]\n",
    "cols_R = [c for c in cols_R if c not in zb_cols_r]\n",
    "\n",
    "df_pths['ID'] = df_pths.apply(lambda row: row['MICS_ID'] if row['study'] == '3T' else row['PNI_ID'], axis=1) # create single ID col\n",
    "\n",
    "log_pth = f\"/host/verges/tank/data/daniel/01_3T7T/z/outputs/debug/logs/mapDifs_smoothed-3T7T-zb_{datetime.datetime.now().strftime('%d%b%Y-%H%M%S')}\"\n",
    "logger = tsutil._get_file_logger(__name__, log_file_path=log_pth)\n",
    "print(f\"Logging to: {log_pth}\")\n",
    "logger.info(f\"Summary stats for difference between 3T7T smoothed maps and zBrains smoothed maps...\\n\")\n",
    "\n",
    "for idx, c_l, c_r in zip(range(len(cols_L)), cols_L, cols_R):\n",
    "    \n",
    "    print(f\"{idx}/{len(cols_L)}...\")\n",
    "    \n",
    "    if ('unsmth' in c_l or 'unsmth' in c_r) or ('res-0p8' in c_l or 'res-0p8' in c_r):\n",
    "        #logger.info(f\"{'-'*100}\\n\\t3T7T cols [idx: {idx}]:\\n\\t\\t{c_l}\\t\\t|\\t{c_r}\")\n",
    "        #logger.info(f\"\\tSkipping (smth=0 or res=0.8mm)\\n\")\n",
    "        continue\n",
    "    \n",
    "    zb_l, zb_r = correspZBCol(c_l, c_r)\n",
    "    logger.info(f\"{'-'*100}\\n\\t3T7T cols [idx: {idx}]:\\n\\t\\t{c_l}\\t\\t|\\t{c_r}\\n\\tzb cols [idx: {idx}]:\\n\\t\\t{zb_l}\\t\\t|\\t{zb_r}\")\n",
    "    \n",
    "    # A. Find rows with valid paths in zb, 3T7T cols\n",
    "    df_valid = df_pths[\n",
    "        df_pths[c_l].notna() & df_pths[zb_l].notna() &\n",
    "        df_pths[c_r].notna() & df_pths[zb_r].notna()\n",
    "    ]\n",
    "\n",
    "    if df_valid.empty:\n",
    "        logger.info(\"\\tSkipping, 0 rows.\\n\")\n",
    "        continue\n",
    "    \n",
    "    # B. Read in maps\n",
    "    logger.info(f\"\\t{len(df_valid)} valid rows\")\n",
    "    \n",
    "    tTsT_maps = tsutil.get_maps(df_valid, mapCols = [c_l, c_r], col_ID = 'ID', col_study = 'study')\n",
    "    zb_maps = tsutil.get_maps(df_valid, mapCols = [zb_l, zb_r], col_ID = 'ID', col_study = 'study')\n",
    "\n",
    "    logger.info(f\"\\ttTsT [{tTsT_maps.shape}]: {tTsT_maps.index.tolist()}\")\n",
    "    logger.info(f\"\\tzb   [{zb_maps.shape}]: {zb_maps.index.tolist()}\")\n",
    "    \n",
    "    # C. take difference and print summary stats to log\n",
    "    describeMapDif(tTsT_maps, zb_maps, logger, n_bins=12, investigate=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f9db19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tTsT_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
