{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a75c12be",
   "metadata": {},
   "source": [
    "# Check that hemispheres are:\n",
    "# 1. not identical\n",
    "# 2. correclty assigned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96068824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import nibabel as nib\n",
    "os.chdir(\"/host/verges/tank/data/daniel/01_3T7T/z/code/analyses/\")\n",
    "import tTsTGrpUtils as tsutil\n",
    "import utils_plots as up\n",
    "import importlib\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2bcf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pth_df_map_pths = \"/host/verges/tank/data/daniel/01_3T7T/z/outputs/04c_dfPths_dsMaps_30Nov2025-101445.csv\"\n",
    "dl_pth = \"/host/verges/tank/data/daniel/3T7T/z/outputs/04d_dl_maps_26Nov2025-153407.pkl\"\n",
    "\n",
    "# Study details\n",
    "MICs = {\n",
    "    \"name\": \"MICs\",\n",
    "    \"dir_root\": \"/data/mica3/BIDS_MICs\",\n",
    "    \"dir_raw\": \"/rawdata\",\n",
    "    \"dir_deriv\": \"/derivatives\",\n",
    "    \"dir_mp\": \"/micapipe_v0.2.0\",\n",
    "    \"dir_hu\": \"/hippunfold_v1.3.0/hippunfold\",\n",
    "    \"dir_zb\": \"/zbrains_clinical\",\n",
    "    \"study\": \"3T\",\n",
    "    \"ID_ctrl\" : [\"HC\"], # patterns for control IDs in demographics file\n",
    "    \"ID_Pt\" : [\"PX\"] # patterns for patient IDs in demographics file\n",
    "    }\n",
    "\n",
    "PNI = {\n",
    "    \"name\": \"PNI\",\n",
    "    \"dir_root\": \"/data/mica3/BIDS_PNI\",\n",
    "    \"dir_raw\": \"/rawdata\",\n",
    "    \"dir_deriv\": \"/derivatives\",\n",
    "    \"dir_mp\": \"/micapipe_v0.2.0\",\n",
    "    \"dir_hu\": \"/hippunfold_v1.3.0/hippunfold\",\n",
    "    \"dir_zb\": \"/zbrains_clinical\",\n",
    "    \"study\": \"7T\",\n",
    "    \"ID_col\" : [\"PNC\", \"Pilot\"], # column for ID in demographics file\n",
    "    }\n",
    "\n",
    "studies = [MICs, PNI]\n",
    "\n",
    "# Demographics details\n",
    "demographics = {\n",
    "    \"df_pths_qc_pth\" : \"/host/verges/tank/data/daniel/01_3T7T/z/outputs/03b_mapPths_QC_26Nov2025-125407.csv\", # NOTE: path to demographics file with merged QC cols produced by 02_demo.ipynb\n",
    "    # column names:\n",
    "    'nStudies': True, # whether multiple studies are included\n",
    "    \"ID_7T\" : \"PNI_ID\", \n",
    "    \"ID_3T\" : \"MICS_ID\",\n",
    "    \"SES\" : \"SES\",\n",
    "    \"date\": \"Date\",\n",
    "    \"age\": \"age\",\n",
    "    \"sex\": \"sex\",\n",
    "    \"grp\" : \"grp_detailed\" # col name for participant grouping variable to use\n",
    "}\n",
    "\n",
    "specs  = { # all spec values to be in lists to allow for iteration across these values\n",
    "    # directories\n",
    "    'prjDir_root' : \"/host/verges/tank/data/daniel/01_3T7T/z\", \n",
    "    'prjDir_outs' : \"/outputs\",\n",
    "    'prjDir_out_stats': \"/outputs/stats\",\n",
    "    'prjDir_out_figs': \"/outputs/figures\",\n",
    "    'prjDir_maps': \"/maps\", # output directory for smoothed cortical maps\n",
    "    'prjDir_dictLists': \"/maps/dictLists\",\n",
    "    'prjDir_mapPths' : \"/output/paths\",\n",
    "    'prjDir_maps_dfs': \"/outputs/dfs/04a_maps_dfs\",\n",
    "    'prjDir_parc_dfs': \"/outputs/dfs/04b_maps_parc\",\n",
    "    'prjDir_winComp_dfs': \"/outputs/dfs/05a_winComp\",\n",
    "    'prjDir_grpFlip_dfs': \"/outputs/dfs/05b_grpFlip\",\n",
    "    'prjDir_winD_dfs': \"/outputs/dfs/05c_winD\",\n",
    "    'prjDir_btwD_dfs': \"/outputs/dfs/05d_btwComp\",\n",
    "\n",
    "    # downsampling\n",
    "    'ds_study': ['PNI'], # list of study codes to apply downsampling to\n",
    "    'ds_foi': ['T1map'], # features to downsample\n",
    "    'ds_res': [0.8], # resolution (in mm) to downsample volumes to. NOTE. should be same length as ds_foi with each value corresponding to that in ds_foi \n",
    "    'ds_vol_dir': '/downsampled_vols', # name of directory within project dir root\n",
    "\n",
    "    # analysis regions\n",
    "    'ctx': True, # whether to include cortical analyses\n",
    "    'surf_ctx': ['fsLR-32k', 'fsLR-5k'],\n",
    "    'parcellate_ctx': 'glasser', # parcellation to use, or None if no parcellation.\n",
    "    'parc_lbl_ctx': 'glasser_int', # what name to fetch for parcellation values\n",
    "    'lbl_ctx': ['midthickness', 'pial', 'white'], # pial, midthick, white, etc\n",
    "    'ft_ctx': ['thickness', 'T1map'], # features: T1map, flair, thickness, FA, ADC\n",
    "    'smth_ctx': [5, 10], # in mm\n",
    "    \n",
    "    'hipp': True, # whether to include hippocampal analyses\n",
    "    'surf_hipp': ['den-0p5mm'],\n",
    "    'parcellate_hipp': 'DK25',\n",
    "    'parc_lbl_hipp': 'idx',\n",
    "    'lbl_hipp': ['midthickness', \"inner\", \"outer\"], # outer, inner, midthickness, etc\n",
    "    'ft_hipp': ['thickness', 'T1map'], # features: T1map, flair, thickness, FA, ADC\n",
    "    'smth_hipp': [2, 5], # in mm\n",
    "        \n",
    "    # within study comparisons\n",
    "    'col_grp': 'grp_detailed',  # column in df_demo with group labels\n",
    "    'winComp_stats': ['z'], # what stats to run for within study comparisons ('z' for z-scoring, 'w' for w-scoring)\n",
    "    'covars': [demographics['age'], demographics['sex']],\n",
    "\n",
    "    'ipsiTo' : 'L', # what hemisphere for controls ipsi should be mapped to\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835fe68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(tsutil)\n",
    "\n",
    "def bin(n_vertices, n_bins):\n",
    "    import numpy as np\n",
    "    return np.array_split(np.arange(n_vertices), n_bins)\n",
    "\n",
    "def describeMapDif(x, y, logger, investigate = False, n_bins = 20, zeroVal = 1e-4):\n",
    "    \"\"\"\n",
    "    Describe the difference in df of same size (eg., maps with v vertices by n participants)\n",
    "    between two dataframes x and y (eg., left and right hemisphere maps).\n",
    "\n",
    "\n",
    "    investigate: bool\n",
    "        If True, then if the mean difference within any bin i more than 1 std from the overall mean, \n",
    "        then split this bin into sub-bins and print include those results\n",
    "    \"\"\"\n",
    "    # print summary stats\n",
    "    logger.info(\"\\tDifference L-R summary stats:\")\n",
    "    binned_stats = [] # init output\n",
    "\n",
    "    zeroVal_fmt = f\"{zeroVal:.0e}\"\n",
    "    subbins = max(1, n_bins // 4)\n",
    "    \n",
    "    d = x - y\n",
    "    d_vals = d.values # convert to array\n",
    "    all_flat = d_vals.ravel()    \n",
    "    n_above_all = int(np.count_nonzero(np.logical_and(~np.isnan(all_flat), abs(all_flat) > zeroVal)))\n",
    "    #  Summary stats across all vertices\n",
    "    all_stats = pd.Series({\n",
    "        'name': 'all',\n",
    "        'n_vertices': d_vals.shape[1],\n",
    "        'n_subjects': d_vals.shape[0],\n",
    "        'n_totVals': all_flat.size,\n",
    "        'mean': np.nanmean(all_flat),\n",
    "        'std': np.nanstd(all_flat),\n",
    "        'min': np.nanmin(all_flat),\n",
    "        '25%': np.nanpercentile(all_flat, 25),\n",
    "        '50%': np.nanpercentile(all_flat, 50),\n",
    "        '75%': np.nanpercentile(all_flat, 75),\n",
    "        'max': np.nanmax(all_flat),\n",
    "        f'n_above{zeroVal_fmt}': n_above_all,\n",
    "        f'%_above{zeroVal_fmt}': (n_above_all / all_flat.size) * 100\n",
    "    })\n",
    "\n",
    "    binned_stats.append(all_stats)\n",
    "    subbin_stats = []\n",
    "\n",
    "    n_subj, n_vertices = d.shape\n",
    "    idx_grps = bin(n_vertices = n_vertices, n_bins=n_bins)\n",
    "    for bin_idx, idxs in enumerate(idx_grps, start=1):\n",
    "        bin_values = d_vals[:, idxs]\n",
    "        flat_bin = bin_values.ravel()\n",
    "        n_above = int(np.count_nonzero(np.logical_and(~np.isnan(flat_bin), flat_bin > zeroVal)))\n",
    "        # Calculate stats directly with numpy (much faster)\n",
    "        bin_stats = pd.Series({\n",
    "            'name': f'bin_{bin_idx}',\n",
    "            'n_vertices': bin_values.shape[1],\n",
    "            'n_subjects': bin_values.shape[0],\n",
    "            'n_totVals': flat_bin.size,\n",
    "            'mean': np.nanmean(flat_bin),\n",
    "            'std': np.nanstd(flat_bin),\n",
    "            'min': np.nanmin(flat_bin),\n",
    "            '25%': np.nanpercentile(flat_bin, 25),\n",
    "            '50%': np.nanpercentile(flat_bin, 50),\n",
    "            '75%': np.nanpercentile(flat_bin, 75),\n",
    "            'max': np.nanmax(flat_bin),\n",
    "            f'n_above{zeroVal_fmt}': n_above,\n",
    "            f'%_above{zeroVal_fmt}': (n_above / flat_bin.size) * 100\n",
    "        })\n",
    "        binned_stats.append(bin_stats)\n",
    "\n",
    "        if investigate:\n",
    "            if bin_stats[f'%_above{zeroVal_fmt}'] >= 5:\n",
    "                # Further investigate this bin by splitting into sub-bins\n",
    "                idx_subgrps = bin(n_vertices = bin_values.shape[1], n_bins=subbins)\n",
    "                for subbin_idx, sub_idxs in enumerate(idx_subgrps, start=1):\n",
    "                    subbin_vals = bin_values[:,sub_idxs]\n",
    "                    flat_sub = subbin_vals.ravel()\n",
    "                    \n",
    "                    n_above_sub = int(np.count_nonzero(np.logical_and(~np.isnan(flat_sub), flat_sub > zeroVal)))\n",
    "                    count = subbin_vals.size\n",
    "\n",
    "                    if count == 0:\n",
    "                        sub_bin_stats = pd.Series({\n",
    "                            'name': f'bin_{bin_idx}-{subbin_idx}',\n",
    "                            'n_vertices': subbin_vals.shape[1],\n",
    "                            'n_subjects': subbin_vals.shape[0],\n",
    "                            'n_totVals': count,\n",
    "                            'mean': np.nan,\n",
    "                            'std': np.nan,\n",
    "                            'min': np.nan,\n",
    "                            '25%': np.nan,\n",
    "                            '50%': np.nan,\n",
    "                            '75%': np.nan,\n",
    "                            'max': np.nan,\n",
    "                            f'n_above{zeroVal_fmt}': count,\n",
    "                            f'%_above{zeroVal_fmt}': np.nan\n",
    "                        })\n",
    "                    else:\n",
    "                        sub_bin_stats = pd.Series({\n",
    "                            'name': f'bin_{bin_idx}-{subbin_idx}',\n",
    "                            'n_vertices': subbin_vals.shape[1],\n",
    "                            'n_subjects': subbin_vals.shape[0],\n",
    "                            'n_totVals': count,\n",
    "                            'mean': np.nanmean(flat_sub),\n",
    "                            'std': np.nanstd(flat_sub),\n",
    "                            'min': np.nanmin(flat_sub),\n",
    "                            '25%': np.nanpercentile(flat_sub, 25),\n",
    "                            '50%': np.nanpercentile(flat_sub, 50),\n",
    "                            '75%': np.nanpercentile(flat_sub, 75),\n",
    "                            'max': np.nanmax(flat_sub),\n",
    "                            f'n_above{zeroVal_fmt}': n_above_sub,\n",
    "                            f'%_above{zeroVal_fmt}': (n_above_sub / count) * 100\n",
    "                        })\n",
    "                    subbin_stats.append(sub_bin_stats)\n",
    "    \n",
    "    # Combine all bin statistics into a single DataFrame\n",
    "    binned_df = pd.concat(binned_stats, axis=1)\n",
    "    logger.info(binned_df.round(3).to_string())\n",
    "    \n",
    "    if investigate and subbin_stats:\n",
    "        logger.info(\"\\n\\tInvestigated Sub-bins stats:\")\n",
    "        subbin_df = pd.concat(subbin_stats, axis=1)\n",
    "        logger.info(subbin_df.round(3).to_string())\n",
    "\n",
    "def parse_map_colname(s):\n",
    "    \"\"\"\n",
    "    Return dict with keys: region, hemi, surf, label, feature, smoothing (may be None)\n",
    "    \"\"\"\n",
    "    import re\n",
    "    ft_wrong = False\n",
    "    # regex tuned for patterns like:\n",
    "    # ctx_hemi-R_surf-fsLR-32k_label-white_T1map_smth-10mm\n",
    "    # or zb_ctx_hemi-L_surf-fsLR-5k_label-pial_feature-T1map_smooth-5mm\n",
    "    \n",
    "    pat = re.compile(\n",
    "        r'^(?P<region>[^_]+)_hemi-(?P<hemi>[LR])_surf-(?P<surf>[^_]+)_label-(?P<label>[^_]+)'\n",
    "        r'(?:_(?:feature-)?(?P<feature>[^_]+))?(?:_(?:smth|smooth)-(?P<smoothing>\\d+)mm)?$'\n",
    "    )\n",
    "\n",
    "    m = pat.match(s)\n",
    "    if m:\n",
    "        \n",
    "        out = m.groupdict()\n",
    "        \n",
    "        if 'smth' in out['feature'] or 'smooth' in out['feature']:\n",
    "            ft_wrong = True\n",
    "            smth_kernel = out['feature'].split('-')[1]\n",
    "            if smth_kernel.endswith('mm'):\n",
    "                smth_kernel = smth_kernel.replace('mm','')\n",
    "            out['smoothing'] = smth_kernel\n",
    "            \n",
    "        # ensure feature extracted (sometimes appears before label or without \"feature-\")\n",
    "        if out['label'] == 'thickness':\n",
    "            out['feature'] = 'thickness'\n",
    "            out['label'] = 'midthickness'\n",
    "        elif out['feature'] is None or ft_wrong:\n",
    "            # try to get trailing token after label (e.g. ..._label-white_T1map_smth-10mm)\n",
    "            parts = s.split('_')\n",
    "            if len(parts) >= 5:\n",
    "                out['feature'] = parts[4].replace('feature-', '')\n",
    "        return out\n",
    "\n",
    "    # fallback: structured split (best-effort)\n",
    "    parts = s.split('_')\n",
    "    out = {'region': None, 'hemi': None, 'surf': None, 'label': None, 'feature': None, 'smoothing': None}\n",
    "    try:\n",
    "        out['region'] = parts[0]\n",
    "        out['hemi'] = parts[1].split('-', 1)[1]\n",
    "        out['surf'] = parts[2].split('-', 1)[1]\n",
    "        # label token may be 'label-XXX' or 'label-XXX_feature-YYY'\n",
    "        lab_tok = parts[3]\n",
    "        out['label'] = lab_tok.split('-', 1)[1]\n",
    "        \n",
    "        if out['label'] == 'thickness':\n",
    "            out['feature'] = 'thickness'\n",
    "        else:\n",
    "            # Find the feature between 'label-{label}_' and the next '_'\n",
    "            label_pattern = f\"label-{out['label']}_\"\n",
    "            if label_pattern in s:\n",
    "                after_label = s.split(label_pattern, 1)[1]\n",
    "            # Get everything before the next '_' or end of string\n",
    "            feature_part = after_label.split('_')[0]\n",
    "            # Remove any prefix like 'feature-'\n",
    "            out['feature'] = feature_part.replace('feature-', '')\n",
    "            \n",
    "        # Fix smoothing search - look for smoothing tokens in the original string\n",
    "        for part in parts:\n",
    "            if 'smth-' in part or 'smooth-' in part:\n",
    "                if 'smth-' in part:\n",
    "                    smoothing_part = part.split('smth-')[1]\n",
    "                else:\n",
    "                    smoothing_part = part.split('smooth-')[1]\n",
    "                if smoothing_part.endswith('mm'):\n",
    "                    out['smoothing'] = smoothing_part.replace('mm','')  # Remove 'mm' suffix\n",
    "                break            \n",
    "        \n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return out\n",
    "\n",
    "def correspZBCol(tTsTCol_l, tTsTCol_r):\n",
    "    \"\"\"\n",
    "    Take name of 3T7T smoothed map path column and return corresponding zb smoothed map path column.\n",
    "    \"\"\"\n",
    "    d_l = parse_map_colname(tTsTCol_l)\n",
    "    d_r = parse_map_colname(tTsTCol_r)\n",
    "    \n",
    "    #print(tsutil.print_dict(d_l, return_txt = True))\n",
    "    #print(tsutil.print_dict(d_r, return_txt = True))\n",
    "    \n",
    "    assert d_l['region'] == d_r['region'], \"Regions do not match\"\n",
    "    \n",
    "    zb_base_l = f\"zb_{d_l['region']}_hemi-{d_l['hemi']}\"\n",
    "    zb_base_r = f\"zb_{d_r['region']}_hemi-{d_r['hemi']}\"\n",
    "    \n",
    "    if d_l['region'] == 'hipp':\n",
    "        zb_l = f\"{zb_base_l}_surf-{d_l['surf']}_label-{d_l['label']}_feature-{d_l['feature']}_smooth-{d_l['smoothing']}mm\"\n",
    "        zb_r = f\"{zb_base_r}_surf-{d_r['surf']}_label-{d_r['label']}_feature-{d_r['feature']}_smooth-{d_r['smoothing']}mm\"\n",
    "    else:\n",
    "        zb_l = f\"{zb_base_l}_surf-{d_l['surf']}_label-{d_l['label']}_feature-{d_l['feature']}_smooth-{d_l['smoothing']}mm\"\n",
    "        zb_r = f\"{zb_base_r}_surf-{d_r['surf']}_label-{d_r['label']}_feature-{d_r['feature']}_smooth-{d_r['smoothing']}mm\"\n",
    "    return zb_l, zb_r\n",
    "\n",
    "def zbrainsMaps(study, id, ses, region, feat, lbl, surf, smth):\n",
    "    \"\"\"\n",
    "    Gets path to z-brains smoothed maps\n",
    "\n",
    "    Inputs:\n",
    "        study: dict\n",
    "            paths to directories including root, deriv, z-brains\n",
    "        id: str\n",
    "            participant ID (no sub-)\n",
    "        ses: str\n",
    "            session ID (no ses-)\n",
    "        region: str\n",
    "            'cortex'/'ctx' or 'hippocampus'/'hip'\n",
    "        feat: str\n",
    "            feature name\n",
    "        lbl: str\n",
    "            label name (ie. midthickness, white, inner, outer etc)\n",
    "        surf: str\n",
    "            surface name (fsLR-5k, fsLR-32k)\n",
    "        smth: int\n",
    "            smoothing value in mm\n",
    "\n",
    "    Returns L, R if both exist\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import numpy as np\n",
    "    if region == 'ctx' or region == 'cortex':\n",
    "        region = 'cortex'\n",
    "    elif region == 'hip' or region == 'hippocampus':\n",
    "        region = 'hippocampus'\n",
    "    else:\n",
    "        raise ValueError(f\"region <{region}> not recognized. Should be 'cortex' or 'hippocampus'\")\n",
    "    \n",
    "    if smth == 'NA' or smth == 0: # should get unsmoothed path from micapipe outputs\n",
    "        root_mp = f\"{study['dir_root']}{study['dir_deriv']}{study['dir_mp']}/sub-{id}/ses-{ses}/maps\"\n",
    "        if feat == \"thickness\":\n",
    "            out_pth_L_filename = f\"hemi-L_surf-{surf}_label-{feat}\"\n",
    "            out_pth_R_filename = f\"hemi-R_surf-{surf}_label-{feat}\"\n",
    "        else:\n",
    "            out_pth_L_filename = f\"hemi-L_surf-{surf}_label-{lbl}_{feat}\"\n",
    "            out_pth_R_filename = f\"hemi-R_surf-{surf}_label-{lbl}_{feat}\"\n",
    "            \n",
    "        # ii. Paths to micapipe unsmoothed maps    \n",
    "        pth_L = f\"{root_mp}/sub-{id}_ses-{ses}_{out_pth_L_filename}.func.gii\"\n",
    "        pth_R = f\"{root_mp}/sub-{id}_ses-{ses}_{out_pth_R_filename}.func.gii\"\n",
    "\n",
    "    else: # get zbrains smoothed map paths\n",
    "        base_pth = f\"{study['dir_root']}{study['dir_deriv']}{study['dir_zb']}/sub-{id}/ses-{ses}/maps/{region}\"\n",
    "        if region == 'cortex':\n",
    "            pth_L = f\"{base_pth}/sub-{id}_ses-{ses}_hemi-L_surf-{surf}_label-{lbl}_feature-{feat}_smooth-{smth}mm.func.gii\"\n",
    "            pth_R = f\"{base_pth}/sub-{id}_ses-{ses}_hemi-R_surf-{surf}_label-{lbl}_feature-{feat}_smooth-{smth}mm.func.gii\"\n",
    "        else:\n",
    "            pth_L = f\"{base_pth}/sub-{id}_ses-{ses}_hemi-L_{surf}_label-{lbl}_feature-{feat}_smooth-{smth}mm.func.gii\"\n",
    "            pth_R = f\"{base_pth}/sub-{id}_ses-{ses}_hemi-R_{surf}_label-{lbl}_feature-{feat}_smooth-{smth}mm.func.gii\"\n",
    "\n",
    "    \n",
    "    if os.path.exists(pth_L) == True and os.path.exists(pth_R) == True:   \n",
    "        print(f\"[zbrainsMaps] \\tL: {pth_L}\\n\\t\\tR: {pth_R}\")\n",
    "        return pth_L, pth_R\n",
    "    else:\n",
    "        print(f\"[zbrainsMaps] NO EXIST \\t\\tL: {pth_L}\\n\\t\\t\\tR: {pth_R}\")\n",
    "        return np.nan, np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748b6e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Compare hemispheres (3T7T data only)\n",
    "dl = tsutil.loadPickle(dl_pth)\n",
    "\n",
    "log_pth = f\"/host/verges/tank/data/daniel/01_3T7T/z/outputs/debug/logs/mapDifs_{datetime.datetime.now().strftime('%d%b%Y-%H%M%S')}\"\n",
    "logger = tsutil._get_file_logger(__name__, log_file_path=log_pth)\n",
    "logger.info(f\"Log started: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "logger.info(f\"[debug script: mapValues] Comparing L and R hemisphere maps.\\n\\t If processing is correct, these difference values should not be 0\")\n",
    "print(f\"Logging to: {log_pth}\")\n",
    "tsutil.print_dict(dl)\n",
    "\n",
    "for i, itm in enumerate(dl):\n",
    "    print(f\"{i}/{len(dl)}...\")\n",
    "    logger.info(f\"{'-'*100}\\n{tsutil.printItemMetadata(itm, return_txt=True)}\") \n",
    "    mps = tsutil.loadPickle(itm['df_maps'])\n",
    "    mps_L, mps_R = tsutil.splitHemis(mps, rmv_lbl=True)\n",
    "    describeMapDif(mps_L, mps_R, logger)\n",
    "print(f\"Log saved to: {log_pth}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398740c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Compare 3T7T maps to zbrains maps\n",
    "\n",
    "import datetime\n",
    "\n",
    "## 0. Load df with paths to 3T7T smoothed maps\n",
    "reimport_src = False\n",
    "if 'df_clean' not in globals() or df_clean is None or reimport_src == True:\n",
    "    df_clean_ds = pd.read_csv(pth_df_map_pths, dtype=str)\n",
    "df_zb = df_clean_ds.copy()\n",
    "print(f'Initial shape df: {df_zb.shape}')\n",
    "\n",
    "## a. get path to z-brains smoothed maps and add to df_clean_ds\n",
    "for idx, row in df_zb.iterrows():\n",
    "    study_name = row['study']\n",
    "    if study_name == '3T':\n",
    "        study = MICs\n",
    "    elif study_name == '7T':\n",
    "        study = PNI\n",
    "    else:\n",
    "        raise ValueError(f\"study name <{study_name}> not recognized [index: {idx}]\")\n",
    "    \n",
    "    id_ = row[demographics['ID_3T']] if study_name == '3T' else row[demographics['ID_7T']]\n",
    "    ses = row['SES']\n",
    "\n",
    "    if specs['ctx']:\n",
    "        for feat in specs['ft_ctx']:\n",
    "            for lbl in specs['lbl_ctx']:\n",
    "                for surf in specs['surf_ctx']:\n",
    "                    for smth in specs['smth_ctx']:\n",
    "                        pth_L, pth_R = zbrainsMaps(study, id_, ses, 'cortex', feat, lbl, surf, smth)\n",
    "                        col_L = f\"zb_ctx_hemi-L_surf-{surf}_label-{lbl}_feature-{feat}_smooth-{smth}mm\"\n",
    "                        col_R = f\"zb_ctx_hemi-R_surf-{surf}_label-{lbl}_feature-{feat}_smooth-{smth}mm\"\n",
    "                        df_zb.at[idx, col_L] = pth_L\n",
    "                        df_zb.at[idx, col_R] = pth_R\n",
    "                        #assert 0 == 1, \"STOP\"\n",
    "\n",
    "    if specs['hipp']:\n",
    "        for feat in specs['ft_hipp']:\n",
    "            for lbl in specs['lbl_hipp']:\n",
    "                for surf in specs['surf_hipp']:\n",
    "                    for smth in specs['smth_hipp']:\n",
    "                        \n",
    "                        pth_L, pth_R = zbrainsMaps(study, id_, ses, 'hippocampus', feat, lbl, surf, smth)\n",
    "                        col_L = f\"zb_hipp_hemi-L_surf-{surf}_label-{lbl}_feature-{feat}_smooth-{smth}mm\"\n",
    "                        col_R = f\"zb_hipp_hemi-R_surf-{surf}_label-{lbl}_feature-{feat}_smooth-{smth}mm\"\n",
    "                        \n",
    "                        df_zb.at[idx, col_L] = pth_L\n",
    "                        df_zb.at[idx, col_R] = pth_R\n",
    "                        \n",
    "print(f'Final shape df: {df_zb.shape}')\n",
    "\n",
    "if 'UID_study_ses' in df_zb.columns:\n",
    "    try:\n",
    "        df_zb.drop(columns='UID_study_ses', inplace=True)\n",
    "    except:\n",
    "        print(\"[main] could not drop column UID_study_ses\")\n",
    "\n",
    "\n",
    "# save df_zb_clean\n",
    "out_pth = f\"{specs['prjDir_root'] + specs['prjDir_outs']}/debug/04c_dfPths_dsMaps_withZb_{datetime.datetime.now()}.csv\"\n",
    "df_zb.to_csv(out_pth, index=False)\n",
    "print(f\"[main] df_zb_clean saved to {out_pth}\")\n",
    "\n",
    "\"\"\"\n",
    "## create dictionary list for z-brains smoothed maps\n",
    "coi = [c for c in df_zb.columns if c.startswith('zb')]\n",
    "cols_L, cols_R = tsutil.get_mapCols(coi, verbose=True)\n",
    "\n",
    "dl_hipp_zb = tsutil.extractMap(df_mapPaths = df_zb, cols_L = cols_L, cols_R = cols_R, \n",
    "                                specs = specs, studies = studies, demographics = demographics, qc_thresh = 2,\n",
    "                                save_df_pth = specs['prjDir_root'] + specs['prjDir_maps_dfs'], log_save_pth = specs['prjDir_root'] + specs['prjDir_outs'],\n",
    "                                region = \"hippocampus\", verbose=True, test = test)\n",
    "\"\"\"\n",
    "\n",
    "## Load 3T7T, zBrains data and compare \n",
    "df_pths_zb = out_pth # has paths to 3T7T smoothed maps and zbsmoothed maps\n",
    "\n",
    "# for rows with analogous paths to 3T7T and zb maps, load both and compute difference\n",
    "df_pths = pd.read_csv(df_pths_zb)\n",
    "cols_L, cols_R = tsutil.get_mapCols(df_pths.columns, verbose=False)\n",
    "\n",
    "zb_cols_l = [c for c in cols_L if 'zb_' in c]\n",
    "zb_cols_r = [c for c in cols_R if 'zb_' in c]\n",
    "\n",
    "# remove all elements in zb_cols_l from cols_L\n",
    "cols_L = [c for c in cols_L if c not in zb_cols_l]\n",
    "cols_R = [c for c in cols_R if c not in zb_cols_r]\n",
    "\n",
    "df_pths['ID'] = df_pths.apply(lambda row: row['MICS_ID'] if row['study'] == '3T' else row['PNI_ID'], axis=1) # create single ID col\n",
    "\n",
    "log_pth = f\"/host/verges/tank/data/daniel/01_3T7T/z/outputs/debug/logs/mapDifs_smoothed-3T7T-zb_{datetime.datetime.now().strftime('%d%b%Y-%H%M%S')}\"\n",
    "logger = tsutil._get_file_logger(__name__, log_file_path=log_pth)\n",
    "print(f\"Logging to: {log_pth}\")\n",
    "logger.info(f\"Summary stats for difference between 3T7T smoothed maps and zBrains smoothed maps...\\n\")\n",
    "\n",
    "for idx, c_l, c_r in zip(range(len(cols_L)), cols_L, cols_R):\n",
    "    \n",
    "    print(f\"{idx}/{len(cols_L)}...\")\n",
    "    \n",
    "    if ('unsmth' in c_l or 'unsmth' in c_r) or ('res-0p8' in c_l or 'res-0p8' in c_r):\n",
    "        #logger.info(f\"{'-'*100}\\n\\t3T7T cols [idx: {idx}]:\\n\\t\\t{c_l}\\t\\t|\\t{c_r}\")\n",
    "        #logger.info(f\"\\tSkipping (smth=0 or res=0.8mm)\\n\")\n",
    "        continue\n",
    "    \n",
    "    zb_l, zb_r = correspZBCol(c_l, c_r)\n",
    "    logger.info(f\"{'-'*100}\\n\\t3T7T cols [idx: {idx}]:\\n\\t\\t{c_l}\\t\\t|\\t{c_r}\\n\\tzb cols [idx: {idx}]:\\n\\t\\t{zb_l}\\t\\t|\\t{zb_r}\")\n",
    "    \n",
    "    # A. Find rows with valid paths in zb, 3T7T cols\n",
    "    df_valid = df_pths[\n",
    "        df_pths[c_l].notna() & df_pths[zb_l].notna() &\n",
    "        df_pths[c_r].notna() & df_pths[zb_r].notna()\n",
    "    ]\n",
    "\n",
    "    if df_valid.empty:\n",
    "        logger.info(\"\\tSkipping, 0 rows.\\n\")\n",
    "        continue\n",
    "    \n",
    "    # B. Read in maps\n",
    "    logger.info(f\"\\t{len(df_valid)} valid rows\")\n",
    "    \n",
    "    tTsT_maps = tsutil.get_maps(df_valid, mapCols = [c_l, c_r], col_ID = 'ID', col_study = 'study')\n",
    "    zb_maps = tsutil.get_maps(df_valid, mapCols = [zb_l, zb_r], col_ID = 'ID', col_study = 'study')\n",
    "\n",
    "    logger.info(f\"\\ttTsT [{tTsT_maps.shape}]: {tTsT_maps.index.tolist()}\")\n",
    "    logger.info(f\"\\tzb   [{zb_maps.shape}]: {zb_maps.index.tolist()}\")\n",
    "    \n",
    "    # C. take difference and print summary stats to log\n",
    "    describeMapDif(tTsT_maps, zb_maps, logger, n_bins=12, investigate=True)\n",
    "print(f\"Log saved to: {log_pth}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f9db19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tTsT_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
