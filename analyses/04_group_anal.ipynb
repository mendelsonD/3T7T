{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdcd09b9",
   "metadata": {},
   "source": [
    "# Group comparison 3T and 7T epilepsy  \n",
    "\n",
    "Surface-based comparisons  \n",
    "- vertex-wise T-test : are controls and pts different?\n",
    "- vertex-wise effect size : how big are the distances between the vertex differences?\n",
    "    - Use own function\n",
    "\n",
    "\n",
    "For figures: \n",
    "- Visualize effect size on a brain masked for significant p-values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d522203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import pickle\n",
    "import datetime\n",
    "import brainstat as bstat\n",
    "import copy\n",
    "\n",
    "import importlib\n",
    "import tTsTGrpUtils as tsutil\n",
    "import vrtx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d95bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(tsutil)\n",
    "importlib.reload(vrtx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5686b7b0",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4537cc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify root directories\n",
    "MICs = {\n",
    "    \"name\": \"MICs\",\n",
    "    \"dir_root\": \"/data/mica3/BIDS_MICs\",\n",
    "    \"dir_raw\": \"/rawdata\",\n",
    "    \"dir_deriv\": \"/derivatives\",\n",
    "    \"dir_mp\": \"/micapipe_v0.2.0\",\n",
    "    \"dir_hu\": \"/hippunfold_v1.3.0/hippunfold\",\n",
    "    \"dir_zb\": \"/DM_zb_37comp\",\n",
    "    \"study\": \"3T\",\n",
    "    \"ID_ctrl\" : [\"HC\"],\n",
    "    \"ID_Pt\" : [\"PX\"]\n",
    "    }\n",
    "\n",
    "PNI = {\n",
    "    \"name\": \"PNI\",\n",
    "    \"dir_root\": \"/data/mica3/BIDS_PNI\",\n",
    "    \"dir_raw\": \"/rawdata\",\n",
    "    \"dir_deriv\": \"/derivatives\",\n",
    "    \"dir_mp\": \"/micapipe_v0.2.0\",\n",
    "    \"dir_hu\": \"/hippunfold_v1.3.0/hippunfold\",\n",
    "    \"dir_zb\": \"/DM_zb_37comp\",\n",
    "    \"study\": \"7T\",\n",
    "    \"ID_col\" : [\"PNC\", \"Pilot\"], # column for ID in demographics file\n",
    "    }\n",
    "\n",
    "studies = [MICs, PNI]\n",
    "\n",
    "demographics = {\n",
    "    \"pth\" : \"/host/verges/tank/data/daniel/3T7T/z/data/pt/demo_27Aug2025.csv\",\n",
    "    # column names:\n",
    "    'nStudies': True, # whether multiple studies are included\n",
    "    \"ID_7T\" : \"PNI_ID\", \n",
    "    \"ID_3T\" : \"MICS_ID\",\n",
    "    \"SES\" : \"SES\",\n",
    "    \"date\": \"Date\",\n",
    "    \"age\": \"age\",\n",
    "    \"sex\": \"sex\",\n",
    "    \"grp\" : \"grp_detailed\" # col name for participant grouping variable of interest\n",
    "}\n",
    "\n",
    "ctrl_grp = {'ctrl' : ['CTRL']}\n",
    "\n",
    "px_grps = { # specify patient group labels to compare to controls\n",
    "    'allPX' : ['TLE_U', 'MFCL', 'FLE_R', 'MFCL_bTLE', 'UKN_L', 'mTLE_R', 'mTLE_L', 'FLE_L', 'UKN_U', 'TLE_L', 'TLE_R'],\n",
    "    'TLE' : ['TLE_L', 'TLE_R', 'TLE_U', 'mTLE_R', 'mTLE_L'],\n",
    "    'TLE_L': ['TLE_L', 'mTLE_L', 'bTLE_L'],\n",
    "    'TLE_R': ['TLE_R', 'mTLE_R', 'bTLE_R'],\n",
    "    'FCD' : ['FLE_R', 'FLE_L'],\n",
    "    'MFCL' : ['MFCL', 'bTLE'],\n",
    "    'UKN' : ['UKN_L', 'UKN_U']\n",
    "}\n",
    "\n",
    "# Make list of dict items for group definitions\n",
    "groups = [\n",
    "    {'TLE_L': px_grps['TLE_L']},\n",
    "    {'TLE_R': px_grps['TLE_R']},\n",
    "    ctrl_grp\n",
    "]\n",
    "\n",
    "#features = [\"thickness\"]\n",
    "\n",
    "specs  = { # all spec values to be in lists to allow for iteration across these values\n",
    "    'prjDir_root' : \"/host/verges/tank/data/daniel/3T7T/z\", # output directory for smoothed cortical maps\n",
    "    'prjDir_outs' : \"/outputs\",\n",
    "    'prjDir_out_stats': \"/outputs/stats\",\n",
    "    'prjDir_out_figs': \"/outputs/figures\",\n",
    "    'prjDir_maps' : \"/maps\",\n",
    "    'prjDir_dictLists': \"/maps/dictLists\",\n",
    "    'prjDir_mapPths' : \"/output/paths\",\n",
    "\n",
    "    'ctx': True, # whether to include cortical analyses\n",
    "    'surf_ctx': ['fsLR-5k'],\n",
    "    'lbl_ctx': ['midthickness'], # pial, midthick, white, etc\n",
    "    'ft_ctx': ['thickness', 'T1map', 'flair'], # features: T1map, flair, thickness, FA, ADC\n",
    "    'smth_ctx': [5, 10], # in mm\n",
    "    \n",
    "    'hipp': True, # whether to include hippocampal analyses\n",
    "    'surf_hipp': ['0p5mm'],\n",
    "    'lbl_hipp': ['midthickness'], # outter, inner, midthickness, etc\n",
    "    'ft_hipp': ['thickness', 'T1map', 'flair'], # features: T1map, flair, thickness, FA, ADC\n",
    "    'smth_hipp': [2, 5] # in mm\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5105b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = pd.read_csv(demographics['pth'], dtype=str)\n",
    "\n",
    "# run parameters\n",
    "verbose = True\n",
    "test = False\n",
    "test_frac = 0.01 # fraction of demo to use for testing if test=True\n",
    "\n",
    "if test:\n",
    "    # take a random 10% subset of demo for testing\n",
    "    demo = demo.sample(frac=test_frac).reset_index(drop=True)\n",
    "    demo = demo.dropna(axis=1, how='all') # drop empty columns\n",
    "    print(f\"[TEST MODE] Running on random 10% subset of demographics ({len(demo)} rows).\")\n",
    "\n",
    "print(demo[['MICS_ID', 'PNI_ID', 'study', 'SES', 'Date', 'grp']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2588a910",
   "metadata": {},
   "source": [
    "# I. Smooth maps  \n",
    "\n",
    "Strategy:\n",
    "Add paths to relevant maps to df containing demographic information. Each row is one participant at a unique session.\n",
    "\n",
    "Hippocampal maps: identify path to smoothed hippocampal maps, add to row-wise df\n",
    "Cortical maps: take raw maps from micapipe, apply smoothing then save these maps in project directory and add path of the smoothed map to the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7cf510",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(tsutil)\n",
    "df_pths, log = tsutil.idToMap(demo, studies, demographics, specs, verbose=True)\n",
    "\n",
    "# Save DataFrames with map paths\n",
    "if test:\n",
    "    out_pth = f\"{specs['prjDir_root']}{specs['prjDir_mapPths']}/TEST_demo_pths_{datetime.datetime.now().strftime('%d%b%Y')}.csv\"\n",
    "    log_pth = f\"{specs['prjDir_root']}{specs['prjDir_mapPths']}/TEST_idToMap_log_{datetime.datetime.now().strftime('%d%b%Y')}.txt\"\n",
    "else:\n",
    "    out_pth = f\"{specs['prjDir_root']}{specs['prjDir_mapPths']}/demo_pths_{datetime.datetime.now().strftime('%d%b%Y-%H%M%S')}.csv\"\n",
    "    log_pth = f\"{specs['prjDir_root']}{specs['prjDir_mapPths']}/idToMap_log_{datetime.datetime.now().strftime('%d%b%Y-%H%M%S')}.txt\"\n",
    "\n",
    "df_pths.to_csv(out_pth, index=False)\n",
    "print(f\"\\n\\n[main] Saved df with map paths: {out_pth}\")\n",
    "\n",
    "# Write the log string to file, preserving line breaks and tabs\n",
    "with open(log_pth, 'w') as f:\n",
    "    f.write(log)\n",
    "print(f\"[main] Saved idToMap log: {log_pth}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d5fb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check length of surfaces\n",
    "\"\"\"\n",
    "surf_L = \"/data/mica3/BIDS_PNI/derivatives/hippunfold_v1.3.0/hippunfold/sub-PNE029/ses-a1/surf/sub-PNE029_ses-a1_hemi-L_space-T1w_den-0p5mm_label-hipp_midthickness.surf.gii\"\n",
    "surf_R = \"/data/mica3/BIDS_PNI/derivatives/hippunfold_v1.3.0/hippunfold/sub-PNE029/ses-a1/surf/sub-PNE029_ses-a1_hemi-R_space-T1w_den-0p5mm_label-hipp_midthickness.surf.gii\"\n",
    "\n",
    "map_unsmth_L = \"/host/verges/tank/data/daniel/3T7T/z/maps/sub-PNE029_ses-a1/sub-PNE029_ses-a1_hipp_hemi-L_surf-0p5mm_label-midthickness_T1map_smth-NA.func.gii\"\n",
    "map_unsmth_R = \"/host/verges/tank/data/daniel/3T7T/z/maps/sub-PNE029_ses-a1/sub-PNE029_ses-a1_hipp_hemi-R_surf-0p5mm_label-midthickness_T1map_smth-NA.func.gii\"\n",
    "\n",
    "map_smth_L = \"/host/verges/tank/data/daniel/3T7T/z/maps/sub-PNE029_ses-a1/sub-PNE029_ses-a1_hipp_hemi-L_surf-0p5mm_label-midthickness_T1map_smth-2mm.func.gii\"\n",
    "map_smth_R = \"/host/verges/tank/data/daniel/3T7T/z/maps/sub-PNE029_ses-a1/sub-PNE029_ses-a1_hipp_hemi-R_surf-0p5mm_label-midthickness_T1map_smth_2mm.func.gii\"\n",
    "\n",
    "L = [surf_L, map_unsmth_L, map_smth_L]\n",
    "R = [surf_R, map_unsmth_R, map_smth_R]\n",
    "\n",
    "for pth in L + R:\n",
    "    gii = nib.load(pth)\n",
    "    if len(gii.darrays) > 1:\n",
    "        arr = gii.darrays[1].data\n",
    "        print(f\"{pth}: shape {arr.shape} (using second array)\")\n",
    "    else:\n",
    "        arr = gii.darrays[0].data\n",
    "        print(f\"{pth}: shape {arr.shape} (using first array)\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2ad0b7",
   "metadata": {},
   "source": [
    "# II. CLEAN DATA\n",
    "- summarize missing data\n",
    "- clean_demoPths: NA for missing hemisphere pairs, remove rows with completely missing map data, remove for missing 3T-7T pairs\n",
    "- clean_ses: select single session for each participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2910786",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_L = [col for col in df_pths.columns if 'SES' in col]\n",
    "print(col_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315230a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = False\n",
    "\n",
    "test_frac = 0.1 # fraction of demo to use for testing if test=True\n",
    "\n",
    "if test:\n",
    "    filename = \"/host/verges/tank/data/daniel/3T7T/z/maps/paths/\" + \"demo_pths_04Sep2025-135322.csv\"\n",
    "    df_pths = pd.read_csv(filename, dtype=str)\n",
    "    # take a random 10% subset of demo for testing\n",
    "    df_pths = df_pths.sample(frac=test_frac).reset_index(drop=True)\n",
    "    df_pths = df_pths.dropna(axis=1, how='all') # drop empty columns\n",
    "    print(f\"[TEST MODE] Running on random {test_frac *100}% subset of demographics ({df_pths.shape[0]} rows).\")\n",
    "\n",
    "print(f\"Unique participants: {df_pths['MICS_ID'].nunique()}\")\n",
    "print(df_pths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151ce23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a summary of errors in df_pths\n",
    "importlib.reload(tsutil)\n",
    "\n",
    "cols_L = [col for col in df_pths.columns if 'hemi-L' in col]\n",
    "cols_R = [col for col in df_pths.columns if 'hemi-R' in col]\n",
    "cols = cols_L + cols_R\n",
    "\n",
    "error_summary = tsutil.countErrors(df_pths, cols, save=f\"{specs['prjDir_root']}{specs['prjDir_mapPths']}\")\n",
    "#error_summary\n",
    "\n",
    "# save\n",
    "error_summary_pth = f\"{specs['prjDir_root']}{specs['prjDir_mapPths']}/errorSummary_{datetime.datetime.now().strftime('%d%b%Y-%H%M%S')}.csv\"\n",
    "error_summary.to_csv(error_summary_pth, index=False)\n",
    "print(f\"\\n\\n[main] Saved error summary: {error_summary_pth}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21017b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(tsutil)\n",
    "df_pths_clean, df_pths_rmv = tsutil.clean_demoPths(df_pths, nStudies=2, save=\"/host/verges/tank/data/daniel/3T7T/z/maps/paths\", verbose=False) # missing hemisphere pairs, missing study pairs\n",
    "df_pths_clean_final = tsutil.clean_ses(df_pths_clean, col_ID=demographics['ID_3T'], save = \"/host/verges/tank/data/daniel/3T7T/z/maps/paths\", col_study='study', verbose=True) # should return single session per ID (MICS_ID col chosen randomly). All rows returned should have valid map paths for this analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b196351",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pths_clean_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56950013",
   "metadata": {},
   "source": [
    "# III. Analyses\n",
    "## a. Extract maps (dict item for each feature, grp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaae85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dict item for each, study, feature, label <recent: smoothing> pair (including hippocampal)\n",
    "# Note: multiple groups should be kept in same DF. Seperate groups later on\n",
    "\n",
    "def get_maps(df, mapCols, col_grp=\"grp\", col_ID='MICS_ID', verbose=False):\n",
    "    \"\"\"\n",
    "    Take df, choose only most recent session. Return cleaned df.\n",
    "    \n",
    "    Input:\n",
    "        df: DataFrame with columns for ID, SES, Date, and paths to left and right hemisphere maps.\n",
    "            NOTE. Asusme path columns end with '_L' and '_R' for left and right hemisphere respectively.\n",
    "        ID_col: Column name for participant ID in the DataFrame. Default is 'MICS_ID'.\n",
    "    \n",
    "    Output:\n",
    "        df_clean: Cleaned DataFrame with only valid ID-SES combinations, and paths to left and right hemisphere maps.\n",
    "    \"\"\"\n",
    "    assert col_ID in df.columns, f\"[get_maps] df must contain 'ID' column. Cols in df: {df.columns}\"\n",
    "    assert 'SES' in df.columns, f\"[get_maps] df must contain 'SES' column. Cols in df: {df.columns}\"\n",
    "    assert col_grp in df.columns, f\"[get_maps] df must contain '{col_grp}' column. Cols in df: {df.columns}\"\n",
    "    \n",
    "    # Assert that all columns in mapCols exist in df\n",
    "    missing_cols = [col for col in mapCols if col not in df.columns]\n",
    "    assert not missing_cols, f\"[get_maps] The following columns from mapCols are missing in df: {missing_cols}\"\n",
    "\n",
    "    # find appropriate cols\n",
    "    col_L = [i for i in mapCols if  ('hemi-L' in i)]\n",
    "    col_R = [i for i in mapCols if ('hemi-R' in i)]\n",
    "    assert len(col_L) == 1, f\"[get_maps] more than one col with 'hemi-L'. {col_L}\"\n",
    "    assert len(col_R) == 1, f\"[gete_maps] more than one col ending with 'hemi-R'. {col_R}\"\n",
    "    if verbose:\n",
    "        print(f\"[get_maps] {col_L}, {col_R}\")\n",
    "    col_L = col_L[0]\n",
    "    col_R = col_R[0]\n",
    "\n",
    "    # read in the maps and append to df_maps\n",
    "    df_maps = df[[col_ID, 'SES', col_L, col_R]]\n",
    "    \n",
    "    # Stack all hemisphere maps into a DataFrame (vertices as columns)\n",
    "    map_L_matrix = np.vstack([nib.load(x).darrays[0].data for x in df_maps[col_L]])\n",
    "    map_R_matrix = np.vstack([nib.load(x).darrays[0].data for x in df_maps[col_R]])\n",
    "    \n",
    "    # Convert to DataFrame for easier handling\n",
    "    map_L_df = pd.DataFrame(map_L_matrix, index=df_maps.index)\n",
    "    map_R_df = pd.DataFrame(map_R_matrix, index=df_maps.index)\n",
    "\n",
    "    # Rename columns to indicate hemisphere and vertex index\n",
    "    map_L_df.columns = [f\"{v}_L\" for v in map_L_df.columns]\n",
    "    map_R_df.columns = [f\"{v}_R\" for v in map_R_df.columns]\n",
    "    \n",
    "    df_maps = df_maps.drop(columns=[col_L, col_R]) # append map_L_df and map_R_df to df_maps. Remove the original columns col_L and col_R from df_maps\n",
    "    #print(f\"\\tdf_maps cols: {df_maps.columns}\")\n",
    "    \n",
    "    df_maps = pd.concat([df_maps, map_L_df, map_R_df], axis=1)\n",
    "    #print(f\"\\tFinal shape:{df_maps.shape}\")\n",
    "\n",
    "    df_maps = df_maps.sort_values(by=col_ID).reset_index(drop=True)     # sort rows by ID\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\t[get_maps] Maps retrieved. Size: {df_maps.shape}\")\n",
    "    else:\n",
    "        print(f\"\\t[get_maps] Maps retrieved.\")\n",
    "    \n",
    "    return df_maps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ad9707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in cleaned df_pths_clean if not already in memory\n",
    "if 'df_pths_clean' not in locals() or df_pths_clean is None:\n",
    "    print(\"[main] df_pths_clean is not defined or is None, reading in previously cleaned df_pths_clean.\")\n",
    "    df_pths_clean_filepath = \"/host/verges/tank/data/daniel/3T7T/z/outputs/paths/\"\n",
    "    df_pths_clean_name = \"ses_clean_05Sep2025-155514.csv\" # TO DO: find the most recent file in the dir instead of hardcoding\n",
    "    df_pths_clean = pd.read_csv(df_pths_clean_filepath + df_pths_clean_name, dtype=str)\n",
    "\n",
    "print(f\"shape: {df_pths_clean.shape}\")\n",
    "print(f\"columns: {df_pths_clean.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e531ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: cleaned (1 ses per ID per study)\n",
    "# For each cortex, hippocampus (as appropriate):\n",
    "# Iterate over relevant map cols (the same as iterating across features, smoothing, label, surf)\n",
    "# Iterate over studies\n",
    "# drop cases with missing values in the current map col\n",
    "# extract maps: load map data from paths to specific ft, lbl, surface, smoothing map and return df with rows as participants, columns as vertices\n",
    "\n",
    "importlib.reload(tsutil)\n",
    "save = True\n",
    "save_name = \"01_maps\"\n",
    "test = False\n",
    "verbose = True\n",
    "\n",
    "print(f\"[main] Reading in maps and storing in dictionary items for each study-feature-label-surface smoothing pair.\\n\\tNote. Not seperating groups yet.\")\n",
    "\n",
    "map_cols_L = [ col for col in df_pths_clean.columns if ('hemi-L' in col) ] # find all map cols\n",
    "map_cols_R = [ col for col in df_pths_clean.columns if ('hemi-R' in col) ] # find all map cols\n",
    "print(f\"{len(map_cols_L) + len(map_cols_R)} map columns found.\")\n",
    "\n",
    "map_dictlist = []\n",
    "\n",
    "if specs['ctx']:\n",
    "    \n",
    "    ctx_cols_L = [col for col in map_cols_L if 'ctx' in col or 'cortex' in col]\n",
    "    ctx_cols_R = [col for col in map_cols_R if 'ctx' in col or 'cortex' in col]\n",
    "    if ctx_cols_L == [] or ctx_cols_R == []:\n",
    "        print(\"\\n[WARNING] No cortex map columns found. Skipping hippocampal analyses.\")\n",
    "    else:\n",
    "        print(f\"\\n{len(ctx_cols_L) + len(ctx_cols_R)} cortical map columns found.\")\n",
    "    \n",
    "    for col_L, col_R in zip(ctx_cols_L, ctx_cols_R):\n",
    "        \n",
    "        assert col_L.replace('hemi-L', '') == col_R.replace('hemi-R', ''), f\"Left and right hemisphere columns do not match: {col_L}, {col_R}\"\n",
    "        # Find the substring after 'hemi-L' and 'hemi-R' that is common between col_L and col_R\n",
    "        hemi_L_idx = col_L.find('hemi-L_') + len('hemi-L_')\n",
    "        hemi_R_idx = col_R.find('hemi-R_') + len('hemi-R_')\n",
    "        commonName = col_L[hemi_L_idx:]\n",
    "        print(f\"\\n[ctx] Processing {commonName}... (cols: {col_L} {col_R})\")\n",
    "        \n",
    "        df_tmp = df_pths_clean.dropna(subset=[col_L, col_R]) # remove IDs with missing values in col_L or col_R\n",
    "        if verbose: \n",
    "            print(f\"\\t{len(df_pths_clean) - len(df_tmp)} rows removed due to missing values for these maps. [{(len(df_pths_clean))} rows before, {len(df_tmp)} rows remain]\")\n",
    "        # Remove participants who do not have data for all studies in this analysis\n",
    "        required_studies = [s['study'] for s in studies]\n",
    "        participant_counts = df_tmp.groupby('MICS_ID')['study'].nunique()\n",
    "        valid_ids = participant_counts[participant_counts == len(required_studies)].index.tolist()\n",
    "        df_tmp_drop = df_tmp[~df_tmp['MICS_ID'].isin(valid_ids)].copy()\n",
    "        df_tmp = df_tmp[df_tmp['MICS_ID'].isin(valid_ids)]\n",
    "\n",
    "        n_before = df_pths_clean['MICS_ID'].nunique()\n",
    "        n_after = df_tmp['MICS_ID'].nunique()\n",
    "        n_removed = n_before - n_after\n",
    "        print(f\"\\t{n_after} unique patients remain after removing {n_removed} IDs due to incomplete study.\")\n",
    "        if verbose:\n",
    "            if n_removed > 0:\n",
    "                print(f\"\\tIDs removed: {sorted(df_tmp_drop['MICS_ID'].unique())}\")\n",
    "\n",
    "        # TO DO: extract only vars relevant for statistics (eg., grp, w-scoring values)\n",
    "        for study in studies:\n",
    "            study_name = study['name']\n",
    "            study_code = study['study']\n",
    "            \n",
    "            col_ID = tsutil.get_IDCol(study_name, demographics) # determine ID col name based on study name\n",
    "            #print(f\"ID col name: {col_ID}\")\n",
    "            \n",
    "            df_tmp_study = df_tmp[df_tmp['study'] == study_code]\n",
    "            print(f\"\\t[{study_code}] {len(df_tmp_study)} rows\")\n",
    "\n",
    "            # extract maps\n",
    "            maps = get_maps(df_tmp_study, mapCols=[col_L, col_R], col_grp = \"grp_detailed\", col_ID = col_ID)\n",
    "\n",
    "            # add to dict list\n",
    "            surf = col_L.split('surf-')[1].split('_label')[0]\n",
    "            lbl = col_L.split('_label-')[1].split('_')[0]\n",
    "            if lbl == 'thickness':\n",
    "                ft = 'thickness'\n",
    "            else:\n",
    "                ft = col_L.split('_label-')[1].split('_')[1]\n",
    "            smth = col_L.split('_smth-')[1].split('mm')[0]\n",
    "\n",
    "            map_dictlist.append({\n",
    "                'study': study_name,\n",
    "                'region': 'cortex',\n",
    "                'surf': surf,\n",
    "                'label': lbl,\n",
    "                'feature': ft,\n",
    "                'smth': smth,\n",
    "                'df_demo': df_tmp_study,\n",
    "                'df_maps': maps,\n",
    "            })\n",
    "\n",
    "if specs['hipp']:\n",
    "    \n",
    "    hipp_cols_L = [col for col in map_cols_L if 'hipp' in col or 'hippocampus' in col]\n",
    "    hipp_cols_R = [col for col in map_cols_R if 'hipp' in col or 'hippocampus' in col]\n",
    "\n",
    "    if hipp_cols_L == [] or hipp_cols_R == []:\n",
    "        print(\"\\n[WARNING] No hippocampal map columns found. Skipping hippocampal analyses.\")\n",
    "    else:\n",
    "        print(f\"\\n{len(hipp_cols_L) + len(hipp_cols_R)} hippocampal map columns found.\")\n",
    "\n",
    "    for col_L, col_R in zip(hipp_cols_L, hipp_cols_R):\n",
    "        \n",
    "        assert col_L.replace('hemi-L', '') == col_R.replace('hemi-R', ''), f\"Left and right hemisphere columns do not match: {col_L}, {col_R}\"\n",
    "        \n",
    "        # Find the substring after 'hemi-L' and 'hemi-R' that is common between col_L and col_R\n",
    "        hemi_L_idx = col_L.find('hemi-L_') + len('hemi-L_')\n",
    "        hemi_R_idx = col_R.find('hemi-R_') + len('hemi-R_')\n",
    "        commonName = col_L[hemi_L_idx:]\n",
    "        print(f\"\\n[hipp] Processing {commonName}... (cols: {col_L} {col_R})\")\n",
    "        \n",
    "        # TO DO: extract only vars relevant for statistics (eg., grp, w-scoring values)\n",
    "        df_tmp = df_pths_clean.dropna(subset=[col_L, col_R]) # remove IDs with missing values in col_L or col_R\n",
    "        if verbose: \n",
    "            print(f\"\\t{len(df_pths_clean) - len(df_tmp)} rows removed due to missing values for these maps. [{(len(df_pths_clean))} rows before, {len(df_tmp)} rows remain]\")\n",
    "        \n",
    "        # Remove participants who do not have data for all studies in this analysis\n",
    "        required_studies = [s['study'] for s in studies]\n",
    "        participant_counts = df_tmp.groupby('MICS_ID')['study'].nunique()\n",
    "        valid_ids = participant_counts[participant_counts == len(required_studies)].index.tolist()\n",
    "        df_tmp_drop = df_tmp[~df_tmp['MICS_ID'].isin(valid_ids)].copy()\n",
    "        df_tmp = df_tmp[df_tmp['MICS_ID'].isin(valid_ids)]\n",
    "\n",
    "        n_before = df_pths_clean['MICS_ID'].nunique()\n",
    "        n_after = df_tmp['MICS_ID'].nunique()\n",
    "        n_removed = n_before - n_after\n",
    "        print(f\"\\t{n_after} unique patients remain after removing {n_removed} IDs due to incomplete study.\")\n",
    "        if verbose:\n",
    "            if n_removed > 0:\n",
    "                print(f\"\\tIDs removed: {sorted(df_tmp_drop['MICS_ID'].unique())}\")\n",
    "\n",
    "        for study in studies:\n",
    "            study_name = study['name']\n",
    "            study_code = study['study']\n",
    "            \n",
    "            col_ID = tsutil.get_IDCol(study_name, demographics) # determine ID col name based on study name\n",
    "            #print(f\"ID col name: {col_ID}\")\n",
    "\n",
    "            df_tmp_study = df_tmp[df_tmp['study'] == study_code]\n",
    "            print(f\"\\t[{study_code}] {len(df_tmp_study)} rows\")\n",
    "\n",
    "            # extract maps\n",
    "            maps = get_maps(df_tmp_study, mapCols=[col_L, col_R], col_grp = \"grp_detailed\", col_ID = col_ID)\n",
    "\n",
    "            # add to dict list\n",
    "            surf = col_L.split('surf-')[1].split('_label')[0]\n",
    "            lbl = col_L.split('_label-')[1].split('_')[0]\n",
    "            if lbl == 'thickness':\n",
    "                ft = 'thickness'\n",
    "            else:\n",
    "                ft = col_L.split('_label-')[1].split('_')[1]\n",
    "            smth = col_L.split('_smth-')[1].split('mm')[0]\n",
    "\n",
    "            map_dictlist.append({\n",
    "                'study': study_name,\n",
    "                'region': 'hippocampus',\n",
    "                'surf': surf,\n",
    "                'label': lbl,\n",
    "                'feature': ft,\n",
    "                'smth': smth,\n",
    "                'df_demo': df_tmp_study,\n",
    "                'df_maps': maps,\n",
    "            })\n",
    "\n",
    "print(f\"\\n[main] {len(map_dictlist)} dictionary items created for each study-feature-label-surface-smoothing pair.\")\n",
    "\n",
    "# save\n",
    "## Note. These are the firs dict lists produced and thus, get prefix 01\n",
    "if save:\n",
    "    if test: save_name = f\"TEST_{save_name}\"\n",
    "    out_pth = f\"{specs['prjDir_root']}{specs['prjDir_outs']}/{save_name}_{datetime.datetime.now().strftime('%d%b%Y-%H%M%S')}.pkl\"\n",
    "    # save\n",
    "    with open(out_pth, 'wb') as f:\n",
    "        pickle.dump(map_dictlist, f)\n",
    "    print(f\"[main] Saved: {out_pth}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cdfbe6",
   "metadata": {},
   "source": [
    "# Within study, vertex-wise statistics (z-, w- scores)\n",
    "- compares _all_ participants to controls \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337fce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "pth = \"/host/verges/tank/data/daniel/3T7T/z/outputs/\"\n",
    "file = \"/01_maps_09Sep2025-162453.pkl\"\n",
    "\n",
    "with open(pth + file, \"rb\") as f:\n",
    "    dl = pickle.load(f)\n",
    "    \n",
    "tsutil.print_dict(dl, df_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d952e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics\n",
    "import time\n",
    "importlib.reload(tsutil)\n",
    "\n",
    "col_grp = 'grp_detailed'  # column in df_demo with group labels\n",
    "# grp = [\"TLE_L\", \"TLE_R\"]\n",
    "grp = None\n",
    "z = True\n",
    "w = True\n",
    "verbose = True\n",
    "toPrint = False\n",
    "save = True\n",
    "test = False\n",
    "save_pth = specs['prjDir_root'] + specs['prjDir_outs']\n",
    "save_name = \"02_stats\"\n",
    "covars = [demographics['age'], demographics['sex']]\n",
    "\n",
    "ctrl_values = [val for sublist in ctrl_grp.values() for val in sublist]\n",
    "\n",
    "if test:\n",
    "    idx_len = 2 # number of indices\n",
    "    idx = np.random.choice(len(dl), size=idx_len, replace=False).tolist()  # randomly choose index\n",
    "    dl_iterate = [dl[i] for i in idx]\n",
    "    print(f\"[TEST MODE] Running z-scoring on {idx_len} randomly selected dict items: {idx}\")\n",
    "else:\n",
    "    dl_iterate = dl.copy()  # Create a copy of the original list to iterate over\n",
    "\n",
    "for i, item in enumerate(dl_iterate): # can be parallelized\n",
    "    \n",
    "    study = item['study']\n",
    "    region = item['region']\n",
    "    surf = item['surf']\n",
    "    label = item['label']\n",
    "    feature = item['feature']\n",
    "    smth = item['smth']\n",
    "    \n",
    "    col_ID = tsutil.get_IDCol(study, demographics) # determine ID col name based on study name\n",
    "    if test:\n",
    "        print(f\"\\n[{study}] {region}: {feature}, {surf}, {label}, {smth}mm (idx {idx[i]})\")\n",
    "    else:\n",
    "        print(f\"\\n[{study}] {region}: {feature}, {surf}, {label}, {smth}mm (idx {i})\")\n",
    "    demo = item[f'df_demo'].copy() # contains all participants\n",
    "    maps = item[f'df_maps'].copy() # contains all participants\n",
    "    if verbose: print(f\"\\tInput shapes:\\t\\t[demo] {demo.shape} | [maps] {maps.shape}\")\n",
    "    \n",
    "    # use ID_SES as index\n",
    "    col_ID = tsutil.get_IDCol(study, demographics)\n",
    "    col_SES = demographics['SES']\n",
    "\n",
    "    demo['ID_SES'] = demo[col_ID].astype(str) + '_' + demo['SES'].astype(str) # concat ID and SES into single col \n",
    "    demo.set_index('ID_SES', inplace=True)\n",
    "\n",
    "    maps['ID_SES'] = maps[col_ID].astype(str) + '_' + maps['SES'].astype(str) # concat ID and SES into single col\n",
    "    maps.set_index('ID_SES', inplace=True)\n",
    "\n",
    "    # Ensure that covariates are properly formatted, and that all participants have data for all covariates\n",
    "    covars_copy = covars.copy()\n",
    "    for c in list(covars_copy): \n",
    "        if c not in demographics.keys(): # ensure exist in demographics dict (thus there is accurate mapping to df col name)\n",
    "            print(f\"\\tWARNING. Covariate '{c}' not a key in the demographics dictionary. Skipping this covar.\")\n",
    "            covars_copy.remove(c)\n",
    "            continue\n",
    "        if c not in demo.columns: # ensure exists in demo dataframe\n",
    "            print(f\"\\tWARNING. Covariate '{c}' not found in demographics dataframe. Skipping this covar.\")\n",
    "            covars_copy.remove(c)\n",
    "            continue\n",
    "\n",
    "    # convert covar cols to numeric or dummy code if categorical\n",
    "    exclude_cols = [col for col in demo.columns if col not in covars_copy]  # exclude all columns but covars\n",
    "    \n",
    "    demo_numeric, log = tsutil.catToDummy(demo, exclude_cols = exclude_cols)\n",
    "    # TO DO: append log to log file\n",
    "\n",
    "    missing_idx = []\n",
    "    if w and (covars_copy == [] or covars_copy is None):\n",
    "        print(\"[WARNING] No valid covariates specified. Skipping w-scoring.\")\n",
    "        w = False\n",
    "        demo_num = demo_numeric.copy() # keep all rows in demo_numeric\n",
    "    else:\n",
    "        # remove rows with missing covariate data regardless of the statistic to compute\n",
    "        covar_cols = [demographics[c] for c in covars_copy]\n",
    "        if grp: # add grouping variable back\n",
    "            covar_cols.append(col_grp)\n",
    "        demo_num = demo_numeric.loc[:, covar_cols].copy() # keep only covariate columns in demo dataframe\n",
    "    \n",
    "        # remove cases with missing covar data\n",
    "        missing_cols = demo_num.columns[demo_num.isnull().any()]\n",
    "        if len(missing_cols) > 0:\n",
    "            # count total number of cases with missing data\n",
    "            num_missing_total = demo_num.isnull().any(axis=1).sum()\n",
    "            missing_idx = demo_num.index[demo_num.isnull().any(axis=1)].tolist()\n",
    "            print(\"\\tIndices with missing covariate values:\", missing_idx)\n",
    "            demo_num_clean = demo_num.dropna().copy()\n",
    "            maps_clean = maps.loc[demo_num_clean.index, :].copy()\n",
    "        else:\n",
    "            missing_idx = []\n",
    "            demo_num_clean = demo_num.copy()\n",
    "            maps_clean = maps.copy()\n",
    "    \n",
    "    if w and demo_num_clean.shape[0] < 5:\n",
    "        print(\"WARNING. Skipping w-scoring, ≤5 controls.\")\n",
    "        w = False\n",
    "    \n",
    "    # A. Create subsets\n",
    "    # Exclude indices with missing covariate data from controls\n",
    "    ids_ctrl = [j for j in demo[demo[col_grp].isin(ctrl_values)].index if j not in missing_idx]\n",
    "    demo_ctrl = demo_num_clean.loc[ids_ctrl].copy() # extract indices from demo_num_clean\n",
    "    maps_ctrl = maps_clean.loc[ids_ctrl].copy() # extract indices from maps_clean\n",
    "\n",
    "    if verbose: print(f\"\\tControl group shapes:\\t[demo] {demo_ctrl.shape} | [maps] {maps_ctrl.shape}\")\n",
    "    \n",
    "    if grp is not None and len(grp) > 0: # select only participants in specified group\n",
    "        # Exclude indices in missing_idx from being extracted\n",
    "        valid_idx = [j for j in demo_num_clean[demo_num_clean[col_grp].isin(grp)].index if j not in missing_idx]\n",
    "        demo_test = demo_num_clean.loc[valid_idx].copy()\n",
    "        maps_test = maps_clean.loc[valid_idx].copy()\n",
    "        print(f\"\\tTest group restricted to {grp}.\")\n",
    "\n",
    "    else: # default is to include all participants (controls and patients)\n",
    "        demo_test = demo_num_clean.copy()\n",
    "        maps_test = maps_clean.copy()\n",
    "\n",
    "    if col_grp in demo_num_clean.columns:\n",
    "            demo_num_clean.drop(columns=[col_grp], inplace=True)\n",
    "\n",
    "    if verbose: print(f\"\\tTest group shapes:\\t[demo] {demo_test.shape} | [maps] {maps_test.shape}\")\n",
    "    \n",
    "    demo_ctrl = demo_numeric.loc[demo_ctrl.index, :].copy() # keep only rows in demo_ctrl\n",
    "    demo_test = demo_numeric.loc[demo_test.index, :].copy() # keep only rows in demo_test\n",
    "\n",
    "    if not test:\n",
    "        dl[i][f'ctrl_IDs'] = maps_ctrl[[col_ID, 'SES']].reset_index(drop=True) # add to output dictionary item\n",
    "    else: # if test, change dl_iterate but not dl. Must use idx list to find correct index in dl \n",
    "        dl[idx[i]][f'ctrl_IDs'] = maps_ctrl[[col_ID, 'SES']].reset_index(drop=True)\n",
    "    \n",
    "    # B. Calculate statistics    \n",
    "    # b.i. Prepare output dataframes\n",
    "    df_out = maps_test[[col_ID, 'SES']].copy()\n",
    "    df_out['ID_SES'] = df_out[col_ID].astype(str) + '_' + df_out['SES'].astype(str) # concat ID and SES into single col\n",
    "    df_out.set_index('ID_SES', inplace=True)\n",
    "    df_out.drop([col_ID, 'SES'], axis=1, inplace=True)\n",
    "    map_cols = [col for col in maps_clean.columns if col not in [col_ID, 'SES']] # extract map cols (each col represents single vertex)\n",
    "    df_out = pd.DataFrame(index=df_out.index, columns=map_cols) \n",
    "    if verbose: \n",
    "        print(f\"\\tOutput shape:\\t\\t[map stats] {df_out.shape}\")\n",
    "    \n",
    "    if z and demo_ctrl.shape[0] > 3:\n",
    "        print(f\"\\tComputing z scores [{demo_ctrl.shape[0]} controls]...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        z_scores = tsutil.get_z(x = maps_test[map_cols], ctrl = maps_ctrl[map_cols])\n",
    "        if not test:\n",
    "            dl[i]['df_z'] = z_scores\n",
    "        else: dl[idx[i]]['df_z'] = z_scores\n",
    "        duration = time.time() - start_time\n",
    "        print(f\"\\t\\tZ-scores computed in {int(duration // 60):02d}:{int(duration % 60):02d} (mm:ss).\")\n",
    "\n",
    "    elif z:\n",
    "        if not test:\n",
    "            dl[i]['df_z'] = None\n",
    "        else: dl[idx[i]]['df_z'] = None\n",
    "        print(\"\\tWARNING. Skipping z-score: ≤2 controls.\")\n",
    "\n",
    "    if w and demo_ctrl.shape[0] > 5 * len(covars_copy): #  do not perform if fewer than 5 controls per covariate\n",
    "        print(f\"\\tComputing w scores [{demo_ctrl.shape[0]} controls, {len(covars_copy)} covars]...\")\n",
    "        start_time = time.time()\n",
    "        if demo_ctrl.shape[0] < 10 * len(covars_copy):\n",
    "            print(f\"\\t\\tWARNING. INTERPRET WITH CAUTION: Few participants for number of covariates. Linear regression likely to be biased.\")\n",
    "        \n",
    "        df_w_models = pd.DataFrame(\n",
    "            index=['intercept'] + [str(c) for c in covars_copy],\n",
    "            columns=map_cols\n",
    "        )\n",
    "\n",
    "        df_w_out = df_out.copy() # n row by p map cols\n",
    "        \n",
    "        df_w_out, w_models = tsutil.get_w(map_ctrl = maps_ctrl[map_cols], demo_ctrl=demo_ctrl, map_test = maps_test[map_cols], demo_test = demo_test, covars=covars_copy)\n",
    "        if not test:\n",
    "            dl[i]['df_w'] = df_w_out\n",
    "            dl[i]['df_w_models'] = w_models\n",
    "        else: \n",
    "            dl[idx[i]]['df_w'] = df_w_out\n",
    "            dl[idx[i]]['df_w_models'] = w_models\n",
    "        duration = time.time() - start_time\n",
    "        print(f\"\\t\\tW-scores computed in {int(duration // 60):02d}:{int(duration % 60):02d} (mm:ss).\")\n",
    "\n",
    "    elif w:\n",
    "        if not test:\n",
    "            dl[i]['df_w'] = None\n",
    "            dl[i]['df_w_models'] = None\n",
    "        else:\n",
    "            dl[idx[i]]['df_w'] = None\n",
    "            dl[idx[i]]['df_w_models'] = None\n",
    "        print(f\"\\tWARNING. Skipping w-scoring, ≤{5 * len(covars_copy)} controls (5 * number of covars).\\n\\t\\tInsufficient number of controls for number of covariates. [{demo_ctrl.shape[0]} controls, {len(covars_copy)} covars].\\n\\t\\tGuidelines suggest at least 5-10 controls per covariate to ensure stable regression estimates.\")\n",
    "\n",
    "# Save the updated map_dictlist to a pickle file\n",
    "if save:\n",
    "    date = datetime.datetime.now().strftime(\"%d%b%Y-%H%M%S\")\n",
    "    if test:\n",
    "        save_name = f\"TEST_{save_name}\"\n",
    "    out_pth = f\"{save_pth}/{save_name}_{date}.pkl\"\n",
    "    with open(out_pth, \"wb\") as f:\n",
    "        pickle.dump(dl_iterate, f)\n",
    "    print(f\"Saved map_dictlist with z-scores to {out_pth}\")\n",
    "\n",
    "if toPrint:\n",
    "    try:\n",
    "        if test:\n",
    "            tsutil.print_dict(dl, df_print=False, idx=idx)\n",
    "        else:\n",
    "            tsutil.print_dict(dl)\n",
    "    except:\n",
    "        print(dl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4177c5b1",
   "metadata": {},
   "source": [
    "# a. Select groups of interest only\n",
    "# b. Ipsi/contra Flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edf6182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list for R sided\n",
    "def search_df(df, ptrn, out_cols, search_col='grp_detailed', searchType='end'):\n",
    "    \"\"\"\n",
    "    Return values from 'out_cols' for rows whose search_col matches 'value' based on searchType\n",
    "\n",
    "    Input:\n",
    "        df: DataFrame to search in, with columns search_col and out_col.\n",
    "        value: Value to search for in the search_col.\n",
    "        out_cols: Column name(s) to return the values from. Can be a string or list of strings.\n",
    "            If list, values will be joined with '_'.\n",
    "        searchType: Type of search. Options:\n",
    "            'end' - search for values that end with the specified value,\n",
    "            'begin' - search for values that begin with the specified value.\n",
    "            'contains' - search for values that contain the specified value.\n",
    "        search_col: Column to search for the value in.\n",
    "\n",
    "\n",
    "    Output:\n",
    "        out: List of out_col values where search_col matches the specified criteria.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    # Ensure out_col is a list\n",
    "    if isinstance(out_cols, str):\n",
    "        out_cols = [out_cols]\n",
    "    else:\n",
    "        out_cols = out_cols\n",
    "\n",
    "    # Check that all required columns exist in the dataframe\n",
    "    required_cols = [search_col] + out_cols\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"[search_df] Error: Missing columns {missing_cols} in dataframe\")\n",
    "        return []\n",
    "\n",
    "    # Filter for pattern in search_col based on searchType\n",
    "    if searchType.lower() == 'end':\n",
    "        filtered_df = df[df[search_col].str.endswith(ptrn)]\n",
    "    elif searchType.lower() == 'begin':\n",
    "        filtered_df = df[df[search_col].str.startswith(ptrn)]\n",
    "    elif searchType.lower() == 'contains':\n",
    "        filtered_df = df[df[search_col].str.contains(ptrn)]\n",
    "    else:\n",
    "        print(f\"[search_df] Warning: searchType `{searchType}` not recognized. Use 'end', 'begin', or 'contains'.\")\n",
    "        return []\n",
    "\n",
    "    # Get values from out_col(s)\n",
    "    if len(out_cols) == 1:\n",
    "        out = filtered_df[out_cols[0]]\n",
    "    else:\n",
    "        # Join multiple columns with '_'\n",
    "        out = filtered_df[out_cols].astype(str).apply('_'.join, axis=1)\n",
    "    \n",
    "    return out.unique().tolist()\n",
    "\n",
    "def toIC(df_r, df_l):\n",
    "    \"\"\"\n",
    "    Take in two dataframes, one for patients with L sided pathology, one for R sided pathology.\n",
    "    Dataframes hold vertex numbers and end with _L or _R to indicate hemisphere.\n",
    "\n",
    "    Return dataframe with ipsi and contra columns.\n",
    "    out shape: (n_r + n_l) x n_vertices [assuming both inputs have identical column names]\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # if pathology on R then col names ending with _R --> ipsi, _L --> contra\n",
    "    df_r_ic = df_r.rename(columns=lambda x: x.replace('_R', '_ipsi').replace('_L', '_contra') if x.endswith('_R') or x.endswith('_L') else x)\n",
    "    # if pathology on L then col names ending with _L --> ipsi, _R --> contra\n",
    "    df_l_ic = df_l.rename(columns=lambda x: x.replace('_L', '_ipsi').replace('_R', '_contra') if x.endswith('_L') or x.endswith('_R') else x)\n",
    "    \n",
    "    df_ic = pd.concat([df_r_ic, df_l_ic], axis=0) # concatenate both dataframes\n",
    "\n",
    "    return df_ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c56f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dictionary list based on previous dl.\n",
    "# New dl will have the same number of dictionary items (one for each study, ft, label, surf, smth, region combination).\n",
    "#   Keys of each dictionary items may change. One df for each combination of [group[len(goi)] x lateralization[_R, _L, _ic] + 1 (ctrl)] x stat[<_z>, <_w>]] \n",
    "#   If df_{stat} is none, nothing regarding this statistic will be added to dict item.\n",
    "\n",
    "importlib.reload(tsutil)\n",
    "\n",
    "save = True\n",
    "test = False\n",
    "verbose = True\n",
    "toPrint = True\n",
    "\n",
    "save_pth = specs['prjDir_root'] + specs['prjDir_outs']\n",
    "save_name = f\"03_stats_grp\"\n",
    "\n",
    "col_grp = 'grp_detailed'\n",
    "goi = [\"TLE\"] # group(s) of interest. Store main diagnosis abrev in list to allow for multiple groups\n",
    "\n",
    "\n",
    "# import\n",
    "pth = \"/host/verges/tank/data/daniel/3T7T/z/outputs/02_stats_10Sep2025-091747.pkl\"\n",
    "\n",
    "with open(pth, \"rb\") as f:\n",
    "    dl = pickle.load(f)\n",
    "    \n",
    "if toPrint:\n",
    "    tsutil.print_dict(dl)\n",
    "    print('-'*100)\n",
    "\n",
    "\n",
    "if test:\n",
    "    idx_len = 1 # number of indices\n",
    "    idx = np.random.choice(len(dl), size=idx_len, replace=False).tolist()  # randomly choose index\n",
    "    dl_iterate = [dl[i] for i in idx]\n",
    "    dl_grp_ic = copy.deepcopy([dl[i] for i in idx])\n",
    "    \n",
    "    print(f\"[TEST MODE] Running ipsi/contra flipping on {idx_len} randomly selected dict items: {idx}\")\n",
    "else:\n",
    "    dl_iterate = dl.copy()  # Create a copy of the original list to iterate over\n",
    "    dl_grp_ic = copy.deepcopy(dl)  # Create a copy of the original list for output and to iterate over\n",
    "\n",
    "print(f\"Performing two steps:\\n\\ta. Selecting patients belonging to {goi}.\\n\\tb. Ipsi/contra flip.\")\n",
    "start_time = time.time()\n",
    "print(f\"Start time: {time.ctime()}\")\n",
    "\n",
    "\n",
    "for i, item in enumerate(dl_iterate): # safe to iterate over dl because not changing its structure\n",
    "    \n",
    "    study = item['study']\n",
    "    region = item['region']\n",
    "    surf = item['surf']\n",
    "    label = item['label']\n",
    "    feature = item['feature']\n",
    "    smth = item['smth']\n",
    "    demo = item[f'df_demo'].copy() # contains all participants\n",
    "\n",
    "    col_ID = tsutil.get_IDCol(study, demographics)\n",
    "    col_SES = demographics['SES']\n",
    "\n",
    "    if test:\n",
    "        print(f\"\\n[{study}] {region}: {feature}, {surf}, {label}, {smth}mm (idx {idx[i]})\")\n",
    "    else:\n",
    "        print(f\"\\n[{study}] {region}: {feature}, {surf}, {label}, {smth}mm (idx {i})\")\n",
    "\n",
    "    IDs_ctrl = item.get('ctrl_IDs', None)\n",
    "    if IDs_ctrl is not None:\n",
    "        print(col_ID, col_SES)\n",
    "        IDs_ctrl = [f\"{row[col_ID]}_{row[col_SES]}\" for _, row in IDs_ctrl.iterrows()]\n",
    "        #print(IDs_ctrl)\n",
    "    \n",
    "    for grp_val in goi: # for each group\n",
    "    \n",
    "        # extract L, R IDs\n",
    "        demo_grp = demo[demo[col_grp].str.contains(grp_val)].copy() # extract only participants in group of interest\n",
    "        IDs_right = search_df(df=demo_grp, ptrn='R', searchType='end', search_col=col_grp, out_cols=[col_ID, col_SES])\n",
    "        IDs_left = search_df(df=demo_grp, ptrn='L', searchType='end', search_col=col_grp, out_cols=[col_ID, col_SES])\n",
    "        \n",
    "        # add group IDs to output dictionary item\n",
    "        dl_grp_ic[i][f'{grp_val}_IDs_R'] = pd.DataFrame([id.split('_') for id in IDs_right], columns=[col_ID, col_SES])\n",
    "        dl_grp_ic[i][f'{grp_val}_IDs_L'] = pd.DataFrame([id.split('_') for id in IDs_left], columns=[col_ID, col_SES])\n",
    "        \n",
    "        print(f\"\\tCtrl: {len(IDs_ctrl)}\")\n",
    "        if verbose: print(f\"\\t\\tIDs Ctrl: {IDs_ctrl}\")\n",
    "        print(f\"\\tGroup {grp_val}: {len(IDs_left)} L | {len(IDs_right)} R\")\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\t\\tIDs L: {IDs_left}\\n\\t\\tIDs R: {IDs_right}\")\n",
    "        \n",
    "        # Create df_{stat} for each side\n",
    "        df_z = item.get('df_z')\n",
    "        df_w = item.get('df_w')\n",
    "\n",
    "        if df_z is not None:\n",
    "            df_z_grp = df_z.copy()\n",
    "            # search indexes of df_z for values in IDs_right. No need to split by SES, as df_z index is ID_SES\n",
    "            df_z_r = df_z_grp[df_z_grp.index.isin(IDs_right)]\n",
    "            df_z_l = df_z_grp[df_z_grp.index.isin(IDs_left)]\n",
    "            df_z_ic = toIC(df_r = df_z_r, df_l = df_z_l)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"\\t\\tShapes of df_z: L {df_z_l.shape} | R {df_z_r.shape} | IC {df_z_ic.shape}\")\n",
    "\n",
    "            dl_grp_ic[i][f'df_z_{grp_val}_R'] = df_z_r\n",
    "            dl_grp_ic[i][f'df_z_{grp_val}_L'] = df_z_l\n",
    "            dl_grp_ic[i][f'df_z_{grp_val}_ic'] = df_z_ic\n",
    "\n",
    "            if dl_grp_ic[i].get('df_z_ctrl', None) is None: # if ctrl df not yet created, create it\n",
    "                df_z_ctrl = df_z_grp[df_z_grp.index.isin(IDs_ctrl)]\n",
    "                dl_grp_ic[i]['df_z_ctrl'] = df_z_ctrl\n",
    "                if verbose:\n",
    "                    print(f\"\\t\\tControl group df_z shape: {df_z_ctrl.shape}\")\n",
    "        else:\n",
    "            \"\"\"Removed below to avoid having None items\n",
    "            dl_grp_ic[i][f'df_z_{grp_val}_R'] = None\n",
    "            dl_grp_ic[i][f'df_z_{grp_val}_L'] = None\n",
    "            dl_grp_ic[i][f'df_z_{grp_val}_ic'] = None\n",
    "            dl_grp_ic[i]['df_z_ctrl'] = None\n",
    "            \"\"\"\n",
    "\n",
    "        if df_w is not None:\n",
    "            df_w_grp = df_w.copy()\n",
    "            \n",
    "            df_w_r = df_w_grp[df_w_grp.index.isin(IDs_right)]\n",
    "            df_w_l = df_w_grp[df_w_grp.index.isin(IDs_left)]\n",
    "            df_w_ic = toIC(df_r = df_w_r, df_l = df_w_l)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"\\t\\tShapes of df_z: L {df_w_l.shape} | R {df_w_r.shape} | IC {df_w_ic.shape}\")\n",
    "\n",
    "            dl_grp_ic[i][f'df_w_{grp_val}_R'] = df_z_r\n",
    "            dl_grp_ic[i][f'df_w_{grp_val}_L'] = df_z_l\n",
    "            dl_grp_ic[i][f'df_w_{grp_val}_ic'] = df_z_ic\n",
    "            \n",
    "            if dl_grp_ic[i].get('df_w_ctrl', None) is None: # if ctrl df not yet created, create it\n",
    "                df_w_ctrl = df_w_grp[df_w_grp.index.isin(IDs_ctrl)]\n",
    "                dl_grp_ic[i]['df_w_ctrl'] = df_w_ctrl\n",
    "                if verbose:\n",
    "                    print(f\"\\t\\tControl group df_z shape: {df_w_ctrl.shape}\")\n",
    "        else:\n",
    "            \"\"\" Removed below to avoid having None items\n",
    "            dl_grp_ic[i][f'df_w_{grp_val}_R'] = None\n",
    "            dl_grp_ic[i][f'df_w_{grp_val}_L'] = None\n",
    "            dl_grp_ic[i][f'df_w_{grp_val}_ic'] = None\n",
    "            dl_grp_ic[i]['df_w_ctrl'] = None\n",
    "            \"\"\"\n",
    "\n",
    "duration = time.time() - start_time\n",
    "print(f\"Completed in {int(duration // 60):02d}:{int(duration % 60):02d} (mm:ss).\")\n",
    "\n",
    "# Save the updated map_dictlist to a pickle file\n",
    "if save:\n",
    "    if test:\n",
    "        save_name = f\"TEST_{save_name}\"\n",
    "    date = datetime.datetime.now().strftime(\"%d%b%Y-%H%M%S\")\n",
    "    out_pth = f\"{save_pth}/{save_name}_{date}.pkl\"\n",
    "   \n",
    "    with open(out_pth, \"wb\") as f:\n",
    "        pickle.dump(dl_grp_ic, f)\n",
    "    print(f\"Saved dictlist with groups and ipsi/contra statistics dfs to {out_pth}\")\n",
    "\n",
    "if toPrint:\n",
    "    print('-'*100)\n",
    "    try:\n",
    "        if test:\n",
    "            tsutil.print_dict(dl_grp_ic, df_print=False, idx=idx)\n",
    "        else:\n",
    "            tsutil.print_dict(dl_grp_ic)\n",
    "    except:\n",
    "        print(dl_grp_ic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a922f84",
   "metadata": {},
   "source": [
    "# Within study Cohen's D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5152f874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate d-score\n",
    "# define list of grps of interest\n",
    "# define list of ctrl\n",
    "importlib.reload(tsutil)\n",
    "\n",
    "load = True\n",
    "save = True\n",
    "test = False\n",
    "verbose = False\n",
    "toPrint = True\n",
    "\n",
    "save_pth = specs['prjDir_root'] + specs['prjDir_outs']\n",
    "save_name = f\"04_dStats_grp\"\n",
    "\n",
    "ipsiTo = 'L' # what hemisphere for controls ipsi should be mapped to\n",
    "stats = ['z', 'w'] # statistics to compute d-scores for\n",
    "\n",
    "# import\n",
    "if load:\n",
    "    pth = \"/host/verges/tank/data/daniel/3T7T/z/outputs/03_stats_grp_10Sep2025-163811.pkl\"\n",
    "\n",
    "    with open(pth, \"rb\") as f:\n",
    "        dl_stats = pickle.load(f)\n",
    "        \n",
    "    tsutil.print_dict(dl_stats)\n",
    "    print('-'*100)\n",
    "\n",
    "if test:\n",
    "    idx_len = 1 # number of indices\n",
    "    idx = np.random.choice(len(dl_stats), size=idx_len, replace=False).tolist()  # randomly choose index\n",
    "    dl_iterate = [dl_stats[i] for i in idx]\n",
    "    dl = copy.deepcopy(dl_stats)\n",
    "    \n",
    "    print(f\"[TEST MODE] Running d-scoring on {idx_len} randomly selected dict items: {idx}\")\n",
    "else:\n",
    "    dl_iterate = dl_stats.copy()  # Create a copy of the original list to iterate over\n",
    "    dl = copy.deepcopy(dl_stats)  \n",
    "\n",
    "start_time = time.time()\n",
    "print(f\"Start time: {start_time}\")\n",
    "\n",
    "for i, item in enumerate(dl_iterate):\n",
    "\n",
    "    study = item['study']\n",
    "    region = item['region']\n",
    "    surf = item['surf']\n",
    "    label = item['label']\n",
    "    feature = item['feature']\n",
    "    smth = item['smth']\n",
    "    demo = item[f'df_demo'].copy() # contains all participants\n",
    "\n",
    "    col_ID = tsutil.get_IDCol(study, demographics)\n",
    "    col_SES = demographics['SES']\n",
    "    \n",
    "    if test:\n",
    "        print(f\"\\n[{study}] {region}: {feature}, {surf}, {label}, {smth}mm (idx {idx[i]})\")\n",
    "    else:\n",
    "        print(f\"\\n[{study}] {region}: {feature}, {surf}, {label}, {smth}mm (idx {i})\")\n",
    "\n",
    "    # list to save before concatenating all dfs into single df\n",
    "    d_dfs = [] \n",
    "    d_dfs_ic = []\n",
    "\n",
    "    # compare every other df to the control df, put results into a single df_{stat}_d\n",
    "    for stat in stats: # iterate over all statistics of interest [df_z, df_w]\n",
    "        \n",
    "        df_stat_key = f'df_{stat}'\n",
    "        df_stat = item.get(df_stat_key, False)\n",
    "        \n",
    "        if df_stat is False:\n",
    "            \n",
    "            print(f\"\\tWARNING. {stat} statistics dataframe does not exist in this dictionary. Skipping d-score.\")\n",
    "            continue\n",
    "        elif df_stat is None:\n",
    "            print(f\"\\tWARNING. {stat} statistic has `None` as dataframe, indicating failed computation in above steps. Skipping d-score.\")\n",
    "            continue\n",
    "        \n",
    "        df_stat_ctrl = item.get(f'df_{stat}_ctrl', None)\n",
    "        \n",
    "        if df_stat_ctrl is None:\n",
    "            print(f\"\\tWARNING. No control group df for {stat}. Skipping d-score.\")\n",
    "            continue\n",
    "        \n",
    "        # identify all other dfs for this stat\n",
    "        keys_stats = [key for key in item.keys() if key.startswith(f'df_{stat}_') and key != f'df_{stat}_ctrl' and key != f'df_{stat}_models']\n",
    "        if len(keys_stats) == 0:\n",
    "            print(f\"\\tWARNING. No group dfs for {stat}. Skipping d-score.\")\n",
    "            continue      \n",
    "        if verbose:\n",
    "            print(f\"\\t\\tDataframes computing d-scores for {keys_stats}-score\")\n",
    "        \n",
    "        # create df_stat_ctrl_ic\n",
    "        df_stat_ctrl_ic = df_stat_ctrl.copy()\n",
    "        if ipsiTo == 'L': # TO DO: use tsutil.ipsi_contra for this\n",
    "            if verbose:\n",
    "                print(f\"\\t\\tMapping controls ipsi to left hemisphere.\")\n",
    "            df_stat_ctrl_ic.columns = [col.replace('_R', '_contra') if '_R' in col else col.replace('_L', '_ipsi') for col in df_stat_ctrl.columns]\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"\\t\\tMapping controls ipsi to right hemisphere.\")\n",
    "            df_stat_ctrl_ic.columns = [col.replace('_L', '_contra') if '_L' in col else col.replace('_R', '_ipsi') for col in df_stat_ctrl.columns]\n",
    "\n",
    "        print(f\"\\tCalculating d-scores for {stat}...\")\n",
    "        if verbose:\n",
    "            print(f\"\\t\\t{stat}-score control group shape {df_stat_ctrl.shape}.\")\n",
    "        # initialize output dfs\n",
    "        d_df = pd.DataFrame(columns=df_stat_ctrl.columns)\n",
    "        d_df_ic = pd.DataFrame(columns=df_stat_ctrl_ic.columns)\n",
    "\n",
    "        #  compute d scores\n",
    "        for key in keys_stats:\n",
    "            df_stat = item.get(key, None)\n",
    "            if verbose:\n",
    "                print(f\"\\t\\t{key} (shape {df_stat.shape})\")\n",
    "            else:\n",
    "                print(f\"\\t\\t{key}\")\n",
    "            \n",
    "            if df_stat is None or df_stat.shape[0] == 0:\n",
    "                print(f\"\\t\\tNo data in {key}. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            key_name = key.replace(f'df_{stat}]', '')\n",
    "\n",
    "            if 'ic' in key.lower(): # use appropriate control df with ipsi/contra labelled cols\n",
    "                key_name = f\"{key_name}_ipsiTo-{ipsiTo}\"\n",
    "                if verbose:\n",
    "                    print(f\"\\t\\tUsing control df with ipsi/contra flipped for d-score calculation.\")\n",
    "                out_ic = tsutil.get_d(ctrl = df_stat_ctrl_ic, test = df_stat, varName = stat, test_name = key_name)\n",
    "                # append out_ic to d_df_ic\n",
    "                d_df_ic = pd.concat([d_df_ic, out_ic], axis=0)\n",
    "                d_df_ic.drop_duplicates(inplace=True)\n",
    "            else:\n",
    "                out = tsutil.get_d(ctrl = df_stat_ctrl, test = df_stat, varName = stat, test_name = key_name)\n",
    "                # append out to d_df\n",
    "                d_df = pd.concat([d_df, out], axis=0)\n",
    "                d_df.drop_duplicates(inplace=True)\n",
    "            if verbose:\n",
    "                print(f\"\\t\\tD-scores computed.\")\n",
    "        d_dfs.append(d_df)\n",
    "        d_dfs_ic.append(d_df_ic)\n",
    "    \n",
    "    # concatenate all d_dfs into single df\n",
    "    if len(d_dfs) > 1:\n",
    "        d_df = pd.concat(d_dfs, axis=0)\n",
    "    elif len(d_dfs_ic) == 1:\n",
    "        d_df = d_df[0]\n",
    "    else:\n",
    "        d_df = None\n",
    "\n",
    "    if len(d_dfs_ic) > 1:\n",
    "        d_df_ic = pd.concat(d_dfs_ic, axis=0)\n",
    "    elif len(d_dfs_ic) == 1:\n",
    "        d_df_ic = d_dfs_ic[0]\n",
    "    else:\n",
    "        d_df_ic = None\n",
    "\n",
    "    # add out and out_ic to dictionary item\n",
    "    if test:\n",
    "        dl[idx[i]][f'df_d'] = d_df\n",
    "        dl[idx[i]][f'df_d_ic'] = d_df_ic\n",
    "    else:\n",
    "        dl[i][f'df_d'] = d_df\n",
    "        dl[i][f'df_d_ic'] = d_df_ic\n",
    "\n",
    "duration = time.time() - start_time\n",
    "print(f\"Completed in {int(duration // 60):02d}:{int(duration % 60):02d} (mm:ss).\")\n",
    "\n",
    "# Save the updated map_dictlist to a pickle file\n",
    "if save:\n",
    "    if test:\n",
    "        save_name = f\"TEST_{save_name}\"\n",
    "    date = datetime.datetime.now().strftime(\"%d%b%Y-%H%M%S\")\n",
    "    out_pth = f\"{save_pth}/{save_name}_{date}.pkl\"\n",
    "   \n",
    "    with open(out_pth, \"wb\") as f:\n",
    "        pickle.dump(dl, f)\n",
    "    print(f\"Saved dictlist with groups and ipsi/contra statistics dfs to {out_pth}\")\n",
    "\n",
    "if toPrint:\n",
    "    print('-'*100)\n",
    "    try:\n",
    "        if test:\n",
    "            tsutil.print_dict(dl, df_print=False, idx=idx)\n",
    "        else:\n",
    "            tsutil.print_dict(dl)\n",
    "    except:\n",
    "        print(dl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fff04cc",
   "metadata": {},
   "source": [
    "# Between study: D-score differences\n",
    "- Identify pairs of dictionary items\n",
    "- Extract d scoring statitics and compute:\n",
    "- raw d dif\n",
    "- d dif / ctrl d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbf4bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute difference betweenn vertex-wise means for all non-ctrl groups\n",
    "# save outputs in comps list\n",
    "\n",
    "importlib.reload(tsutil)\n",
    "\n",
    "load = True\n",
    "save = True\n",
    "test = False\n",
    "verbose = True\n",
    "toPrint = False\n",
    "\n",
    "save_pth = specs['prjDir_root'] + specs['prjDir_outs']\n",
    "save_name = f\"05_comps\"\n",
    "\n",
    "comps = []\n",
    "skip_idx = []\n",
    "counter = 0\n",
    "\n",
    "if load:# import\n",
    "    pth = \"/host/verges/tank/data/daniel/3T7T/z/outputs/04_dStats_grp_10Sep2025-180606.pkl\"\n",
    "    with open(pth, \"rb\") as f:\n",
    "        dl = pickle.load(f)\n",
    "        \n",
    "    #tsutil.print_dict(dl)\n",
    "    print('-'*100)\n",
    "\n",
    "dl_iterate = dl.copy()  # Create a copy of the original list to iterate over\n",
    "dl = copy.deepcopy(dl)  # Create a copy of the original list for output and to iterate over\n",
    "\n",
    "start_time = time.time()\n",
    "print(f\"Start time: {start_time}\")\n",
    "\n",
    "for i, item in enumerate(dl_iterate):\n",
    "    if i in skip_idx:\n",
    "        continue\n",
    "    else:\n",
    "        skip_idx.append(i)\n",
    "   \n",
    "    counter = counter+1\n",
    "    \n",
    "    tsutil.printItemMetadata(item, idx=i)\n",
    "    \n",
    "    idx_other = tsutil.get_pair(dl_iterate, idx = i, mtch=['region', 'surf', 'label', 'feature', 'smth'], skip_idx=skip_idx)\n",
    "    if idx_other is None:\n",
    "        print(f\"\\tNo matching index found. Skipping.\")\n",
    "        continue\n",
    "    skip_idx.append(idx_other)\n",
    "    \n",
    "    \n",
    "    item_other = dl[idx_other]\n",
    "    print(f\"pair {counter}:\")\n",
    "    tsutil.printItemMetadata(item_other, idx=idx_other)\n",
    "\n",
    "    # if df_z and df_w are None, skip\n",
    "    if item.get('df_d', None) is None and item.get('df_d_ic', None) is None:\n",
    "        print(f\"\\tNo d-score dataframes in this dictionary item. Skipping.\")\n",
    "        continue \n",
    "\n",
    "    # Initialize output item\n",
    "    out_item = {\n",
    "            'studies': (item['study'], item_other['study']),\n",
    "            'region': item['region'],\n",
    "            'feature': item['feature'],\n",
    "            'surf': item['surf'],\n",
    "            'label': item['label'],\n",
    "            'smth': item['smth'],\n",
    "        }\n",
    "    \n",
    "    ID_keys = [key for key in item.keys() if 'IDs' in key]\n",
    "    \n",
    "    keys_to_copy = ID_keys + ['df_d', 'df_d_ic']\n",
    "    if verbose:\n",
    "        print(f\"\\tCopying keys: {keys_to_copy}\")\n",
    "    for key in keys_to_copy: # add all ID_keys are their corresponding dataframes to out_item\n",
    "        out_item[key] = [item[key], item_other[key]] # stores as list of items. In case of dfs, list of dataframes\n",
    "\n",
    "    # identify which study is which (to know how to subtract)\n",
    "    if study == 'MICs':\n",
    "        item_tT = item\n",
    "        item_sT = item_other\n",
    "    else:\n",
    "        item_tT = item_other\n",
    "        item_sT = item\n",
    "    \n",
    "    for df in ['df_d', 'df_d_ic']:\n",
    "        metrics_df = None\n",
    "        df_tT = item_tT.get(df, None)\n",
    "        df_sT = item_sT.get(df, None)\n",
    "        \n",
    "        if df_tT is None or df_sT is None:\n",
    "            print(f\"\\tOne of the d-score dataframes is None. Skipping {df} comparison.\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\tComputing difference metrics for {df}...\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\t\\t3 T shape: {df_tT.shape}\\n\\t\\t7 T shape: {df_sT.shape}.\")\n",
    "        \n",
    "        assert df_tT.columns.equals(df_sT.columns) == True, f\"[comps] Columns of d-scores DataFrames do not match. Check input data. {df_tT.columns} != {df_sT.columns}\"\n",
    "\n",
    "        # Identfiy the rows that refer to d statistics and ensure that both dataframes have these rows\n",
    "        stats_tT = df_tT.index.tolist()\n",
    "        stats_sT = df_sT.index.tolist()\n",
    "        d_stats = [s for s in stats_tT if s in stats_sT and s.startswith('d_')]\n",
    "        if verbose:\n",
    "            print(f\"\\t\\tCohen's d statistic indices: {d_stats}\")\n",
    "        \n",
    "        # want to only apply operations to rows in d_stats\n",
    "        df_d_stats_tT = df_tT.loc[d_stats, :].copy()\n",
    "        df_d_stats_sT = df_sT.loc[d_stats, :].copy()\n",
    "\n",
    "        # compute difference metrics\n",
    "        d_dif = df_d_stats_sT - df_d_stats_tT # 7 T - 3 T\n",
    "        d_dif_by3T = d_dif / df_d_stats_tT\n",
    "        d_dif_by7T = d_dif / df_d_stats_sT\n",
    "        \n",
    "        # Stack the rows from each matrix into the metrics_df\n",
    "        d_dif_renamed = d_dif.copy()\n",
    "        d_dif_renamed.index = [idx + '_Δd' for idx in d_dif_renamed.index]\n",
    "        \n",
    "        d_dif_by3T_renamed = d_dif_by3T.copy()\n",
    "        d_dif_by3T_renamed.index = [idx + '_Δd_by3T' for idx in d_dif_by3T_renamed.index]\n",
    "        \n",
    "        d_dif_by7T_renamed = d_dif_by7T.copy()\n",
    "        d_dif_by7T_renamed.index = [idx + '_Δd_by7T' for idx in d_dif_by7T_renamed.index]\n",
    "\n",
    "        metrics_df = pd.concat([d_dif_renamed, d_dif_by3T_renamed, d_dif_by7T_renamed])\n",
    "\n",
    "        # Add original stats from both datasets to metrics_df\n",
    "        df_tT_renamed = df_tT.copy()\n",
    "        df_tT_renamed.index = [idx + '_3T' for idx in df_tT_renamed.index]\n",
    "        df_sT_renamed = df_sT.copy()\n",
    "        df_sT_renamed.index = [idx + '_7T' for idx in df_sT_renamed.index]\n",
    "        metrics_df = pd.concat([metrics_df, df_tT_renamed, df_sT_renamed])\n",
    "        print(f\"\\t\\tShape of metrics_df: {metrics_df.shape}\")\n",
    "        print(f\"\\t\\tIndices: {metrics_df.index}\")        \n",
    "\n",
    "        out_item[f'comps_{df}'] = metrics_df # add to output item\n",
    "    # if comps[] is None then don't add\n",
    "    comps.append(out_item) # append to dict list\n",
    "    out_item = None\n",
    "\n",
    "\n",
    "duration = time.time() - start_time\n",
    "print(f\"Completed in {int(duration // 60):02d}:{int(duration % 60):02d} (mm:ss).\")\n",
    "\n",
    "# Save the updated map_dictlist to a pickle file\n",
    "if save:\n",
    "    if test:\n",
    "        save_name = f\"TEST_{save_name}\"\n",
    "    date = datetime.datetime.now().strftime(\"%d%b%Y-%H%M%S\")\n",
    "    out_pth = f\"{save_pth}/{save_name}_{date}.pkl\"\n",
    "   \n",
    "    with open(out_pth, \"wb\") as f:\n",
    "        pickle.dump(comps, f)\n",
    "    print(f\"Saved dictlist with groups and ipsi/contra statistics dfs to {out_pth}\")\n",
    "\n",
    "if toPrint:\n",
    "    print('-'*100)\n",
    "    try:\n",
    "        if test:\n",
    "            tsutil.print_dict(comps, df_print=False, idx=idx)\n",
    "        else:\n",
    "            tsutil.print_dict(comps)\n",
    "    except:\n",
    "        print(comps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5ce4da",
   "metadata": {},
   "source": [
    "# Visaulize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107e4276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute difference betweenn vertex-wise means for all non-ctrl groups\n",
    "# save outputs in comps list\n",
    "\n",
    "importlib.reload(tsutil)\n",
    "\n",
    "load = True\n",
    "save = True\n",
    "test = False\n",
    "verbose = True\n",
    "toPrint = False\n",
    "\n",
    "save_pth = specs['prjDir_root'] + specs['prjDir_out_figs']\n",
    "save_name = f\"01_dDif\"\n",
    "\n",
    "\n",
    "if load:# import\n",
    "    pth = \"/host/verges/tank/data/daniel/3T7T/z/outputs/05_comps_11Sep2025-170446.pkl\"\n",
    "    with open(pth, \"rb\") as f:\n",
    "        comps = pickle.load(f)\n",
    "        \n",
    "    #tsutil.print_dict(comps)\n",
    "    #print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb86f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "comps[0].get('comps_df_d')\n",
    "comps[0].get('comps_df_d_ic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ea76b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(tsutil)\n",
    "tsutil.print_dict(comps, df_print = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8184708",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(tsutil)\n",
    "\n",
    "df_name = 'comps_df_d_ic'\n",
    "metric = 'm_df_z_TLE_ic_ipsiTo-L_7T'\n",
    "\n",
    "for i, item in enumerate(comps):\n",
    "\n",
    "    tsutil.printItemMetadata(item, idx=idx)\n",
    "    try:\n",
    "        tsutil.itmToVisual(\n",
    "                        item=item, \n",
    "                        df_name = df_name, \n",
    "                        metric = metric, \n",
    "                        ipsiTo = ipsiTo\n",
    "                        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing item {i}: {e}\")\n",
    "        continue\n",
    "    print(f\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747de4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(tsutil)\n",
    "item= comps[3]\n",
    "tsutil.printItemMetadata(item, idx=2)\n",
    "metric = 'm_df_w_TLE_ic_ipsiTo-L_7T'\n",
    "print(metric)\n",
    "tsutil.itmToVisual(\n",
    "                        item=item, \n",
    "                        df_name = df_name, \n",
    "                        metric = metric, \n",
    "                        ipsiTo = ipsiTo,\n",
    "                        max_val = 2\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650b35cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "item= comps[3]\n",
    "tsutil.printItemMetadata(item, idx=2)\n",
    "metric = 'm_df_w_TLE_ic_ipsiTo-L_3T'\n",
    "print(metric)\n",
    "tsutil.itmToVisual(\n",
    "                        item=item, \n",
    "                        df_name = df_name, \n",
    "                        metric = metric, \n",
    "                        ipsiTo = ipsiTo,\n",
    "                        max_val = 2\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7743148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "item= comps[3]\n",
    "df_test = item['comps_df_d_ic']\n",
    "# add to thise a row with numbers from -1 to 1 generated from a normal distribution\n",
    "import numpy as np\n",
    "df_test.loc['test'] = np.random.normal(0, 0.5, size=df_test.shape[1])\n",
    "metric = 'test'\n",
    "print(metric)\n",
    "\n",
    "tsutil.itmToVisual(\n",
    "                        item=item, \n",
    "                        df_name = df_name, \n",
    "                        metric = metric, \n",
    "                        ipsiTo = ipsiTo\n",
    "                        )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891a67f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(tsutil)\n",
    "\n",
    "tsutil.visMean(\n",
    "    comps,\n",
    "    df_name = \"comps_df_d_ic\",\n",
    "    df_metric=\"d_df_z_TLE_ic_ipsiTo-L_Δd\",\n",
    "    title=\"Mean z difference (7T-3T)\",\n",
    "    ipsiTo=\"L\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7bff29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unsmoothed map\n",
    "\n",
    "pth = \"/host/verges/tank/data/daniel/3T7T/z/outputs/05_comps_11Sep2025-170446.pkl\"\n",
    "with open(pth, \"rb\") as f:\n",
    "        comps = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85de48c3",
   "metadata": {},
   "source": [
    "# Apply parcellations and add to dict lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07ce81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(tsutil)\n",
    "\n",
    "pth_win = \"/host/verges/tank/data/daniel/3T7T/z/outputs/stats/dl_d_ic_13Jun2025-1633.pkl\"\n",
    "pth_comps = \"/host/verges/tank/data/daniel/3T7T/z/outputs/results/comps_15Jun2025-1317.pkl\"\n",
    "\n",
    "win = pd.read_pickle(pth_win)\n",
    "comps = pd.read_pickle(pth_comps)\n",
    "\n",
    "print(\"============= Within Study ===============\")\n",
    "tsutil.print_dict(win, df_print=False)\n",
    "print(\"=============== Comparison =============\")\n",
    "tsutil.print_dict(comps, df_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777ad35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "win = pd.read_pickle(pth_win)\n",
    "comps = pd.read_pickle(pth_comps)\n",
    "\n",
    "labelType = \"LLobe\"  # specify the label type for glasser parcellations\n",
    "dictlists = [win, comps]\n",
    "save_pths = [\"dl_d_ic_glsr\", \"comps_glsr\"]\n",
    "out = []\n",
    "\n",
    "for dl_it, sve_pth in zip(dictlists, save_pths):\n",
    "    items_glsr = []\n",
    "    #tsutil.print_dict(dl_it)\n",
    "    for i, item in enumerate(dl_it):\n",
    "        \n",
    "        #tsutil.print_dict(item)\n",
    "        for region in ['crtx']:\n",
    "            ipsiTo = None\n",
    "\n",
    "            # Only process Cohen's d DataFrames\n",
    "            if 'df_d_' + region + '_ic' in item:\n",
    "                df_name = f'df_d_{region}_ic'\n",
    "                df = item[df_name]\n",
    "                ipsiTo = \"L\"\n",
    "            elif 'comps_' + region in item:\n",
    "                df_name = f'comps_{region}'\n",
    "                df = item[df_name]\n",
    "                ipsiTo = \"L\"\n",
    "            else:\n",
    "                print(f\"Skipping item {i} for region {region} (no d-score DataFrame).\")\n",
    "                continue\n",
    "            #print(df.shape)\n",
    "            \n",
    "            if df is None:\n",
    "                print(f\"Skipping item {i} for region {region} as df is None.\")\n",
    "                continue\n",
    "\n",
    "            df_glasser = tsutil.apply_glasser(df, ipsiTo=ipsiTo, labelType=labelType, addHemiLbl = True)\n",
    "            print(df_glasser.shape)\n",
    "            df_glasser_mean = df_glasser.groupby(df_glasser.columns, axis=1).mean()\n",
    "            print(f\"Glasser mean shape: {df_glasser_mean.shape}\")\n",
    "\n",
    "            df_out_name = f'{df_name}_glsr_{labelType}'\n",
    "            item[df_out_name] = df_glasser_mean\n",
    "\n",
    "        items_glsr.append(item)\n",
    "    dl_it.clear()\n",
    "    dl_it.extend(items_glsr)\n",
    "\n",
    "    date = datetime.datetime.now().strftime(\"%d%b%Y-%H%M\")\n",
    "    save_path = f\"/host/verges/tank/data/daniel/3T7T/z/outputs/{sve_pth}_{date}.pkl\"\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        pickle.dump(dl_it, f)\n",
    "    print(f\"Saved dict list with glasser parcellations: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f783f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsutil.print_dict(dl_it, df_print=False, idx=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367f6f49",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0958b9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import importlib\n",
    "import tTsTGrpUtils as tsutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2430be59",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(tsutil)\n",
    "\n",
    "pth_win = \"/host/verges/tank/data/daniel/3T7T/z/outputs/dl_d_ic_glsr_15Jun2025-1340.pkl\"\n",
    "pth_btw = \"/host/verges/tank/data/daniel/3T7T/z/outputs/comps_glsr_15Jun2025-1341.pkl\"\n",
    "\n",
    "win = pd.read_pickle(pth_win)\n",
    "comps = pd.read_pickle(pth_btw)\n",
    "\n",
    "print(f\"---------WITHIN----------\")\n",
    "tsutil.print_dict(win, df_print=False)\n",
    "print(f\"\\n\\n---------COMPS----------\")\n",
    "tsutil.print_dict(comps, df_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e209f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = comps[0]['comps_crtx'].loc['dD']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088f31d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate one figure per group-lbl combination\n",
    "importlib.reload(tsutil)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import datetime\n",
    "\n",
    "save_pth = \"/host/verges/tank/data/daniel/3T7T/z/outputs/fig_stats/figs\"\n",
    "\n",
    "winstudy = win\n",
    "btwstudy = comps\n",
    "\n",
    "win_metric = 'd'\n",
    "btw_metric = 'dD_by7T'\n",
    "\n",
    "skip_indices = []\n",
    "for i, item in enumerate(winstudy):\n",
    "    \n",
    "    if item in skip_indices:\n",
    "        continue\n",
    "\n",
    "    winstudy_pair = tsutil.pairedItems(item, winstudy, mtch=['label'])\n",
    "    btwstudy_pair = tsutil.pairedItems(item, btwstudy, mtch=['label'])\n",
    "    print(f\"Pairs for ({item['label']}): [{winstudy_pair}] [{btwstudy_pair}]\")\n",
    "\n",
    "    skip_indices.extend(winstudy_pair)\n",
    "\n",
    "    figs = []\n",
    "    # generate figures for within study comparisons\n",
    "    for i in winstudy_pair:\n",
    "        fig = tsutil.vis_item(win[i], metric = win_metric, ipsiTo = 'L', save_pth=None)\n",
    "        figs.append(fig)\n",
    "\n",
    "    for i in btwstudy_pair: # generate figures for between study comparisons\n",
    "        fig = tsutil.vis_item(btwstudy[i], metric = btw_metric, ipsiTo = 'L',save_pth=None)\n",
    "        figs.append(fig)\n",
    "    \n",
    "    # add each figure as seperate page to a pdf\n",
    "    date = pd.Timestamp.now().strftime(\"%d%b%Y-%H%M\")\n",
    "    pdf_path = f\"{save_pth}/{item['label']}_{win_metric}_{btw_metric}_{date}.pdf\"\n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        for fig in figs:\n",
    "            pdf.savefig(fig)\n",
    "            plt.close(fig)\n",
    "    print(f\"\\tSaved PDF: {pdf_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b32183",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = comps[0]['comps_crtx'].loc[['dD']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483bcc4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfe6440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify paired items in zmean and comps\n",
    "# send these three items to vis_win_comp\n",
    "\n",
    "def vis_win_comp(tT_item, sT_item, comp_item, save_name=None, save_path = None, ipsiTo='L'):\n",
    "    # in three rows, plot:\n",
    "    # [0] 3T grp to ctrl : annotation 3T \\n mean z ({n} 'grp' to {n} 'ctrl')\n",
    "    # [1] 7T grp to ctrl : annotation 7T \\n mean z ({n} 'grp' to {n} 'ctrl')\n",
    "    # [2] 3T-7T grp to ctrl : z difference (7T - 3T / 3T)\n",
    "    # annotate the figures with L/R (if grp is TLE_L or TLE_R) or ipsi/contra (if grp is TLE_IC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222d773f",
   "metadata": {},
   "source": [
    "# OUTDATED below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af806701",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_pths_clean = []\n",
    "final_ses = []\n",
    "save_pth = \"/host/verges/tank/data/daniel/3T7T/z/outputs/pt\"\n",
    "\n",
    "# Step 1: Clean sessions for each item in map_pths using clean_ses\n",
    "map_pths_clean = []\n",
    "for item in map_pths:\n",
    "    df = item['map_pths']\n",
    "        # Clean sessions\n",
    "    df_clean = clean_ses(df, ID_col=\"ID\", method=\"newest\", silent=True)\n",
    "    if df_clean is not None and not df_clean.empty:\n",
    "        new_item = item.copy()\n",
    "        new_item['map_pths'] = df_clean\n",
    "        map_pths_clean.append(new_item)\n",
    "\n",
    "# save dictionary list to a pickle file\n",
    "save_cleanMaps = \"/host/verges/tank/data/daniel/3T7T/z/maps/paths\"\n",
    "date = datetime.datetime.now().strftime(\"%d%b%Y-%H%M\")\n",
    "with open(f\"{save_cleanMaps}/map_pths_{date}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(map_pths_clean, f)\n",
    "print(f\"Saved map_pths to {save_cleanMaps}/map_pths_{date}.pkl\")\n",
    "\n",
    "\n",
    "# Step 2: Return sessions used per ID\n",
    "save_finalSES = \"/host/verges/tank/data/daniel/3T7T/z/outputs/pt\"\n",
    "finalSES = get_finalSES(map_pths_clean, demo=demographics, save_pth=save_finalSES, long=True, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2717c5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dict(map_pths_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc25d60",
   "metadata": {},
   "source": [
    "# Add ipsi/contra groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f536a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ipsi-contra dict entries\n",
    "grps = [\"TLE_L\", \"TLE_R\"]\n",
    "\n",
    "ic_entries = []\n",
    "\n",
    "print(len(map_pths_clean))\n",
    "\n",
    "for study in studies:\n",
    "    print(f\"\\n[flip_TLE] Processing study {study['name']} for ipsi-contra entries\")\n",
    "    # find all entries for the groups of interest\n",
    "    index_list = [\n",
    "        i for i, item in enumerate(map_pths_clean)\n",
    "        if item['study'] == study['name'] and item['grp'] in grps\n",
    "    ]\n",
    "    print(index_list)\n",
    "    print(f\"\\n{len(index_list)} entries for groups {grps}\")\n",
    "\n",
    "    paired = set()  # to keep track of paired indices\n",
    "    loop_count = 0\n",
    "\n",
    "    for i in index_list:\n",
    "        if i in paired:\n",
    "            continue\n",
    "        else:\n",
    "            paired.add(i)\n",
    "\n",
    "        # Only pair if surface, label, and feature match\n",
    "        item_i = map_pths_clean[i]\n",
    "        grp_i = item_i['grp']\n",
    "        df_i = item_i['map_pths']\n",
    "        surface_i = item_i['surface']\n",
    "        label_i = item_i['label']\n",
    "        feature_i = item_i['feature']\n",
    "        # add grp col to DF\n",
    "        df_i['grp'] = grp_i\n",
    "\n",
    "        if grp_i == 'TLE_L':\n",
    "            grp_other = 'TLE_R'\n",
    "        elif grp_i == 'TLE_R':\n",
    "            grp_other = 'TLE_L'\n",
    "        else:\n",
    "            print(f\"Skipping item {i} with group {grp_i} (not TLE_L or TLE_R)\")\n",
    "            continue\n",
    "\n",
    "        # Find paired index with matching surface, label, feature, and opposite group\n",
    "        other_i = None\n",
    "        for j in index_list:\n",
    "            if j == i or j in paired:\n",
    "                continue\n",
    "            item_j = map_pths_clean[j]\n",
    "            if (\n",
    "                item_j['grp'] == grp_other and\n",
    "                item_j['surface'] == surface_i and\n",
    "                item_j['label'] == label_i and\n",
    "                item_j['feature'] == feature_i\n",
    "            ):\n",
    "                other_i = j\n",
    "                break\n",
    "\n",
    "        if other_i is not None:\n",
    "            item_other = map_pths_clean[other_i]\n",
    "            df_other = item_other['map_pths']\n",
    "            if df_other is not None: df_other['grp'] = grp_other\n",
    "            paired.add(other_i)\n",
    "        else:\n",
    "            item_other = None\n",
    "            grp_other = None\n",
    "            df_other = None\n",
    "\n",
    "        print(f\"\\t{loop_count} {i} & {other_i} | {item_i['label']}, {item_i['feature']}, {item_i['surface']} | {grp_i} & {grp_other}\")\n",
    "\n",
    "        out = item_i.copy()  # copy to avoid modifying original\n",
    "\n",
    "        # rename path cols\n",
    "        # Find the correct map column names for L/R\n",
    "        hemi_L = [col for col in df_i.columns if col.endswith('_L')]\n",
    "        hemi_R = [col for col in df_i.columns if col.endswith('_R')]\n",
    "\n",
    "        col_L = hemi_L[0]\n",
    "        col_R = hemi_R[0]\n",
    "\n",
    "        if grp_i == 'TLE_L':\n",
    "            df_i = df_i.rename(columns={col_L: 'pth_ipsi', col_R: 'pth_contra'})\n",
    "            if df_other is not None:\n",
    "                df_other = df_other.rename(columns={col_R: 'pth_ipsi', col_L: 'pth_contra'})\n",
    "        else:  # grp_i == 'TLE_R'\n",
    "            df_i = df_i.rename(columns={col_R: 'pth_ipsi', col_L: 'pth_contra'})\n",
    "            if df_other is not None:\n",
    "                df_other = df_other.rename(columns={col_L: 'pth_ipsi', col_R: 'pth_contra'})\n",
    "\n",
    "        # combine modified dfs\n",
    "        if df_other is not None:\n",
    "            dfs = pd.concat([df_i, df_other], ignore_index=True)\n",
    "            out['grp_labels'] = [grp_i, grp_other]\n",
    "        else:\n",
    "            dfs = df_i\n",
    "            out['grp_labels'] = [grp_i]\n",
    "\n",
    "        out['map_pths'] = dfs  # add the combined dataframe to the output item\n",
    "        out['grp'] = 'TLE_ic'  # ic for ipsi-contra\n",
    "\n",
    "        # add to the list of outputs\n",
    "        ic_entries.append(out)\n",
    "\n",
    "        loop_count += 1\n",
    "\n",
    "map_pths_clean_ic = map_pths_clean + ic_entries  # combine original entries with ipsi-contra entries\n",
    "\n",
    "# Save the updated map_pths_clean to a pickle file\n",
    "save_cleanMaps = \"/host/verges/tank/data/daniel/3T7T/z/maps/paths\"\n",
    "date = datetime.datetime.now().strftime(\"%d%b%Y-%H%M\")\n",
    "with open(f\"{save_cleanMaps}/map_pths_clean_{date}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(map_pths_clean_ic, f)\n",
    "print(f\"Saved map_pths_clean to {save_cleanMaps}/map_pths_clean_{date}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7824fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find entries with TLE_L, pial, FA, MICs\n",
    "selected = [item for item in map_pths_clean_ic if item['grp'] == 'TLE_R' and item['label'] == 'white' and item['feature'] == 'FA' and item['study'] == 'PNI']\n",
    "print_dict(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6206959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the structure of the selected items in map_pths_clean using index_list\n",
    "print_dict(map_pths_clean_ic[270:290], df_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c543b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print structure of map_pths_clean\n",
    "print_dict(map_pths_clean_ic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f633320d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print specific study-group combinations\n",
    "print_grpDF(dict=map_pths_clean_ic, grp='ctrl', study='PNI', df=\"pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb09f517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find path for specific participant\n",
    "study = \"PNI\"\n",
    "sub = \"PNC018\"\n",
    "ses = \"01\"\n",
    "\n",
    "grp_idx = \"ctrl\"\n",
    "\n",
    "entry = next((item for item in map_pths if item['study'] == study and item['grp'] == grp_idx), None)\n",
    "\n",
    "if entry is not None:\n",
    "    df = entry['map_pths']\n",
    "    row = df[(df[col_ID] == sub) & (df['SES'] == ses)]\n",
    "    if not row.empty:\n",
    "        print(row['pth_L'].values[0])\n",
    "    else:\n",
    "        print(f\"No entry found for subject {sub} and session {ses}\")\n",
    "else:\n",
    "    print(f\"No entry found for study {study} and group {grp_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578bfc9d",
   "metadata": {},
   "source": [
    "# Extract map values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078f8437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"/host/verges/tank/data/daniel/3T7T/z/maps/paths/map_pths_clean_03Jun2025-2138.pkl\", \"rb\") as f:\n",
    "    map_pths_clean_ic = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb987b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dict([map_pths_clean_ic[25]], df_print=True)# print specific study-group combinations (prints first feature-label combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018ecb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print specific study-group combinations (prints first feature-label combination)\n",
    "print_grpDF(dict=map_pths_clean_ic, grp='allPX', study='MICs', hipp = True, df=\"maps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae57c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each row is a vertex, each column is a subject-session\n",
    "## note for ipsi/contra groups, the following map values are stored under keys : maps_ipsi, maps_contra\n",
    "maps = []\n",
    "print_dict(maps)\n",
    "\n",
    "for item in map_pths_clean_ic:\n",
    "\n",
    "    #print(f\"{item['study']} - {item['grp']} - {item['feature']} - {item['label']} - {item['surface']}\")\n",
    "\n",
    "    df = item['map_pths']\n",
    "\n",
    "    # Find the ID column (use existing variable if available)\n",
    "    if 'ID_col' in globals():\n",
    "        id_col = col_ID\n",
    "    else:\n",
    "        id_col = [col for col in df.columns if 'ID' in col.upper()][0]\n",
    "\n",
    "    # identify pth columns\n",
    "    pth_cols = [col for col in df.columns if col.startswith('pth_') or col.startswith('map_')]\n",
    "    print(f\"\\t{pth_cols}\")\n",
    "\n",
    "    new_item = item.copy()  # Only copy once per item\n",
    "\n",
    "    for pth in pth_cols:\n",
    "        df_map = pd.DataFrame()\n",
    "        for i, row in df.iterrows():\n",
    "            sub = row[id_col]\n",
    "            ses = row['SES']\n",
    "\n",
    "            # extract map values\n",
    "            val = row[pth]\n",
    "            # Check for missing or invalid path\n",
    "            if pd.isnull(val) or str(val).startswith(\"ERROR\") or str(val).lower() == \"nan\":\n",
    "                print(f\"{item['study']} - {item['grp']} - {item['feature']} - {item['label']} - {item['surface']}\")\n",
    "                print(f\"\\t[main] WARNING: No map path for {sub} {ses} ({pth}): {val}\")\n",
    "                continue\n",
    "            try:\n",
    "                gii = nib.load(val)\n",
    "                map_data = gii.darrays[0].data\n",
    "            except Exception as e:\n",
    "                print(f\"\\t[main] ERROR loading {val}: {e}\")\n",
    "                continue\n",
    "\n",
    "            col_name = f\"{sub}-{ses}\"\n",
    "            df_map[col_name] = map_data\n",
    "\n",
    "        # add the dataframe to dict item if not empty\n",
    "        key_name = pth.replace('pth_', '').replace('surf_', '')\n",
    "        if not df_map.empty:\n",
    "            new_item[f'{key_name}'] = df_map\n",
    "\n",
    "    # add this item to the maps list\n",
    "    maps.append(new_item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95223c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the structure of the maps\n",
    "print_dict(maps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb681d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the unique values for the key 'grp' \n",
    "unique_grps = set(item['grp'] for item in maps)\n",
    "print(f\"Unique groups in maps: {unique_grps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b420f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dictionary item\n",
    "save_pth = \"/host/verges/tank/data/daniel/3T7T/z/maps/map_dfs/\"\n",
    "date = datetime.datetime.now().strftime(\"%d%b%Y-%H%M\")\n",
    "with open(f'{save_pth}/map_values_{date}.pkl', 'wb') as f:    pickle.dump(maps, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1305b807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print specific study-group combinations (prints first feature-label combination)\n",
    "print_grpDF(dict=maps, grp='ctrl', study='PNI', df=\"maps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64db2f2",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52f0769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_test(df1, df2, fdr=True, equal_var=False):\n",
    "    \"\"\"\n",
    "    Perform a vertex wise t-test between two DataFrames.\n",
    "    inputs:\n",
    "        df1: pd.DataFrame with vertex data for group 1. Vertices should be rows, subjects as columns.\n",
    "        df2: pd.DataFrame with vertex data for group 2.\n",
    "        fdr: whether to apply FDR correction to the p-values (default is True)\n",
    "        equal_var: whether to assume equal variance between the two groups (default is False, i.e., Welch's t-test)\n",
    "\n",
    "    outputs:\n",
    "        df with shape n_vertices x 2 . Cols:\n",
    "            't': t-statistic\n",
    "            'p': p-value\n",
    "            'p_fdr' < if fdr = True >: FDR-corrected p-value\n",
    "\n",
    "\n",
    "    Note:\n",
    "     - Assumes:\n",
    "      - Values at each vertex are normally distributed\n",
    "      - Participants are independent\n",
    "      - Unequal variance\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from scipy import stats\n",
    "    import numpy as np\n",
    "\n",
    "    if df1.shape[0] != df2.shape[0]:\n",
    "        raise ValueError(\"[t_test] DataFrames must have the same number of vertices (rows).\")\n",
    "    \n",
    "    if df1.shape[1] < 2 or df2.shape[1] < 2:\n",
    "        raise ValueError(\"[t_test] Each DataFrame must have at least two subjects for t-test. {}\".format((df1.shape, df2.shape)))\n",
    "    \n",
    "    # Initialize output DataFrame\n",
    "    out = pd.DataFrame(index=df1.index)\n",
    "    out['t'] = np.nan\n",
    "    out['p'] = np.nan\n",
    "    out['p_fdr'] = np.nan\n",
    "\n",
    "    # Perform t-test for each vertex\n",
    "    for i in range(df1.shape[0]):\n",
    "        t_stat, p_val = stats.ttest_ind(df1.iloc[i, :], df2.iloc[i, :], equal_var=equal_var)\n",
    "        out.at[i, 't'] = t_stat\n",
    "        out.at[i, 'p'] = p_val\n",
    "\n",
    "    # FDR correction if requested\n",
    "    if fdr:\n",
    "        from statsmodels.stats.multitest import multipletests\n",
    "        out['p_fdr'] = multipletests(out['p'], method='fdr_bh')[1]  # FDR correction\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def d(df1, df2):\n",
    "    \"\"\"\n",
    "    Calculate Cohen's d for each vertex between two DataFrames.\n",
    "\n",
    "    inputs:\n",
    "        df1: pd.DataFrame with vertex data for group 1. Vertices should be rows, subjects as columns.\n",
    "        df2: pd.DataFrame with vertex data for group 2.\n",
    "\n",
    "    outputs:\n",
    "        pd.DataFrame with shape n_vertices x 1. Cols:\n",
    "            'd': Cohen's d value for each vertex\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    if df1.shape[0] != df2.shape[0]:\n",
    "        raise ValueError(\"[d] DataFrames must have the same number of vertices (rows).\")\n",
    "    \n",
    "    if df1.shape[1] < 2 or df2.shape[1] < 2:\n",
    "        raise ValueError(\"[d] Each DataFrame must have at least two subjects for d calculation. {}\".format((df1.shape, df2.shape)))\n",
    "\n",
    "    # Initialize output DataFrame\n",
    "    out = pd.DataFrame(index=df1.index)\n",
    "    out['d'] = np.nan\n",
    "    \n",
    "    # Calculate Cohen's d for each vertex\n",
    "    for i in range(df1.shape[0]):\n",
    "        mean1 = df1.iloc[i, :].mean()\n",
    "        mean2 = df2.iloc[i, :].mean()\n",
    "        std1 = df1.iloc[i, :].std(ddof=1)  # Sample standard deviation\n",
    "        std2 = df2.iloc[i, :].std(ddof=1)\n",
    "        n1 = df1.shape[1]\n",
    "        n2 = df2.shape[1]\n",
    "\n",
    "        pooled_std = np.sqrt(((n1 - 1) * std1**2 + (n2 - 1) * std2**2) / (n1 + n2 - 2))\n",
    "        # Avoid division by zero: if pooled_std is zero, set d to np.nan\n",
    "        if pooled_std == 0 or np.isnan(pooled_std):\n",
    "            out.at[i, 'd'] = np.nan\n",
    "        else:\n",
    "            out.at[i, 'd'] = (mean1 - mean2) / pooled_std\n",
    "\n",
    "    return out\n",
    "\n",
    "def get_stats(maps, ipsiTo = \"L\", ctrl_lbl='ctrl', grps=['each'], fdr=True):\n",
    "    \"\"\"\n",
    "    Perform statistical tests between groups in the maps list.\n",
    "    \n",
    "    inputs:\n",
    "        maps: list of dictionary items with keys 'study', 'grp', 'label', 'feature', 'map_pths'\n",
    "        ipsiTo: hemisphere to compare ipsilateral maps to (default is \"L\").\n",
    "        ctrl_lbl: label for the control group (default is 'ctrl').\n",
    "        grps: list of groups to run for. If \"each\", compare all groups to ctrl (except for ctrl grp to itself).\n",
    "            Default is [\"each\"].\n",
    "\n",
    "    outputs:\n",
    "        stats: list of dictionary items with keys:\n",
    "            'study': study name\n",
    "            'grp': group name\n",
    "            'label': surface label\n",
    "            'feature': feature name\n",
    "            't': t-statistic DataFrame\n",
    "            'p': p-value DataFrame\n",
    "            'p_fdr': FDR-corrected p-value DataFrame (if fdr=True)\n",
    "            'd': Cohen's d DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    out = []\n",
    "\n",
    "    if grps == ['each']:\n",
    "        grps = list({item['grp'] for item in maps if item['grp'] != ctrl_lbl})\n",
    "\n",
    "    for grp in grps:\n",
    "        # find the index of these groups in the maps list\n",
    "        grp_indices = [i for i, item in enumerate(maps) if item['grp'] == grp]\n",
    "        \n",
    "        for idx in grp_indices:\n",
    "            comp = maps[idx]\n",
    "            \n",
    "            group = comp['grp']\n",
    "            study = comp['study']\n",
    "            surface = comp['surface']\n",
    "            label = comp['label']\n",
    "            feature = comp['feature']\n",
    "            hippocampal = comp.get('hippocampal', False)\n",
    "            print(f\"\\n[get_stats] [idx: {idx}] {study} {group} ({label}, {feature}, {surface}, {'hipp' if hippocampal else 'cort'})\")\n",
    "            \n",
    "            # find the control group index\n",
    "            idx_ctrl = ctrl_index(maps, idx)            \n",
    "            ctrl = maps[idx_ctrl]\n",
    "            \n",
    "            if 'hippocampal' not in comp:\n",
    "                comp['hippocampal'] = hippocampal\n",
    "            \n",
    "            comp_keys = [key for key in comp.keys() if key.startswith('map_') and not key.endswith('pths')]\n",
    "            ctrl_keys = [key for key in ctrl.keys() if key.startswith('map_') and not key.endswith('pths')]\n",
    "            print(f\"\\tMap keys: {comp_keys}\")\n",
    "            print(f\"\\tControl keys: {ctrl_keys}\")\n",
    "\n",
    "            for key in comp_keys: # compare L and/or R hemisphere maps\n",
    "                \n",
    "                if 'ipsi' in key:\n",
    "                    ctrl_col = f\"map_map_{feature}_{surface}_{label}_{ipsiTo}\"\n",
    "                    comp['ipsiTo'] = ipsiTo\n",
    "                elif 'contra' in key and ipsiTo == \"L\":\n",
    "                    ctrl_col = f\"map_map_{feature}_{surface}_{label}_R\"\n",
    "                    comp['ipsiTo'] = ipsiTo\n",
    "                elif 'contra' in key and ipsiTo == \"R\":\n",
    "                    ctrl_col = f\"map_map_{feature}_{surface}_{label}_L\"\n",
    "                    comp['ipsiTo'] = ipsiTo\n",
    "                elif 'ipsi' in key or 'contra' in key:\n",
    "                    assert key in ctrl_keys, f\"Provided comparison key {key} not in control keys ({ctrl_keys}). Check that ipsiTo is set either to `L` or `R`.\"\n",
    "                    continue  # skip if ipsi/contra is in key but not in control keys\n",
    "                else:\n",
    "                    ctrl_col = key\n",
    "\n",
    "                # ensure that ctrl_col is in ctrl_keys\n",
    "                if ctrl_col not in ctrl_keys:\n",
    "                    print(f\"\\tWARNING. Control column {ctrl_col} not found in control keys. Skipping. {ctrl_keys}.\")\n",
    "                    break\n",
    "                \n",
    "                df_comp = comp[key]\n",
    "                df_ctrl = ctrl[ctrl_col]\n",
    "                print(f\"\\t{key}, ctrl: {ctrl_col} | comp: {df_comp.shape}, ctrl: {df_ctrl.shape}\")\n",
    "\n",
    "                n_ctrl = df_ctrl.shape[1]\n",
    "                n_comp = df_comp.shape[1]\n",
    "\n",
    "                # if n_ctrl and n_comp are not already in comp, add them. If they are, then assert that they are the same\n",
    "                if 'n_ctrl' in comp and 'n_comp' in comp:\n",
    "                    assert comp['n_ctrl'] == n_ctrl, f\"Number of control participants changed from {comp['n_ctrl']} to {n_ctrl} for {key_stats}.\"\n",
    "                    assert comp['n_comp'] == n_comp, f\"Number of comparison participants changed from {comp['n_comp']} to {n_comp} for {key_stats}.\"\n",
    "                else:\n",
    "                    comp['n_ctrl'] = n_ctrl\n",
    "                    comp['n_comp'] = n_comp\n",
    "\n",
    "                if n_ctrl < 2 or n_comp < 2:\n",
    "                    print(f\"[main] Skipping case. too few participants for t-test ({n_comp} comp, {n_ctrl} ctrl).\")\n",
    "                    break\n",
    "                \n",
    "                out_t_p = t_test(df_comp, df_ctrl, fdr=True, equal_var=False)\n",
    "                out_d = d(df_comp, df_ctrl)\n",
    "\n",
    "                # combine results into a single DataFrame\n",
    "                out_df = pd.DataFrame(index=df_comp.index)\n",
    "                out_df['t'] = out_t_p['t']\n",
    "                out_df['p'] = out_t_p['p']\n",
    "                out_df['p_fdr'] = out_t_p['p_fdr']\n",
    "                out_df['d'] = out_d['d']\n",
    "            \n",
    "                # new key:\n",
    "                key_stats = key.replace('map_', 'stats_')\n",
    "\n",
    "                # add out_df to the comp item\n",
    "                comp[key_stats] = out_df\n",
    "            \n",
    "                #print(out_df.shape)\n",
    "\n",
    "            # add to the list of outputs\n",
    "            out.append(comp)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae5f420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx_compl(ldict, idx, silent=True):\n",
    "    \"\"\"\n",
    "    Given a list of dict items and an index, find comparison that is for the same group, label, feature, but different study. \n",
    "    Get the comparison dictionary item at a specific index from a list of comparison items.\n",
    "    \n",
    "    inputs:\n",
    "        ldict: list of comparison dictionary items\n",
    "        idx: index of the item to retrieve\n",
    "\n",
    "    outputs:\n",
    "        idx_compl: str, index of the comparison dictionary item that is complementary to input index\n",
    "\n",
    "    \"\"\"\n",
    "    if idx < 0 or idx >= len(ldict):\n",
    "        print(f\"WARNING: Index {idx} out of bounds for list of length {len(ldict)}\")\n",
    "        return None\n",
    "\n",
    "    study = ldict[idx]['study']\n",
    "\n",
    "    if study == \"PNI\":\n",
    "        other_study = \"MICs\"\n",
    "    elif study == \"MICs\":\n",
    "        other_study = \"PNI\"\n",
    "    else:\n",
    "        print(f\"WARNING: [get_idx_compl] Study code {study} not recognized. Only 'PNI' and 'MICs' are currently supported.\")\n",
    "        return None\n",
    "\n",
    "    # Find all other list items that differ only in study (ignore values for 'stats')\n",
    "    idx_compl = [i for i, item in enumerate(ldict) if item['study'] == other_study and \n",
    "                        item['label'] == ldict[idx]['label'] and \n",
    "                        item['feature'] == ldict[idx]['feature'] and \n",
    "                        item['grp'] == ldict[idx]['grp'] and\n",
    "                        item['surface'] == ldict[idx]['surface'] and\n",
    "                        item.get('hippocampal', False) == ldict[idx].get('hippocampal', False)\n",
    "                ]\n",
    "\n",
    "    idx_compl = idx_compl[0] if idx_compl else None  # Get the first matching index, if any\n",
    "\n",
    "    if idx_compl is None:\n",
    "        print(f\"WARNING: [get_idx_compl] No complementary item found for index {idx} in study {study}.\")\n",
    "\n",
    "    if not silent: print(f\"[get_idx_compl] Complementary index {idx_compl}\")\n",
    "    return idx_compl\n",
    "\n",
    "def stat_dif(df1, df2, stat_col='d'):\n",
    "    \"\"\"\n",
    "    Calculate the difference between columns in two analogous DataFrames.\n",
    "    df1 - df2\n",
    "\n",
    "    inputs:\n",
    "        df1: pd.DataFrame with statistics for comparisons between grp and ctrl for study 1. Vertices should be rows, subjects as columns.\n",
    "        df2: pd.DataFrame with vertex data for comparisons between grp and ctrl for study 2.\n",
    "        stat_col: column name for to compare (default is 'd'). (must be present in both DataFrames)\n",
    "            examples: 'd' for cohen's d, 't' for t-statistic\n",
    "\n",
    "    outputs:\n",
    "        pd.DataFrame with shape n_vertices x 1. Cols:\n",
    "            'd_dif': Difference in Cohen's d value for each vertex\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    if df1.shape[0] != df2.shape[0]:\n",
    "        raise ValueError(\"[d_dif] DataFrames must have the same number of vertices (rows).\")\n",
    "    \n",
    "    # Initialize output DataFrame\n",
    "    out = pd.DataFrame()\n",
    "    out[f'{stat_col}_dif'] = np.nan\n",
    "\n",
    "    # take difference between d values\n",
    "    for i in range(df1.shape[0]):\n",
    "        d1 = df1.at[i, stat_col]\n",
    "        d2 = df2.at[i, stat_col]\n",
    "        \n",
    "        if pd.isna(d1) or pd.isna(d2):\n",
    "            out.at[i, f'{stat_col}_dif'] = np.nan\n",
    "        else:\n",
    "            out.at[i, f'{stat_col}_dif'] = d1 - d2  # Difference in Cohen's d\n",
    "\n",
    "    return out\n",
    "\n",
    "def stat_div(df1, df2, stat_col='d'):\n",
    "    \"\"\"\n",
    "    Calculate the ratio between columns in two analogous DataFrames.\n",
    "    df1 / df2\n",
    "\n",
    "    inputs:\n",
    "        df1: pd.DataFrame with statistics for comparisons between grp and ctrl for study 1. Vertices should be rows, subjects as columns.\n",
    "        df2: pd.DataFrame with vertex data for comparisons between grp and ctrl for study 2.\n",
    "        stat_col: column name for to compare (default is 'd'). (must be present in both DataFrames)\n",
    "            examples: 'd' for cohen's d, 't' for t-statistic\n",
    "\n",
    "    outputs:\n",
    "        pd.DataFrame with shape n_vertices x 1. Cols:\n",
    "            'd_div': Ratio in Cohen's d value for each vertex\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    if df1.shape[0] != df2.shape[0]:\n",
    "        raise ValueError(\"[d_div] DataFrames must have the same number of vertices (rows).\")\n",
    "    \n",
    "    # Initialize output DataFrame\n",
    "    out = pd.DataFrame()\n",
    "    out[f'{stat_col}_div'] = np.nan\n",
    "\n",
    "    # take ratio between d values\n",
    "    for i in range(df1.shape[0]):\n",
    "        d1 = df1.at[i, stat_col]\n",
    "        d2 = df2.at[i, stat_col]\n",
    "        \n",
    "        if pd.isna(d1) or pd.isna(d2) or d2 == 0:\n",
    "            out.at[i, f'{stat_col}_div'] = np.nan\n",
    "        else:\n",
    "            out.at[i, f'{stat_col}_div'] = d1 - d2 / d2  # Ratio in Cohen's d\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b094543c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import maps from saved object\n",
    "\n",
    "import pickle\n",
    "\n",
    "path = \"/host/verges/tank/data/daniel/3T7T/z/maps/map_dfs/map_values_06Jun2025-1808.pkl\"\n",
    "\n",
    "with open(path, \"rb\") as f:\n",
    "    maps = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afb5b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dict([maps[40]], df_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faafafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check map dfs. If L and R are not the same number, then keep only overlapping ID-ses. Save as maps_clean\n",
    "# keys of interest: map_map_{feature}_{surface}_{label}_{hemi}\n",
    "\n",
    "def clean_map_dfs(maps):\n",
    "    \"\"\"\n",
    "    Clean map DataFrames if there are different number of cases in both map value dataframes. In this case, keep only overlapping subjects and sessions.\n",
    "    \n",
    "    inputs:\n",
    "        maps: list of dictionary items with keys 'study', 'grp', 'label', 'feature', 'map_pths'\n",
    "    \n",
    "    outputs:\n",
    "        maps_clean: list of dictionary items with cleaned map DataFrames\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    maps_clean = []\n",
    "\n",
    "    for item in maps:\n",
    "        new_item = item.copy()  # Copy the item to avoid modifying the original\n",
    "        map_keys = [key for key in item.keys() if key.startswith('map_') and not key.endswith('pths')]\n",
    "        \n",
    "        #print(f\"[clean_map_dfs] Maps dataframe keys: {map_keys}\")\n",
    "        if len(map_keys) != 2:\n",
    "            \n",
    "            print(f\"[clean_map_dfs] There are not exactly 2 map value keys. skipping.\")\n",
    "            continue\n",
    "\n",
    "        df1 = new_item[map_keys[0]]\n",
    "        df2 = new_item[map_keys[1]]\n",
    "        if df1.shape[1] != df2.shape[1]:\n",
    "            index = maps.index(item)\n",
    "            print(f\"[clean_map_dfs] [idx: {index}] Different number of cases in {map_keys[0]} ({df1.shape[1]}) and {map_keys[1]} ({df2.shape[1]}). Keeping only overlapping subjects and sessions.\")\n",
    "            \n",
    "            # Get the first map DataFrame to use as a reference\n",
    "            overlap_cols = df1.columns.intersection(df2.columns)\n",
    "\n",
    "            if df1.shape[1] < df2.shape[1]:\n",
    "                df2_new = df2[overlap_cols]\n",
    "                new_item[map_keys[1]] = df2_new\n",
    "            else:\n",
    "                df1_new = df1[overlap_cols]\n",
    "                new_item[map_keys[0]] = df1_new\n",
    "        \n",
    "        maps_clean.append(new_item)\n",
    "        \n",
    "    return maps_clean\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486a6a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "maps_clean = clean_map_dfs(maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457d6009",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(maps))\n",
    "print(len(maps_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53fc9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dict([maps[407]], df_print=True)\n",
    "print_dict([maps_clean[407]], df_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e594afd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAILS for 32k surfaces (can't find overlapping keys in the control dict items)\n",
    "# FAILS for TLE_ic hippocampal maps (can't find overlapping keys in the control dict items)\n",
    "\n",
    "import datetime\n",
    "\n",
    "# up to 15 mins for all groups\n",
    "grps = [\"allPX\", \"TLE_L\", \"TLE_R\", \"TLE_ic\"]\n",
    "\n",
    "comps = get_stats(maps_clean, grps = grps)\n",
    "\n",
    "# save dictionary item\n",
    "save_pth = \"/host/verges/tank/data/daniel/3T7T/z/outputs/results\"\n",
    "date = datetime.datetime.now().strftime(\"%d%b%Y-%H%M\")\n",
    "with open(f'{save_pth}/comparisons_withinStudy_{date}.pkl', 'wb') as f:    pickle.dump(comps, f)\n",
    "print(f\"Saved comparisons to {save_pth}/comparisons_withinScanner_{date}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9525dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dict(comps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8f80ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in comps from saved object\n",
    "import pickle\n",
    "save_pth = \"/host/verges/tank/data/daniel/3T7T/z/outputs/results\"\n",
    "with open(f\"{save_pth}/comparisons_withinStudy_09Jun2025-1127.pkl\", \"rb\") as f:\n",
    "    comps = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c90352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 mins or so\n",
    "\n",
    "# get difference maps of difference maps (3T dif maps - 7T dif maps)\n",
    "\n",
    "tTsT_comp = []\n",
    "compl_list = set()\n",
    "\n",
    "for i in range(len(comps)):\n",
    "    if i in compl_list:\n",
    "        continue\n",
    "\n",
    "    idx_compl = get_idx_compl(comps, i)\n",
    "    \n",
    "    if idx_compl is None:\n",
    "        print(f\"\\t[main] WARNING: No complementary index found for index {i} ({comps[i]['grp']} {comps[i]['feature']} {comps[i]['surface']} {comps[i]['label']}). Skipping.\")\n",
    "        continue\n",
    "\n",
    "    compl_list.add(i)\n",
    "    compl_list.add(idx_compl)\n",
    "\n",
    "    # Assign sT (7T/PNI) and tT (3T/MICs) for consistency\n",
    "    if comps[i]['study'] == \"PNI\":\n",
    "        #print(f\"[main] PNI comparison at index {idx} with complementary index {idx_compl}\")\n",
    "        if not comps[idx_compl]['study'] == \"MICs\":\n",
    "            print(f\"[main] ERROR. Complementary study should be MICs, but got {comps[idx_compl]['study']}.\")\n",
    "        sT = comps[i]\n",
    "        tT = comps[idx_compl]\n",
    "\n",
    "    elif comps[i]['study'] == \"MICs\":\n",
    "        #print(f\"[main] MICs comparison at index {idx} with complementary index {idx_compl}\")\n",
    "        if not comps[idx_compl]['study'] == \"PNI\":\n",
    "            print(f\"[main] ERROR. Complementary study should be PNI, but got {comps[idx_compl]['study']}.\")\n",
    "        sT = comps[idx_compl]\n",
    "        tT = comps[i]\n",
    "\n",
    "    group = sT['grp']\n",
    "    feature = sT['feature']\n",
    "    label = sT['label']\n",
    "    surface = sT.get('surface', 'unknown')  # default to 'unknown' if not present\n",
    "    grp_lbls = sT['grp_labels']\n",
    "    print(f\"[main] MICs: {i}, PNI: {idx_compl}. {group}, {feature}, {label}, {surface}.\")\n",
    "    if not (group == tT['grp'] and feature == tT['feature'] and label == tT['label'] and grp_lbls == tT['grp_labels']):\n",
    "        print(f\"\\tSkipping. Mismatch in ≥1 of: group ({group} vs {tT['grp']}), feature ({feature} vs {tT['feature']}), label ({label} vs {tT['label']}), group labels ({grp_lbls} vs {tT['grp_labels']})\")\n",
    "        continue\n",
    "\n",
    "    # identify overlapping stats keys\n",
    "    stats_keys = [k for k in sT.keys() if k.startswith('stats')]\n",
    "    stats_keys = [k for k in stats_keys if k in tT.keys() and sT[k].shape[0] == tT[k].shape[0]]\n",
    "    if not stats_keys:\n",
    "        print(f\"\\tWARNING: No overlapping stats keys. tT n: {tT.get('n_comp')}, sT n: {sT.get('n_comp')}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\tOverlapping keys: {stats_keys}\")\n",
    "    \n",
    "    # create dict element for this comparison\n",
    "    comparison = {\n",
    "        'grp': group,\n",
    "        'feature': feature,\n",
    "        'label': label,\n",
    "        'grp_labels': grp_lbls,\n",
    "        'surface': surface,\n",
    "        'n_grp_sT': sT['n_comp'],\n",
    "        'n_grp_tT': tT['n_comp'],\n",
    "        'n_ctrl_sT': sT['n_ctrl'],\n",
    "        'n_ctrl_tT': tT['n_ctrl'],\n",
    "    }\n",
    "\n",
    "    for key in stats_keys:\n",
    "        # take df from analogous keys\n",
    "        sT_df = sT[key]\n",
    "        tT_df = tT[key]\n",
    "\n",
    "        # merge sT_df and tT_df into df_dif. Add suffixes related to tT and sT\n",
    "        df_dif = pd.concat([sT_df.add_suffix('_sT'), tT_df.add_suffix('_tT')], axis=1)\n",
    "\n",
    "        assert sT_df.shape[0] == tT_df.shape[0], f\"[main] ERROR. Different number of vertices in stats DataFrames: {sT_df.shape[0]} vs {tT_df.shape[0]} for key {key}.\"\n",
    "\n",
    "        # compute differences - sT - tT\n",
    "        df_dif['d_dif'] = stat_dif(sT_df, tT_df, stat_col='d')\n",
    "        df_dif['d_div'] = stat_div(sT_df, tT_df, stat_col='d')\n",
    "        df_dif['t_dif'] = stat_dif(sT_df, tT_df, stat_col='t')\n",
    "        df_dif['t_div'] = stat_div(sT_df, tT_df, stat_col='t')\n",
    "\n",
    "        # add df_dif to the comparison dict\n",
    "        comparison[key + '_dif'] = df_dif\n",
    "\n",
    "    # add comparison dict to the stats list\n",
    "    tTsT_comp.append(comparison)\n",
    "\n",
    "# save dictionary item\n",
    "save_pth = \"/host/verges/tank/data/daniel/3T7T/z/outputs/results\"\n",
    "date = datetime.datetime.now().strftime(\"%d%b%Y-%H%M\")\n",
    "with open(f'{save_pth}/comp_btwScanners_{date}.pkl', 'wb') as f:\n",
    "    pickle.dump(tTsT_comp, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931cfbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dict(tTsT_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ba250f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46f1f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dictionary item\n",
    "save_pth = \"/host/verges/tank/data/daniel/3T7T/z/outputs/results\"\n",
    "date = datetime.datetime.now().strftime(\"%d%b%Y-%H%M\")\n",
    "with open(f'{save_pth}/comp_btwScanners_{date}.pkl', 'wb') as f:    pickle.dump(comps, f)\n",
    "print(f\"Saved comparisons to {save_pth}/comp_btwScanners_{date}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087732a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the structure of the tTsT_comp\n",
    "print_dict(tTsT_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce10bee",
   "metadata": {},
   "source": [
    "# Visualizations\n",
    "- Show 7T comparison (grp vs ctrl), 3T (grp vs ctrl), difference between 3T and 7T\n",
    "- Parcellation analyses (bar graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa19ed43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Load tTsT_comp from saved object\n",
    "path = \"/host/verges/tank/data/daniel/3T7T/z/outputs/results/comp_btwScanners_09Jun2025-1139.pkl\"\n",
    "with open(path, \"rb\") as f:\n",
    "    tTsT_comp = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8ad2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dict(tTsT_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcf22ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "save = False\n",
    "save_pth = \"/host/verges/tank/data/daniel/3T7T/z/outputs/results/fig_stats/figs\"\n",
    "metric = 'd_div' \n",
    "\n",
    "for i, i in enumerate(tTsT_comp):\n",
    "    grp_idx = i['grp']\n",
    "    feature = i['feature']\n",
    "    label = i['label']\n",
    "    surface = i.get('surface', 'unknown')  # default to 'unknown' if not present\n",
    "    # Skip if 'hippocampal' key is missing or False\n",
    "    if not i.get('hippocampal', True):\n",
    "        pass\n",
    "    elif (i.get('hippocampal', False) is True) or (surface == '0p5mm'):\n",
    "        surface = '0p5mm'\n",
    "        continue  # skip hippocampal maps for now, need to plot on a different surface\n",
    "    if surface == 'unknown':\n",
    "       surface = 'fsLR-5k'\n",
    "    \n",
    "    print(f\"\\n[main] {i} {grp_idx}, {feature}, {label}, {surface}\")\n",
    "    print(f\"\\t{metric} {i.keys()}\")\n",
    "    \n",
    "    if 'stats_contra_dif' in i.keys() and 'stats_ipsi_dif' in i.keys():\n",
    "        ic = True\n",
    "        ipsiTo = i['ipsiTo']\n",
    "        print(f\"[{i}] ipsi/contra keys found. Using ipsiTo={ipsiTo}.\")\n",
    "        \n",
    "        if ipsiTo == \"L\":\n",
    "            lh = i['stats_ipsi_dif'][metric]\n",
    "            rh = i['stats_contra_dif'][metric]\n",
    "            print(lh.shape, rh.shape)\n",
    "            \n",
    "        elif ipsiTo == \"R\":\n",
    "            lh = i['stats_contra_dif'][metric]\n",
    "            rh = i['stats_ipsi_dif'][metric]\n",
    "            print(lh.shape, rh.shape)\n",
    "\n",
    "    elif 'stats_L_dif' in i.keys() and 'stats_R_dif' in i.keys():\n",
    "        print(f\"[{i}] L/R keys found\")\n",
    "        lh = i['stats_L_dif'][metric]\n",
    "        rh = i['stats_R_dif'][metric]\n",
    "        print(lh.shape, rh.shape)\n",
    "    else:\n",
    "        if feature == 'T1map':\n",
    "            lh = i[f'stats_stats_T1stats_{surface}_{label}_L_dif'][metric]\n",
    "            rh = i[f'stats_stats_T1stats_{surface}_{label}_L_dif'][metric]\n",
    "        else:\n",
    "            if surface == '0p5mm':\n",
    "                continue\n",
    "            lh = i[f'stats_stats_{feature}_{surface}_{label}_L_dif'][metric]\n",
    "            rh = i[f'stats_stats_{feature}_{surface}_{label}_L_dif'][metric]\n",
    "    \n",
    "    if metric == 'd_dif':\n",
    "        metric_lbl = \"Δd\"\n",
    "    elif metric == 't_dif':\n",
    "        metric_lbl = \"Δt\"\n",
    "    else:\n",
    "        metric_lbl = metric\n",
    "\n",
    "    fig_title = f\"{metric_lbl} (7T-3T): {i['grp']}, {i['feature']}, {i['label']} surface (grp:ctrl 7T: {i['n_grp_sT']}:{i['n_ctrl_sT']}, 3T: {i['n_grp_tT']}:{i['n_ctrl_tT']})\"\n",
    "\n",
    "    name = f\"{metric}_{i['grp']}_{i['feature']}_{i['label']}\"\n",
    "\n",
    "    from IPython.display import display, Markdown\n",
    "    display(Markdown(f\"### {fig_title}\"))\n",
    "    #showBrains(lh, rh, surface='fsLR-5k', min=-2.5, max=2.5, inflated=False, title = fig_title, save_name = name, save_pth=save_pth)\n",
    "    fig = showBrains(lh, rh, surface='fsLR-5k', ipsiTo = ipsiTo, min=-2.5, max=2.5, inflated=True, title = fig_title)\n",
    "    # save figure\n",
    "    if save == True:\n",
    "        fig = showBrains(lh, rh, surface='fsLR-5k', min=-2, max=2, inflated=False, title = fig_title, save_name = name, save_pth=save_pth)\n",
    "    else:\n",
    "        fig = showBrains(lh, rh, surface='fsLR-5k', min=-2, max=2, inflated=False, title = fig_title)\n",
    "    \n",
    "    display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecec0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: \n",
    "## Visualize 3T and 7T comparison maps\n",
    "## Bar graphs for parcellations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f7a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check correspondence between veretices L and R <optional: thorough>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tTsT_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
