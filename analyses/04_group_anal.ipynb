{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdcd09b9",
   "metadata": {},
   "source": [
    "# Group comparison 3T and 7T epilepsy  \n",
    "\n",
    "Surface-based comparisons  \n",
    "- vertex-wise T-test : are controls and pts different\n",
    "    - Use brainstat\n",
    "- vertex-wise effect size : how big are the distances between the vertex differences?\n",
    "    - Use own function\n",
    "\n",
    "\n",
    "For figures: \n",
    "- Visualize effect size on a brain masked for significant p-values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d522203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import pickle\n",
    "import datetime\n",
    "import brainstat as bstat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6658496f",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78ae7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve surfaces from pt of interest\n",
    "def load_surf(study, IDss):\n",
    "    \"\"\"\n",
    "    Get the surface data for a given group.\n",
    "\n",
    "    inputs:\n",
    "    study: dictionary item with keys 'name', 'dir_root', 'study'\n",
    "    IDs: pd.dataframe woth cols IDs and SES indicating all participants IDs to extract surfaces for\n",
    "\n",
    "    outputs:\n",
    "    surfs: pd.dataframe with vertices in rows and unique ID_SES in columns\n",
    "    \"\"\"\n",
    "    \n",
    "    import nibabel as nib\n",
    "    \n",
    "    # get the list of patients in the group\n",
    "    pt_list = bstat.get_subjects(study, pt_grp)\n",
    "    \n",
    "    # get the surface data for each patient\n",
    "    surf_data = {}\n",
    "    for pt in pt_list:\n",
    "        surf_data[pt] = {}\n",
    "        for surf in surfaces:\n",
    "            surf_data[pt][surf] = bstat.get_surface_data(study, pt, surf)\n",
    "    \n",
    "    return surf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54a5f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chk_pth(pth):\n",
    "    \"\"\"\n",
    "    Check if the path exists and is a file.\n",
    "    \n",
    "    inputs:\n",
    "        pth: path to check\n",
    "    \n",
    "    outputs:\n",
    "        True if the path exists and is a file, False otherwise\n",
    "    \"\"\"\n",
    "    \n",
    "    import os\n",
    "    \n",
    "    if os.path.exists(pth) and os.path.isfile(pth):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e298739e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mp_mapsPth(dir, sub, ses, hemi, surf, lbl, ft):\n",
    "    \"\"\"\n",
    "    Returns path to micapipe maps for given subject, session, hemisphere, surface, label, and feature.\n",
    "    \"\"\"\n",
    "    if lbl == \"thickness\":\n",
    "        return f\"{dir}/sub-{sub}_ses-{ses}_hemi-{hemi}_surf-{surf}_label-{lbl}.func.gii\"\n",
    "    else:\n",
    "        return f\"{dir}/sub-{sub}_ses-{ses}_hemi-{hemi}_surf-{surf}_label-{lbl}_{ft}.func.gii\"\n",
    "\n",
    "# when working, add to Utils scripts\n",
    "def get_1pth(root, deriv_fldr, sub, ses, feature, label=\"midthickness\", surf=\"fsLR-5k\", space=\"nativepro\", hemi=\"LR\", check_pth=True,silence=True):\n",
    "    \"\"\"\n",
    "    Get the path to the surface data for a given subject and session.\n",
    "    Assumes BIDS format of data storage.\n",
    "\n",
    "    inputs:\n",
    "        root: root directory of the study\n",
    "        deriv_fldr: name of derivative folder containing the surface data\n",
    "        sub: subject ID (no `sub-` prefix)\n",
    "        ses: session ID (with leading zero if applicable; no `ses-` prefix)\n",
    "        surf: surface type and resolution (e.g., fsLR-32k, fsLR-5k)\n",
    "        label: surface label (e.g., \"pial\", \"white\", \"midThick\")\n",
    "        space: space of the surface data (e.g., \"nativepro\", \"fsnative\")\n",
    "        hemi: hemisphere to extract (default is \"LR\" for both left and right hemispheres)\n",
    "\n",
    "        check_pth: whether to check if the path exists (default is True)\n",
    "        silence: whether to suppress print statements (default is True)\n",
    "    outputs:\n",
    "        path to the surface data files\n",
    "    \"\"\"\n",
    "\n",
    "    # make surf to lower case\n",
    "    label = label.lower()\n",
    "\n",
    "    # ensure that label is well defined\n",
    "    if label == \"thickness\":\n",
    "        label = \"thickness\"\n",
    "    elif label == \"pial\":\n",
    "        label = \"pial\"\n",
    "    elif label == \"white\":\n",
    "        label = \"white\"\n",
    "    elif label == \"midthick\" or label == \"midthickness\":\n",
    "        label = \"midthickness\"\n",
    "    else:\n",
    "        raise ValueError(f\"{label} Invalid label type. Choose from 'pial', 'white', 'midThick' or 'thickness'.\")\n",
    "    \n",
    "    # construct the path to the surface data file\n",
    "    hemi = hemi.upper()\n",
    "    if hemi == \"LEFT\" or hemi == \"L\":\n",
    "        hemi = \"L\"\n",
    "    elif hemi == \"RIGHT\" or hemi == \"R\":\n",
    "        hemi = \"R\"\n",
    "    elif hemi != \"LR\":\n",
    "        raise ValueError(\"Invalid hemisphere. Choose from 'L', 'R', or 'LR'.\")\n",
    "\n",
    "    # handle hippunfold naming convention\n",
    "    if \"micapipe\" in deriv_fldr.lower():\n",
    "        pth = f\"{root}/derivatives/{deriv_fldr}/sub-{sub}/ses-{ses}/maps\"\n",
    "        if hemi == \"L\" or hemi == \"R\":\n",
    "            pth = mp_mapsPth(dir=pth, sub=sub, ses=ses, hemi=hemi, surf=surf, lbl=label, ft=feature)\n",
    "            if not silence: print(f\"[get_1pth] Returning paths for both hemispheres ([0]: L, [1]: R)\")\n",
    "            \n",
    "        else:         \n",
    "            pth_L = mp_mapsPth(dir=pth, sub=sub, ses=ses, hemi=\"L\", surf=surf, lbl=label, ft=feature)\n",
    "            pth_R = mp_mapsPth(dir=pth, sub=sub, ses=ses, hemi=\"R\", surf=surf, lbl=label, ft=feature)        \n",
    "            pth = [pth_L, pth_R]\n",
    "            if not silence: print(f\"[get_1pth] Returning paths for both hemispheres ([0]: L, [1]: R)\")\n",
    "    elif \"hippunfold\" in deriv_fldr.lower():\n",
    "        raise ValueError(\"Hippunfold derivative not yet implemented. Need to create feature maps using hippunfold surfaces.\")\n",
    "        \n",
    "        # space usually: \"T1w\"\n",
    "        # surf usually: \"fsLR\"\n",
    "        # label options: \"hipp_outer\", \"hipp_inner\", \"hipp_midthickness\"\n",
    "\n",
    "        pth = f\"{root}/derivatives/{deriv_fldr}/sub-{sub}/ses-{ses}/surf\"\n",
    "\n",
    "        if hemi == \"L\" or hemi == \"R\":\n",
    "            pth = f\"{pth}/sub-{sub}_ses-{ses}_hemi-{hemi}_space-{space}_den-{surf}_label-{label}.surf.gii\"\n",
    "            if not silence: print(f\"[surf_pth] Returning hippunfold path for {hemi} hemisphere\")\n",
    "        else:\n",
    "            pth = f\"{pth}/sub-{sub}_ses-{ses}_hemi-{hemi}_surf-{surf}_label-{label}_{feature}.func.gii\"\n",
    "            pth_L = f\"{pth}/sub-{sub}_ses-{ses}_hemi-L_-{surf}_label-{label}.surf.gii\"\n",
    "            pth_R = f\"{pth}/sub-{sub}_ses-{ses}_hemi-R_space-{space}_den-{surf}_label-{label}.surf.gii\"\n",
    "            pth = [pth_L, pth_R]\n",
    "            if not silence: print(f\"[surf_pth] Returning hippunfold paths for both hemispheres ([0]: L, [1]: R)\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid derivative folder. Choose from 'micapipe' or 'hippunfold'.\")\n",
    "\n",
    "\n",
    "    if check_pth:\n",
    "        if isinstance(pth, list):\n",
    "            for idx, p in enumerate(pth):\n",
    "                if not chk_pth(p):\n",
    "                    if label == \"thickness\": feature = \"(thickness)\"\n",
    "                    print(f\"\\t[get_1pth] FILE NOT FOUND (ft: {feature}, sub-{sub}_ses-{ses}): {p}\")\n",
    "                    pth[idx] = \"ERROR:\" + p\n",
    "        else:\n",
    "            if not chk_pth(pth):\n",
    "                print(f\"\\t[get_1pth] FILE NOT FOUND (ft: {feature}, sub-{sub}_ses-{ses}): {pth}\")\n",
    "                pth = \"ERROR:\" + pth\n",
    "    \n",
    "    return pth   \n",
    "\n",
    "def get_Npths(demographics, study, groups, feature=\"FA\", derivative=\"micapipe\", label=\"midthickness\", hemi=\"LR\", space=\"nativepro\", surf=\"fsLR-5k\"):\n",
    "    \"\"\"\n",
    "    Get path to surface files for individual groups\n",
    "\n",
    "\n",
    "    Input:\n",
    "    demographics: dict  regarding demographics file. \n",
    "        Required keys: \n",
    "            'pth'\n",
    "            'ID_7T'\n",
    "            'ID_3T'\n",
    "            'SES'\n",
    "            'date'\n",
    "            'grp'\n",
    "    study: dict  regarding study.\n",
    "        Required keys: \n",
    "            'name'\n",
    "            'dir_root'\n",
    "            'study'\n",
    "            'dir_mp'\n",
    "            'dir_hu'\n",
    "    groups: dict    of groups to extract surfaces for. \n",
    "        Each key should be a group name, and the value should be a list of labels in the 'grp' column of demographics file assigned to that group.\n",
    "    label: str  surface label to extract\n",
    "    hemi: str  hemisphere to extract. Default is \"LR\" for both left and right hemispheres.\n",
    "    space: str  space of the surface data. Default is \"nativepro\".\n",
    "    surf: str  surface type and resolution. Default is \"fsLR-5k\".\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    demo = pd.read_csv(demographics['pth'], dtype=str)\n",
    "    \n",
    "    out = []\n",
    "\n",
    "    if derivative == \"hippunfold\":\n",
    "        deriv_fldr = study['dir_hu']\n",
    "    elif derivative == \"micapipe\":\n",
    "        deriv_fldr = study['dir_mp']\n",
    "    else:\n",
    "        deriv_fldr = study['dir_mp']\n",
    "        print(f\"[get_Npths] WARNING: derivative not recognized. Defaulting to micapipe.\")\n",
    "\n",
    "\n",
    "    for grp_name, grp_labels in groups.items():\n",
    "        print(f\"{study['name']} {grp_name} ({grp_labels})\")\n",
    "\n",
    "        # get IDs for this group\n",
    "        ids = demo.loc[\n",
    "            (demo[demographics['grp']].isin(grp_labels)) &\n",
    "            (demo['study'] == study['study']),\n",
    "            [ID_col, demographics['SES'], 'study', 'Date']\n",
    "        ].copy()\n",
    "\n",
    "        for i, row in ids.iterrows():\n",
    "            ID = row[ID_col]\n",
    "            SES = row[demographics['SES']]\n",
    "            date = row[demographics['date']]\n",
    "            #print(f\"\\tsub-{ID}_ses-{SES}\")\n",
    "            pth = get_1pth(root=study['dir_root'], deriv_fldr=deriv_fldr, sub=ID, ses=SES, label=label, surf=surf, feature=feature, space=space, hemi=hemi)\n",
    "            # add this pth to the dataframe\n",
    "            if isinstance(pth, list):\n",
    "                ids.loc[i, f'pth_L'] = pth[0]\n",
    "                ids.loc[i, f'pth_R'] = pth[1]\n",
    "            else:\n",
    "                ids.loc[i, f'pth_{hemi}'] = pth \n",
    "        # if paths are duplicated, then keep only one of those rows\n",
    "        if hemi == \"LR\":\n",
    "            ids = ids.drop_duplicates(subset=[f'pth_L', f'pth_R'])\n",
    "        else:\n",
    "            ids = ids.drop_duplicates(subset=[f'pth_{hemi}'])\n",
    "\n",
    "        # create dictionary item for each group, add to output list\n",
    "        out.append({\n",
    "            'study': study['name'],\n",
    "            'grp': grp_name,\n",
    "            'grp_labels': grp_labels,\n",
    "            'label': label,\n",
    "            'feature': feature,\n",
    "            'map_pths': ids\n",
    "        })\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3cb345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_pths(dl, method=\"newest\", silent=True):\n",
    "    \"\"\"\n",
    "    Keeps only one session per ID\n",
    "    input:\n",
    "        dl (for dictionary list): List of dictionary items (e.g. outputs from get_Npths). \n",
    "            These dict should contain a df under the key 'map_pths'\n",
    "        method: method to use for choosing session.\n",
    "            \"newest\": use most recent session\n",
    "            \"oldest\": use oldest session in the list\n",
    "            {number}: session code to use (e.g. '01' or 'a1' etc)\n",
    "\n",
    "    output:\n",
    "        dl: List of dictionary items with cleaned dataframes\n",
    "\n",
    "    \"\"\"\n",
    "    dl_out = []\n",
    "    \n",
    "    for i, d in enumerate(dl):\n",
    "        if not silent: print(f\"[clean_pths] {d['study']} {d['grp']}: {df.shape}, num unique IDs: {df[ID_col].nunique()}\")\n",
    "\n",
    "        df = d['map_pths']\n",
    "                \n",
    "        if d['study'] == \"PNI\": ID_col = \"PNI_ID\"\n",
    "        else: ID_col = \"MICS_ID\"\n",
    "        #print(ID_col)\n",
    "\n",
    "        if df.empty: # check if the dataframe is empty\n",
    "            print(f\"\\t[clean_pths] WARNING: Empty dataframe for {d['study']} {d['grp']}\")\n",
    "            continue\n",
    "        else:\n",
    "            df_clean = ses_clean(df, ID_col, method=method, silent=True)\n",
    "            #dl[i]['map_pths'] = df_clean\n",
    "\n",
    "        if df_clean.empty:  # check if the cleaned dataframe is empty\n",
    "            print(f\"\\t[clean_pths] WARNING: Cleaned dataframe is empty for {d['study']} {d['grp']}\")\n",
    "            continue\n",
    "\n",
    "        dl_out.append({\n",
    "            'study': d['study'],\n",
    "            'grp': d['grp'],\n",
    "            'grp_labels': d['grp_labels'],\n",
    "            'label': d['label'],\n",
    "            'feature': d['feature'],\n",
    "            'map_pths': df_clean\n",
    "        })\n",
    "\n",
    "    return dl_out\n",
    "\n",
    "\n",
    "def ses_clean(df, ID_col, method=\"newest\", silent=True):\n",
    "    \"\"\"\n",
    "    Choose the session to use for each subject.\n",
    "        If subject has multiple sessions with map path should only be using one of these sessions.\n",
    "\n",
    "    inputs:\n",
    "        df: pd.dataframe with columns for subject ID, session, date and map_paths\n",
    "            Assumes map path is missing if either : map_pth\n",
    "        method: method to use for choosing session. \n",
    "            \"newest\": use most recent session\n",
    "            \"oldest\": use oldest session in the list\n",
    "            {number}: session code to use (e.g. '01' or 'a1' etc)\n",
    "    \"\"\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    import datetime\n",
    "\n",
    "    # check if the dataframe is empty\n",
    "    if df.empty:\n",
    "        print(f\"[ses_clean] WARNING: Empty dataframe. Skipping.\")\n",
    "        return\n",
    "\n",
    "    if not silent: print(f\"[ses_clean] Choosing session according to method: {method}\")\n",
    "    \n",
    "    # Do NOT remove rows with error paths\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # Find repeated IDs (i.e., subjects with multiple sessions)\n",
    "    repeated_ids = df_clean[df_clean.duplicated(subset=ID_col, keep=False)][ID_col].unique()\n",
    "    \n",
    "    if not silent:\n",
    "        if len(repeated_ids) > 0:\n",
    "            print(f\"\\tIDs with multiple sessions: {repeated_ids}\")\n",
    "        else:\n",
    "            print(f\"\\tNo repeated IDs found\")\n",
    "\n",
    "    rows_to_remove = []\n",
    "    \n",
    "    # Convert 'Date' column to datetime for comparison\n",
    "    df_clean['Date_dt'] = pd.to_datetime(df_clean['Date'], format='%d.%m.%Y', errors='coerce')\n",
    "    today = pd.to_datetime('today').normalize()\n",
    "\n",
    "    if len(repeated_ids) > 0:\n",
    "        if method == \"newest\":\n",
    "            for id in repeated_ids:\n",
    "                sub_df = df_clean[df_clean[ID_col] == id]\n",
    "                if sub_df.shape[0] > 1:\n",
    "                    idx_to_keep = sub_df['Date_dt'].idxmax()\n",
    "                    idx_to_remove = sub_df.index.difference([idx_to_keep])\n",
    "                    rows_to_remove.extend(idx_to_remove)\n",
    "        elif method == \"oldest\":\n",
    "            for id in repeated_ids:\n",
    "                sub_df = df_clean[df_clean[ID_col] == id]\n",
    "                if sub_df.shape[0] > 1:\n",
    "                    idx_to_keep = sub_df['Date_dt'].idxmin()\n",
    "                    idx_to_remove = sub_df.index.difference([idx_to_keep])\n",
    "                    rows_to_remove.extend(idx_to_remove)\n",
    "        else:\n",
    "            # Assume method is a session code (e.g., '01', 'a1', etc)\n",
    "            for id in repeated_ids:\n",
    "                sub_df = df_clean[df_clean[ID_col] == id]\n",
    "                if sub_df.shape[0] > 1:\n",
    "                    idx_to_remove = sub_df[sub_df['SES'] != method].index\n",
    "                    rows_to_remove.extend(idx_to_remove)\n",
    "\n",
    "    # Remove the rows marked for removal\n",
    "    df_clean = df_clean.drop(rows_to_remove)\n",
    "    #if not silent: print(df_clean[[ID_col, 'SES']].sort_values(by=ID_col))\n",
    "\n",
    "    # if num rows =/= to num unique IDs then write warning\n",
    "    if df_clean.shape[0] != df_clean[ID_col].nunique():\n",
    "        print(f\"[ses_clean] WARNING: Number of rows ({df_clean.shape[0]}) not equal to num unique IDs ({df_clean[ID_col].nunique()})\")\n",
    "        print(f\"\\tMultiple sessions for IDs: {df_clean[df_clean.duplicated(subset=ID_col, keep=False)][ID_col].unique()}\")\n",
    "\n",
    "    if not silent: \n",
    "        print(f\"\\t{df.shape[0] - df_clean.shape[0]} rows removed, Change in unique IDs: {df_clean[ID_col].nunique() - df[ID_col].nunique()}\")\n",
    "        print(f\"\\t{df_clean.shape[0]} rows remaining\")\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "def get_finalSES(dl, demo, save_pth=None, long=False, silent=True): \n",
    "    \"\"\"\n",
    "    From a list of dictionary items, create a DF with sessions retained for each participant and each feature \n",
    "\n",
    "    input:\n",
    "        dl: List of dictionary items with cleaned dataframes\n",
    "        demo: dictionary with demographics file information.\n",
    "        save_pth: path to save the dataframe to. If None, do not save.\n",
    "        long: if True, return a long format dataframe with one row per subject and session feature. If False, return wide format.\n",
    "\n",
    "    output:\n",
    "        df: pd.dataframe with columns for subject ID, session_feature, grp, study and map_paths\n",
    "            Assumes map path is missing if either : map_pth\n",
    "    \"\"\"\n",
    "    import datetime\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    demo_df = pd.read_csv(demo['pth'], dtype=str)\n",
    "    out = pd.DataFrame()  # Will collect all unique IDs and their session columns\n",
    "\n",
    "    for i, d in enumerate(dl):\n",
    "        feature = d['feature']\n",
    "        label = d['label']\n",
    "\n",
    "        df = d['map_pths']\n",
    "\n",
    "        id_col = [col for col in df.columns if 'ID' in col.upper()][0]  \n",
    "        ses_col = 'SES'\n",
    "        \n",
    "        if not silent: print(f\"[get_finalSES] {d['study']} {d['grp']}: {feature}, {label} ({df.shape[0]} rows)\")\n",
    "\n",
    "        # Use correct study prefix and correct ID column for merge\n",
    "        if d['study'] == \"PNI\": \n",
    "            study_prefix = \"7T\"\n",
    "            merge_id_col = demo['ID_7T']\n",
    "        elif d['study'] == \"MICs\":\n",
    "            study_prefix = \"3T\"\n",
    "            merge_id_col = demo['ID_3T']\n",
    "        else: \n",
    "            study_prefix = \"Unknown\"\n",
    "            merge_id_col = None\n",
    "\n",
    "        if label == \"thickness\": lbl_ft = f\"{label}\"\n",
    "        else: lbl_ft = f\"{label}-{feature}\"\n",
    "\n",
    "        new_col = f'{study_prefix}-ses_{lbl_ft}'\n",
    "\n",
    "        # Mark SES as NA if all path columns are ERROR or missing\n",
    "        path_cols = [col for col in df.columns if col.startswith('pth_') or col.startswith('surf_') or col.startswith('map_')]\n",
    "        def ses_na_row(row):\n",
    "            if not path_cols:\n",
    "                return row[ses_col]\n",
    "            # If all path columns are missing or start with ERROR\n",
    "            if all((not isinstance(row[c], str)) or row[c].startswith(\"ERROR\") or row[c] == \"\" for c in path_cols):\n",
    "                return \"NA\"\n",
    "            return row[ses_col]\n",
    "        df_tmp = df[[id_col, ses_col] + path_cols].copy()\n",
    "        df_tmp[new_col] = df_tmp.apply(ses_na_row, axis=1)\n",
    "        df_tmp = df_tmp.rename(columns={id_col: \"ID\"})\n",
    "        df_tmp = df_tmp[[\"ID\", new_col]]\n",
    "\n",
    "        # If column already exists, add to it\n",
    "        if new_col in out.columns:\n",
    "            # Merge on ID, but keep both values for comparison\n",
    "            merged = pd.merge(out[['ID', new_col]], df_tmp, on=\"ID\", how=\"outer\", suffixes=('_old', '_new'))\n",
    "            \n",
    "            def resolve(row): # For each ID, resolve conflicts\n",
    "                vals = set([row[f\"{new_col}_old\"], row[f\"{new_col}_new\"]])\n",
    "                vals = {v for v in vals if pd.notnull(v)}\n",
    "                if len(vals) == 1:\n",
    "                    return vals.pop()\n",
    "                elif len(vals) > 1:\n",
    "                    # Print warning and keep the latest value (assuming SES is string, keep max)\n",
    "                    if not silent:\n",
    "                        print(f\"[get_finalSES] WARNING: Multiple values for {row['ID']} in {new_col}: {vals}. Keeping latest.\")\n",
    "                    # If SES is numeric string, sort as int, else as string\n",
    "                    try:\n",
    "                        return sorted(vals, key=lambda x: int(x) if isinstance(x, str) and x.isdigit() else str(x))[-1]\n",
    "                    except Exception:\n",
    "                        return sorted(vals)[-1]\n",
    "                else:\n",
    "                    return None\n",
    "                \n",
    "            merged[new_col] = merged.apply(resolve, axis=1)\n",
    "            \n",
    "            # Update out with resolved column\n",
    "            out = pd.merge(out, merged[['ID', new_col]], on=\"ID\", how=\"outer\", suffixes=('', '_resolved'))\n",
    "            out[new_col] = out[new_col + '_resolved'].combine_first(out[new_col])\n",
    "            out = out.drop(columns=[new_col + '_resolved'])\n",
    "        else:\n",
    "            if out.empty:\n",
    "                out = df_tmp\n",
    "            else:\n",
    "                out = pd.merge(out, df_tmp, on=\"ID\", how='outer')\n",
    "\n",
    "    if not long:\n",
    "        # under construction\n",
    "        print(\"[get_finalSES] Wide format not yet implemented. Returning long format instead.\")\n",
    "        long = True\n",
    "\n",
    "    if save_pth is not None:\n",
    "        date = datetime.datetime.now().strftime(\"%d%b%Y-%H%M\")\n",
    "        if long: save = f\"{save_pth}/sesXfeat_long_{date}.csv\"\n",
    "        else: save = f\"{save_pth}/sesXfeat_{date}.csv\"\n",
    "        out.to_csv(save, index=False)\n",
    "        print(f\"[get_finalSES] Saved dataframe to {save}\")\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5686b7b0",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4537cc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify root directories\n",
    "MICs = {\n",
    "    \"name\": \"MICs\",\n",
    "    \"dir_root\": \"/data/mica3/BIDS_MICs\",\n",
    "    \"dir_mp\": \"micapipe_v0.2.0\",\n",
    "    \"dir_hu\": \"hippunfold_v1.3.0/hippunfold\",\n",
    "    \"study\": \"3T\",\n",
    "    \"ID_ctrl\" : [\"HC\"],\n",
    "    \"ID_Pt\" : [\"PX\"]\n",
    "    }\n",
    "\n",
    "PNI = {\n",
    "    \"name\": \"PNI\",\n",
    "    \"dir_root\": \"/data/mica3/BIDS_PNI\",\n",
    "    \"dir_mp\": \"micapipe_v0.2.0\",\n",
    "    \"dir_hu\": \"hippunfold_v1.3.0/hippunfold\",\n",
    "    \"study\": \"7T\",\n",
    "    \"ID_col\" : [\"PNC\", \"Pilot\"], # column for ID in demographics file\n",
    "    }\n",
    "\n",
    "studies = [MICs, PNI]\n",
    "\n",
    "demographics = {\n",
    "    \"pth\" : \"/host/verges/tank/data/daniel/3T7T/z/data/pt/demo_22May2025.csv\",\n",
    "    # column names:\n",
    "    \"ID_7T\" : \"PNI_ID\", \n",
    "    \"ID_3T\" : \"MICS_ID\",\n",
    "    \"SES\" : \"SES\",\n",
    "    \"date\": \"Date\",\n",
    "    \"grp\" : \"grp_detailed\" # col name for participant grouping variable of interest\n",
    "}\n",
    "\n",
    "px_grps = { # specify patient group labels to compare to controls\n",
    "    'allPX' : ['TLE_U', 'MFCL', 'FLE_R', 'MFCL_bTLE', 'UKN_L', 'mTLE_R', 'mTLE_L', 'FLE_L', 'UKN_U', 'TLE_L', 'TLE_R'],\n",
    "    'TLE' : ['TLE_L', 'TLE_R', 'TLE_U', 'mTLE_R', 'mTLE_L'],\n",
    "    'TLE_L': ['TLE_L', 'mTLE_L'],\n",
    "    'TLE_R': ['TLE_R', 'mTLE_R'],\n",
    "    'FCD' : ['FLE_R', 'FLE_L'],\n",
    "    'MFCL' : ['MFCL', 'MFCL_bTLE'],\n",
    "    'UKN' : ['UKN_L', 'UKN_U']\n",
    "}\n",
    "\n",
    "ctrl_grp = {'ctrl' : ['CTRL']}\n",
    "\n",
    "groups = {**px_grps, **ctrl_grp}\n",
    "\n",
    "features = [\"FA\", \"MD\", \"T1map\"]\n",
    "#labels = [\"white\", \"midthick\", \"thickness\", \"swm1.0mm\", \"swm2.0mm\", \"swm3.0mm\"]\n",
    "labels = [\"white\", \"midthick\", \"thickness\"]\n",
    "\n",
    "surfaces = [\"fsLR-5k\"]\n",
    "\n",
    "demo = pd.read_csv(demographics['pth'], dtype=str)\n",
    "demo[[\"MICS_ID\", \"PNI_ID\", \"study\", \"SES\", \"grp\", \"grp_detailed\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857702d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "studies = [MICs, PNI]\n",
    "groups = {**px_grps, **ctrl_grp}\n",
    "\n",
    "\n",
    "map_pths = []\n",
    "map_pths_clean = []\n",
    "final_ses = []\n",
    "save_pth = \"/host/verges/tank/data/daniel/3T7T/z/outputs/pt\"\n",
    "\n",
    "if 'surf_dfs' not in locals():\n",
    "    surf_dfs = {}\n",
    "\n",
    "for study in studies:\n",
    "    print(f\"[main] {study['name']}\")\n",
    "\n",
    "    if study['study'] == \"3T\": ID_col = demographics['ID_3T']\n",
    "    elif study['study'] == \"7T\": ID_col = demographics['ID_7T']\n",
    "    for surf in surfaces:\n",
    "        for label in labels:\n",
    "            print(f\"\\tsurf:{surf}, lbl:{label}\")\n",
    "            if label == \"thickness\":\n",
    "                map_pths.extend(get_Npths(demographics, study, groups, feature=\"\", derivative=\"micapipe\", label=label, surf=surf))\n",
    "            else:\n",
    "                for feature in features:\n",
    "                    print(f\"\\t\\tft: {feature}, surf: {surf}\")\n",
    "                    map_pths.extend(get_Npths(demographics, study, groups, feature=feature, derivative=\"micapipe\", label=label, surf=surf))\n",
    "                    \n",
    "map_pths_clean = clean_pths(map_pths, method=\"newest\")\n",
    "final_SES = get_finalSES(map_pths_clean, demo=demographics, save_pth=save_pth, long=False,silent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34047d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dictionary item\n",
    "save_pth = \"/host/verges/tank/data/daniel/3T7T/z/outputs/pt\"\n",
    "date = datetime.datetime.now().strftime(\"%d%b%Y\")\n",
    "#with open(f'{save_pth}/map_paths_{date}.pkl', 'wb') as f:    pickle.dump(map_pths, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25f8cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load list of dict items with surface paths\n",
    "\n",
    "#with open('map_pths.pkl', 'rb') as f:\n",
    "#    map_pths = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2715fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print maps_pths\n",
    "for i, d in enumerate(map_pths):\n",
    "    print(f\"{i}: {d['study']} {d['grp']} ({d['grp_labels']})\")\n",
    "    print(d['map_pths'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458585c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print specific study-group combinations\n",
    "get_study = \"PNI\"\n",
    "get_grp = \"ctrl\"\n",
    "\n",
    "for item in map_pths:\n",
    "    if item['study'] == get_study and item['grp'] == get_grp:\n",
    "        print(f\"{item['study']}-{item['grp']} ({item['grp_labels']})\")\n",
    "        with pd.option_context('display.max_columns', None):\n",
    "            print(item['map_pths'])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3eb741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find path for specific participant\n",
    "study = \"PNI\"\n",
    "sub = \"PNC018\"\n",
    "ses = \"01\"\n",
    "\n",
    "grp = \"ctrl\"\n",
    "\n",
    "entry = next((item for item in map_pths if item['study'] == study and item['grp'] == grp), None)\n",
    "\n",
    "if entry is not None:\n",
    "    df = entry['map_pths']\n",
    "    row = df[(df[ID_col] == sub) & (df['SES'] == ses)]\n",
    "    if not row.empty:\n",
    "        print(row['pth_L'].values[0])\n",
    "    else:\n",
    "        print(f\"No entry found for subject {sub} and session {ses}\")\n",
    "else:\n",
    "    print(f\"No entry found for study {study} and group {grp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba58a251",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "test = [map_pths[1], map_pths[7]]\n",
    "for i, d in enumerate(test):\n",
    "    print(f\"{i}: {d['study']} {d['grp']}\")\n",
    "    print()\n",
    "\n",
    "# clean tests\n",
    "test_pths_clean = clean_pths(test, method=\"newest\")\n",
    "for i, d in enumerate(map_pths_clean):\n",
    "    print(f\"\\nItem {i}:\")\n",
    "    print(f\"  Keys: {list(d.keys())}\")\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f301afbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b2414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_print = False\n",
    "for i, d in enumerate(map_pths_clean):\n",
    "    print(f\"\\nItem {i}:\")\n",
    "    print(f\"  Keys: {list(d.keys())}\")\n",
    "\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, pd.DataFrame):\n",
    "            print(f\"  {k}: <DataFrame shape={v.shape}>\")\n",
    "            if df_print == True: print(f\"  {k}: {v}\")\n",
    "        else:\n",
    "            print(f\" {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af872448",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc25d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put surface files into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1305b807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flip TLEs --> put all lesions on same side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6491fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get difference maps at 3T (3T ctrl - 3T cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663e1418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get difference maps at 7T (7T ctrl - 7T cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8951597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get difference maps of difference maps (3T dif maps - 7T dif maps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tTsT_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
