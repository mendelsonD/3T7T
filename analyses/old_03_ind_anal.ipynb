{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis - zBrains outputs at 3T and 7T in epilepsy\n",
    "1. zBrain/wBrain (surface)  \n",
    "    a. Histograms of vertex wise scores  \n",
    "        i. sub-comparisons with different smoothing kernels  \n",
    "    b. Quantifying extreme vertex groups  \n",
    "        i. number of identified abnormal areas  \n",
    "        ii. size of each abnormal area (number of adjacent extreme vertices)  \n",
    "2. Brainstats (surface)  \n",
    "    a. t-scores for 3T and 7T  \n",
    "    b. cohen's D map between 3T and 7T images  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import tTsTGrpUtils as tsutil\n",
    "import utils_plots as uplots\n",
    "\n",
    "dl_pth = \"/host/verges/tank/data/daniel/3T7T/z/outputs/04d_dl_maps_19Oct2025-153909.pkl\"\n",
    "dl = tsutil.loadPickle(dl_pth)\n",
    "tsutil.print_dict(dl, df_print=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2\n",
    "item = 'df_demo'\n",
    "df = dl[idx][item]\n",
    "#df = tsutil.loadPickle(dl[idx][item])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from brainspace.plotting import plot_hemispheres\n",
    "from brainspace.mesh.mesh_io import read_surface\n",
    "from hippunfold_toolbox import plotting as plot_hu\n",
    "importlib.reload(tsutil)\n",
    "importlib.reload(uplots)\n",
    "# \n",
    "# 46,47,\n",
    "item_idx = [44,5,20,21,46,47, 52, 53]\n",
    "\n",
    "idx_3T = [\"UID0018_PX071_04\",\"UID0027_PX168_01\", \"UID0030_PX148_01\", \"UID0023_PX173_01\"]\n",
    "idx_7T = [\"UID0018_PNE001_01\", \"UID0027_PNE010_a1\", \"UID0030_PNE013_a1\", \"UID0023_PNE006_a1\"]\n",
    "save_maps = \"/host/verges/tank/data/daniel/3T7T/qualitative_pics/maps\"\n",
    "\n",
    "def get_mesh(region, surface, micapipe = \"/data_/mica1/01_programs/micapipe-v0.2.0\", hipp_surfaces = \"/host/verges/tank/data/daniel/3T7T/z/code/analyses/resources/\", inflated = True):\n",
    "    if region in [\"ctx\", \"cortex\"]:\n",
    "        if surface == 'fsLR-5k':\n",
    "            if inflated == True:\n",
    "                # Load fsLR 5k inflated\n",
    "                surf_lh = read_surface(micapipe + '/surfaces/fsLR-5k.L.inflated.surf.gii', itype='gii')\n",
    "                surf_rh = read_surface(micapipe + '/surfaces/fsLR-5k.R.inflated.surf.gii', itype='gii')\n",
    "            else:\n",
    "                # Load fsLR 5k\n",
    "                surf_lh = read_surface(micapipe + '/surfaces/fsLR-5k.L.surf.gii', itype='gii')\n",
    "                surf_rh = read_surface(micapipe + '/surfaces/fsLR-5k.R.surf.gii', itype='gii')\n",
    "        elif surface == 'fsLR-32k':\n",
    "            if inflated == True:\n",
    "                # Load fsLR 32k inflated\n",
    "                surf_lh = read_surface(micapipe + '/surfaces/fsLR-32k.L.inflated.surf.gii', itype='gii')\n",
    "                surf_rh = read_surface(micapipe + '/surfaces/fsLR-32k.R.inflated.surf.gii', itype='gii')\n",
    "            else:\n",
    "                # Load fsLR 32k\n",
    "                surf_lh = read_surface(micapipe + '/surfaces/fsLR-32k.L.surf.gii', itype='gii')\n",
    "                surf_rh = read_surface(micapipe + '/surfaces/fsLR-32k.R.surf.gii', itype='gii')\n",
    "        else:\n",
    "            raise ValueError(f\"Surface {surface} not recognized. Use 'fsLR-5k' or 'fsLR-32k'.\")\n",
    "        \n",
    "    elif region in [\"hip\", \"hipp\", \"hippocampus\"]: # only have templates for midthickness\n",
    "        if surface in ['0p5mm','den-0p5mm']: # use hippunfold toolbox\n",
    "            surf_lh = read_surface(hipp_surfaces + 'tpl-avg_space-canonical_den-0p5mm_label-hipp_midthickness_L.surf.gii', itype='gii')\n",
    "            surf_rh = read_surface(hipp_surfaces + 'tpl-avg_space-canonical_den-0p5mm_label-hipp_midthickness_R.surf.gii', itype='gii')\n",
    "\n",
    "            # unfolded\n",
    "            surf_lh_unf = read_surface(hipp_surfaces + 'tpl-avg_space-unfold_den-0p5mm_label-hipp_midthickness.surf.gii', itype='gii')\n",
    "            surf_rh_unf = read_surface(hipp_surfaces + 'tpl-avg_space-unfold_den-0p5mm_label-hipp_midthickness.surf.gii', itype='gii')\n",
    "\n",
    "    return surf_lh, surf_rh\n",
    "\n",
    "def hist(z_lh, z_rh, title, save_pth, d=None, mn = True):\n",
    "    import matplotlib.pyplot as plt\n",
    "    z_lh, z_rh = z_map[:len(pt_l)], z_map[len(pt_l):]\n",
    "    import scipy.stats as stats\n",
    "    \n",
    "    # Create smoothed histograms using kernel density estimation\n",
    "    z_lh_clean = z_lh[~np.isnan(z_lh)]\n",
    "    z_rh_clean = z_rh[~np.isnan(z_rh)]\n",
    "    \n",
    "    if len(z_lh_clean) > 0:\n",
    "        kde_lh = stats.gaussian_kde(z_lh_clean)\n",
    "        x_range = np.linspace(min(z_lh_clean.min(), z_rh_clean.min()), \n",
    "                             max(z_lh_clean.max(), z_rh_clean.max()), 200)\n",
    "        plt.plot(x_range, kde_lh(x_range), label='L', linewidth=2)\n",
    "    \n",
    "    if len(z_rh_clean) > 0:\n",
    "        kde_rh = stats.gaussian_kde(z_rh_clean)\n",
    "        x_range = np.linspace(min(z_lh_clean.min(), z_rh_clean.min()), \n",
    "                             max(z_lh_clean.max(), z_rh_clean.max()), 200)\n",
    "        plt.plot(x_range, kde_rh(x_range), label='R', linewidth=2)\n",
    "    if d is not None:\n",
    "        # add annotation\n",
    "        plt.annotate(f\"d = {d:.2f}\", xy=(0.7, 0.9), xycoords='axes fraction', fontsize=12,\n",
    "                     bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"yellow\", ec=\"black\", lw=1))\n",
    "    if mn:\n",
    "        plt.annotate(f\"mn_L = {z_lh.mean():.2f}\\nmn_R = {z_rh.mean():.2f}\", xy=(0.7, 0.75), xycoords='axes fraction', fontsize=12,\n",
    "                     bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"yellow\", ec=\"black\", lw=1))\n",
    "    ax = plt.gca()\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.savefig(save_pth)\n",
    "    plt.close()\n",
    "\n",
    "def smoothMaps(sub, ses, ft, lbl, region, surf, root_dir, root_mp, root_deriv, study):\n",
    "    if ft == 'thickness':\n",
    "        surf_pth = tsutil.get_surf_pth(root=f\"{root_deriv}\", sub=sub, ses=ses, lbl='midthickness', surf = surf)\n",
    "    else:\n",
    "        surf_pth = tsutil.get_surf_pth(root=f\"{root_deriv}\", sub=sub, ses=ses, lbl=lbl, surf = surf)\n",
    "    \n",
    "    out_name_base = f\"map-{sub}-{ses}_{ft}_{lbl}_{region}_{surf}\"\n",
    "    \n",
    "    if region == 'cortex':\n",
    "        smth = 10\n",
    "    else:\n",
    "        smth = 2\n",
    "\n",
    "    final_out_L = f\"{save_maps}/{out_name_base}_L-smth{smth}mm.func.gii\"\n",
    "    final_out_R = f\"{save_maps}/{out_name_base}_R-smth{smth}mm.func.gii\"\n",
    "\n",
    "    if tsutil.chk_pth(final_out_L) and tsutil.chk_pth(final_out_R):\n",
    "        #print(f\"Found existing smoothed maps for {ctrl}, skipping processing.\")\n",
    "        lh = nib.load(final_out_L).darrays[0].data\n",
    "        rh = nib.load(final_out_R).darrays[0].data\n",
    "        return lh, rh\n",
    "\n",
    "    if region == 'cortex':\n",
    "        map_pth = tsutil.get_map_pth(root = root_dir, deriv_fldr=\"micapipe_v0.2.0\", sub = sub, ses = ses, feature = ft,\n",
    "                        label = lbl, surf = surf)\n",
    "        \n",
    "    else:\n",
    "        if ft == 'thickness':\n",
    "            map_pth = tsutil.get_surf_pth(root_dir + \"/derivatives/hippunfold_v1.3.0/hippunfold\", sub=sub, ses=ses, lbl='thickness', surf=surf)\n",
    "            map_pth_lh = map_pth[0]\n",
    "            map_pth_rh = map_pth[1]\n",
    "        else:\n",
    "            vol_pth =tsutil.get_mapVol_pth(root_mp, sub, ses, study, ft)\n",
    "            print(vol_pth)\n",
    "            out_name = f\"{save_maps}/{out_name_base}.func.gii\"\n",
    "            map_pth_lh = tsutil.map(vol_pth, surf_pth[0], out_name)\n",
    "            map_pth_rh = tsutil.map(vol_pth, surf_pth[1], out_name)\n",
    "    \n",
    "    #print(map_pth)\n",
    "\n",
    "    surf_pth_lh = surf_pth[0]\n",
    "    surf_pth_rh = surf_pth[1]\n",
    "\n",
    "    lh_smth_pth = tsutil.smooth_map(surf = surf_pth_lh, map = map_pth_lh, out_name = final_out_L, kernel = smth) \n",
    "    rh_smth_pth = tsutil.smooth_map(surf = surf_pth_rh, map = map_pth_rh , out_name = final_out_R, kernel = smth)\n",
    "\n",
    "    lh = nib.load(lh_smth_pth).darrays[0].data\n",
    "    rh = nib.load(rh_smth_pth).darrays[0].data\n",
    "\n",
    "    return lh, rh\n",
    "\n",
    "for i in item_idx:\n",
    "    itm = dl[i]\n",
    "    study = itm['study']\n",
    "    region = itm['region']\n",
    "    surf = itm['surf']\n",
    "    lbl = itm['label']\n",
    "    ft = itm['feature']\n",
    "    \n",
    "    if study == 'MICs':\n",
    "        pt_idx = idx_3T\n",
    "        id_col = \"MICS_ID\"\n",
    "        study_code = \"3T\"\n",
    "        root_dir = \"/data/mica3/BIDS_MICs\"\n",
    "        cbar_pos = 'right'\n",
    "    else:\n",
    "        pt_idx = idx_7T\n",
    "        id_col = \"PNI_ID\"\n",
    "        study_code = \"7T\"\n",
    "        root_dir = \"/data/mica3/BIDS_PNI\"\n",
    "        cbar_pos = 'left'\n",
    "\n",
    "    root_mp = f\"{root_dir}/derivatives/micapipe_v0.2.0\"\n",
    "    if region in ['hippocampus', 'hip']:\n",
    "        root_deriv = f\"{root_dir}/derivatives/hippunfold_v1.3.0/hippunfold\"\n",
    "    else:\n",
    "        root_deriv = root_mp\n",
    "    \n",
    "    df = itm['df_maps']\n",
    "    if isinstance(df, str):\n",
    "        df = tsutil.loadPickle(df)\n",
    "\n",
    "    df_demo = itm['df_demo']\n",
    "    idx_ctrl = df_demo[(df_demo['grp'] == 'CTRL') & (df_demo['study'] == study_code)]\n",
    "    idx_ctrl['UID_ID_SES'] = idx_ctrl['UID'] + \"_\" + idx_ctrl[id_col] + \"_\" + idx_ctrl['SES']\n",
    "    idx_ctrl = idx_ctrl['UID_ID_SES'].values.tolist()\n",
    "\n",
    "    # get maps of controls\n",
    "    df_maps_ctrl = pd.DataFrame()\n",
    "\n",
    "    for ctrl in idx_ctrl:\n",
    "        parts = ctrl.split('_')\n",
    "        sub = parts[1]  # Extract ID from UID_ID_SES format\n",
    "        ses = parts[2]  # Extract SES from UID_ID_SES format\n",
    "\n",
    "        lh, rh = smoothMaps(sub=sub, ses=ses, ft=ft, lbl=lbl, region=region, surf=surf,\n",
    "                             root_dir=root_dir, root_mp=root_mp, root_deriv=root_deriv,\n",
    "                             study=study)\n",
    "        \n",
    "        combined_data = np.concatenate([lh, rh]) # rh is the second half of the data\n",
    "        df_maps_ctrl[ctrl] = combined_data\n",
    "\n",
    "    print(f\"df_maps_ctrl <{df_maps_ctrl.shape}>: {df_maps_ctrl.columns.tolist()}\")\n",
    "    #print(df_maps_ctrl.head())\n",
    "    \n",
    "    mean_ctrl = df_maps_ctrl.mean(axis=1)\n",
    "    mn_lh, mn_rh = mean_ctrl[:len(lh)], mean_ctrl[len(lh):]\n",
    "    std_ctrl = df_maps_ctrl.std(axis=1)\n",
    "    std_lh, std_rh = std_ctrl[:len(lh)], std_ctrl[len(lh):]\n",
    "    #print(mean_ctrl.head())\n",
    "    #print(std_ctrl.head())\n",
    "    \n",
    "    print(f\"mean: {mean_ctrl.shape}\")\n",
    "    print(f\"std: {std_ctrl.shape}\")\n",
    "\n",
    "    #lh_np = lh.flatten()\n",
    "    #rh_np = rh.to_numpy().flatten()\n",
    "    # save mn as surface mesh\n",
    "    surf_lh, surf_rh = get_mesh(region=region, surface=surf)\n",
    "    data_mn = np.concatenate([mn_lh.values.flatten(), mn_rh.values.flatten()])\n",
    "    save_name_mn = f\"/host/verges/tank/data/daniel/3T7T/qualitative_pics/ctrlMEAN_{itm['feature']}_{itm['study']}_{itm['region']}_{itm['surf']}_{itm['label']}_{itm['smth']}mm.png\"\n",
    "    if ft == 't1map':\n",
    "        min_val = 1000\n",
    "        max_val = 4500\n",
    "    else:git com\n",
    "        min_val = 0\n",
    "        max_val = 4.5\n",
    "    plot_hemispheres(\n",
    "                    surf_lh, surf_rh, array_name=data_mn, \n",
    "                    size=(900, 250), color_bar='right', zoom=1.25, embed_nb=True, interactive=False, share='both',\n",
    "                    nan_color=(0, 0, 0, 1), color_range=(min_val,max_val), cmap='Blues', transparent_bg=True, \n",
    "                    screenshot=True, filename=save_name_mn,\n",
    "                    #, label_text = lbl_text\n",
    "                )\n",
    "    \n",
    "    print(f\"Saved mean figure to {save_name_mn}\")\n",
    "\n",
    "    data_std = np.concatenate([std_lh.values.flatten(), std_rh.values.flatten()])\n",
    "    save_name_std = f\"/host/verges/tank/data/daniel/3T7T/qualitative_pics/ctrlStd_{itm['feature']}_{itm['study']}_{itm['region']}_{itm['surf']}_{itm['label']}_{itm['smth']}mm.png\"\n",
    "    plot_hemispheres(\n",
    "                    surf_lh, surf_rh, array_name=data_std, \n",
    "                    size=(900, 250), color_bar='right', zoom=1.25, embed_nb=True, interactive=False, share='both',\n",
    "                    nan_color=(0, 0, 0, 1), color_range=(0,0.5), cmap='Blues', transparent_bg=True, \n",
    "                    screenshot=True, filename=save_name_std,\n",
    "                    #, label_text = lbl_text\n",
    "                )\n",
    "    print(f\"Saved std figure to {save_name_std}\")\n",
    "   \n",
    "    for pt in pt_idx:\n",
    "        sub = pt.split('_')[1]\n",
    "        ses = pt.split('_')[2]\n",
    "\n",
    "        pt_l, pt_r = smoothMaps(sub=sub, ses=ses, ft=ft, lbl=lbl, region=region, surf=surf,\n",
    "                             root_dir=root_dir, root_mp=root_mp, root_deriv=root_deriv,\n",
    "                             study=study)\n",
    "        mn_pt_l, mn_pt_r = pt_l.mean(), pt_r.mean()\n",
    "        std_pt_l, std_pt_r = pt_l.std(), pt_r.std()\n",
    "        n_l = n_r = len(pt_l)\n",
    "        pooled_std = (((n_l - 1) * std_pt_l**2 + (n_r - 1) * std_pt_r**2) / (n_l + n_r - 2))**0.5\n",
    "        d = (mn_pt_l - mn_pt_r) / pooled_std\n",
    "        print(f\"D: {d}\")\n",
    "        \n",
    "        #print(f\"ctrl_mn: {mean_ctrl[:20]}\")\n",
    "        #print(f\"pt_l: {pt_l[:20]}\")\n",
    "        map_pt =  np.concatenate([pt_l, pt_r])\n",
    "        #print(f\"map_pt:\\n{map_pt[:20]}\")\n",
    "        z_map = (map_pt - mean_ctrl) / std_ctrl\n",
    "        #print(f\"z_map:\\n{z_map[:20]}\")\n",
    "        # plot histogram of z_map\n",
    "        z_lh, z_rh = z_map[:len(pt_l)], z_map[len(pt_l):]\n",
    "        \n",
    "        save_name_pt = f\"/host/verges/tank/data/daniel/3T7T/qualitative_pics/{pt}_{itm['region']}_{itm['feature']}_{itm['study']}_{itm['surf']}_{itm['label']}_{itm['smth']}mm_zmap.png\"\n",
    "        \n",
    "        if region == 'cortex':\n",
    "            z_lh, z_rh = tsutil.apply_midMask(z_lh, z_rh, surf = itm['surf'])\n",
    "            data_pt = np.concatenate([z_lh.values.flatten(), z_rh.values.flatten()])\n",
    "            plot_hemispheres(\n",
    "                surf_lh, surf_rh, array_name=data_pt, \n",
    "                size=(900, 250), color_bar='right', zoom=1.25, embed_nb=True, interactive=False, share='both',\n",
    "                nan_color=(0, 0, 0, 1), color_range=(-4,4), cmap='seismic', transparent_bg=True, \n",
    "                screenshot=True, filename=save_name_pt,\n",
    "                #, label_text = lbl_text\n",
    "            )\n",
    "        else:\n",
    "            lh_array = np.asarray(z_lh).flatten()\n",
    "            rh_array = np.asarray(z_rh).flatten()\n",
    "            data_pt = np.column_stack([z_lh, z_rh])\n",
    "            data_pt = data_pt[:, :, np.newaxis]\n",
    "            plot_hu.surfplot_canonical_foldunfold(data_pt, tighten_cwindow=True, \n",
    "                                                  labels='hipp', color_bar='right', \n",
    "                                                  color_range = (-4,4), cmap='seismic', share = True,\n",
    "                                                 screenshot=True, filename = save_name_pt)\n",
    "            \n",
    "        print(f\"Saved z-map figure to {save_name_pt}\")\n",
    "\n",
    "        hist(z_lh, z_rh, title = f\"{pt} - {study} {region} {ft} {surf}\", save_pth=f\"/host/verges/tank/data/daniel/3T7T/qualitative_pics/{pt}_{itm['feature']}_{itm['study']}_{itm['region']}_{itm['surf']}_{itm['label']}_{itm['smth']}mm_zmap_hist.png\", d = d, mn = [mn_pt_l, mn_pt_r])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read these in\n",
    "a = nib.load(\"/data/mica3/BIDS_MICs/derivatives/hippunfold_v1.3.0/hippunfold/sub-HC129/ses-01/surf/sub-HC129_ses-01_hemi-L_space-T1w_den-0p5mm_label-hipp_midthickness.surf.gii\").darrays[0].data\n",
    "\n",
    "b = nib.load(\"/data/mica3/BIDS_MICs/derivatives/hippunfold_v1.3.0/hippunfold/sub-HC129/ses-01/surf/sub-HC129_ses-01_hemi-L_space-T1w_den-0p5mm_label-hipp_midthickness.surf.gii\").darrays[0].data\n",
    "print(a.shape, b.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make gifs from lower and higher res image\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# -------------------------------\n",
    "# CONFIGURATION\n",
    "# -------------------------------\n",
    "img = \"7T\"\n",
    "dpi = 500\n",
    "fps = 10\n",
    "slice_step = 1  # Use >1 to skip slices for fewer frames\n",
    "\n",
    "if img == \"7T\":\n",
    "    input_path = \"/data/mica3/BIDS_PNI/derivatives/micapipe_v0.2.0/sub-PNE017/sub-PNE017_ses-a1_space-nativepro_T1w_brain.nii.gz\"  \n",
    "    output_folder = \"/host/verges/tank/data/daniel/3T7T/z/coregVols/PNE017_PX174/7T_pngs/\"\n",
    "    output_video = \"/host/verges/tank/data/daniel/3T7T/z/coregVols/PNE017_PX174/7T_pngs/coronal_animation.mp4\"\n",
    "    start_slice = 61\n",
    "    end_slice = 381\n",
    "elif img == \"3T\":\n",
    "    input_path = \"/host/verges/tank/data/daniel/3T7T/z/coregVols/PNE017_PX174/PX174a_0000.nii.gz\"   # Replace with your file\n",
    "    output_folder = \"/host/verges/tank/data/daniel/3T7T/z/coregVols/PNE017_PX174/3T_pngs/\"\n",
    "    output_video = \"/host/verges/tank/data/daniel/3T7T/z/coregVols/PNE017_PX174/3T_pngs/coronal_animation.mp4\"\n",
    "    start_slice = 61\n",
    "    end_slice = 330\n",
    "    \n",
    "\n",
    "# -------------------------------\n",
    "# LOAD VOLUME\n",
    "# -------------------------------\n",
    "img = nib.load(input_path)\n",
    "data = img.get_fdata()\n",
    "norm_data = (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "norm_data = (norm_data * 255).astype(np.uint8)\n",
    "\n",
    "# Coronal plane => slice along axis 1\n",
    "n_slices = data.shape[1]\n",
    "height, width = norm_data.shape[0], norm_data.shape[2]\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "video_writer = cv2.VideoWriter(output_video, fourcc, fps, (width, height), isColor=False)\n",
    "\n",
    "for i in range(0, n_slices, slice_step):\n",
    "    # Extract coronal slice and rotate for correct orientation\n",
    "    slice_img = np.rot90(norm_data[:, i, :])\n",
    "    \n",
    "    # Convert to BGR for video writing if needed (here grayscale video, so no)\n",
    "    video_writer.write(slice_img)\n",
    "\n",
    "video_writer.release()\n",
    "print(f\"MP4 video saved to {output_video}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through volumes, sweep with yellow line for transition\n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Parameters\n",
    "path_3T = \"/host/verges/tank/data/daniel/3T7T/z/coregVols/PNE017_PX174/PX174a_0000_resampled.nii.gz\"\n",
    "tT_min = 473\n",
    "tT_max = 5254\n",
    "path_7T = \"/data/mica3/BIDS_PNI/derivatives/micapipe_v0.2.0/sub-PNE017/ses-a1/anat/sub-PNE017_ses-a1_space-nativepro_T1w.nii.gz\"\n",
    "sT_min = -5\n",
    "sT_max = 79\n",
    "output_video = \"/host/verges/tank/data/daniel/3T7T/z/coregVols/PNE017_PX174/PX174-PNE017.mp4\"\n",
    "fps = 30\n",
    "transition_begin = 1/3\n",
    "transition_frames = 30\n",
    "line_width = 3\n",
    "line_color = (0, 255, 255)  # Yellow in BGR\n",
    "\n",
    "def apply_intensity_window(image, window_min, window_max):\n",
    "    clipped = np.clip(image, window_min, window_max)\n",
    "    scaled = ((clipped - window_min) / (window_max - window_min) * 255.0).astype(np.uint8)\n",
    "    return scaled\n",
    "\n",
    "def vol_to_bgr_frames_with_window(vol, wmin, wmax, n_slices):\n",
    "    frames = []\n",
    "    for i in range(n_slices):\n",
    "        slice_img = np.rot90(vol[:, i, :])\n",
    "        slice_windowed = apply_intensity_window(slice_img, wmin, wmax)\n",
    "        bgr_img = cv2.cvtColor(slice_windowed, cv2.COLOR_GRAY2BGR)\n",
    "        frames.append(bgr_img)\n",
    "    return frames\n",
    "\n",
    "# Load volumes\n",
    "img_a = nib.load(path_3T)\n",
    "img_b = nib.load(path_7T)\n",
    "\n",
    "vol_a = img_a.get_fdata()\n",
    "vol_b = img_b.get_fdata()\n",
    "assert vol_a.shape == vol_b.shape, \"Volumes must have the same shape\"\n",
    "\n",
    "n_slices = vol_a.shape[1]\n",
    "height, width = np.rot90(vol_a[:, 0, :]).shape\n",
    "\n",
    "frames_a = vol_to_bgr_frames_with_window(vol_a, tT_min, tT_max, n_slices)\n",
    "frames_b = vol_to_bgr_frames_with_window(vol_b, sT_min, sT_max, n_slices)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "video_writer = cv2.VideoWriter(output_video, fourcc, fps, (width, height), isColor=True)\n",
    "\n",
    "transition_idx = int(transition_begin * n_slices)\n",
    "\n",
    "# Part 1: Show volume A slices up to transition_idx\n",
    "for i in range(transition_idx):\n",
    "    frame = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    slice_img = frames_a[i]\n",
    "    start_x = (width - slice_img.shape[1]) // 2\n",
    "    frame[:, start_x:start_x + slice_img.shape[1], :] = slice_img\n",
    "    flipped_frame = cv2.flip(frame, 1)\n",
    "    video_writer.write(flipped_frame)\n",
    "\n",
    "# Part 2: Transition sweep iterates over slices and progresses the sweep line\n",
    "for step in range(transition_frames):\n",
    "    slice_index = transition_idx + step\n",
    "    if slice_index >= n_slices:\n",
    "        break\n",
    "    slice_a = frames_a[slice_index]\n",
    "    slice_b = frames_b[slice_index]\n",
    "    slice_width = slice_a.shape[1]\n",
    "    start_x = (width - slice_width) // 2\n",
    "    sweep_x = int(slice_width * step / (transition_frames - 1))\n",
    "\n",
    "    frame = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    # Left side: from image A, columns [0:sweep_x)\n",
    "    print(f\"sweep_x: {sweep_x}, {start_x}\")\n",
    "    frame[:, start_x : start_x + sweep_x, :] = frames_a[:, sweep_x:, :]\n",
    "\n",
    "    # Right side: from image B, columns [sweep_x:end)\n",
    "    frame[:, start_x + sweep_x : start_x + slice_width, :] = frames_b[:, :sweep_x, :]\n",
    "\n",
    "    # Draw vertical yellow sweep line\n",
    "    line_pos = start_x + sweep_x\n",
    "    cv2.rectangle(frame, (line_pos, 0), (min(line_pos + line_width, start_x + slice_width - 1), height), line_color, thickness=-1)\n",
    "\n",
    "    flipped_frame = cv2.flip(frame, 1)\n",
    "    video_writer.write(flipped_frame)\n",
    "\n",
    "# Part 3: Show remaining volume B slices after transition_frames\n",
    "for i in range(transition_idx + transition_frames, n_slices):\n",
    "    frame = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    slice_img = frames_b[i]\n",
    "    start_x = (width - slice_img.shape[1]) // 2\n",
    "    frame[:, start_x:start_x + slice_img.shape[1], :] = slice_img\n",
    "    flipped_frame = cv2.flip(frame, 1)\n",
    "    video_writer.write(flipped_frame)\n",
    "\n",
    "video_writer.release()\n",
    "print(f\"MP4 video saved to {output_video}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample 3T image to have same matrix size as 7T\n",
    "import nibabel as nib\n",
    "from nibabel.processing import resample_from_to\n",
    "import numpy as np\n",
    "\n",
    "def resample_to_same_matrix_but_keep_resolution(source_img, target_img):\n",
    "    # Extract voxel sizes from source affine (scales are norms of affine matrix columns)\n",
    "    voxel_sizes = np.sqrt(np.sum(source_img.affine[:3, :3] ** 2, axis=0))\n",
    "\n",
    "    # Use target image shape but keep source voxel sizes\n",
    "    target_shape = target_img.shape[:3]\n",
    "\n",
    "    # Construct new affine matrix for target: diagonal matrix with voxel sizes and same origin as target\n",
    "    # Extract target origin from target affine translation part\n",
    "    target_origin = target_img.affine[:3, 3]\n",
    "\n",
    "    # Build new affine with diagonal voxel sizes and target origin\n",
    "    new_affine = np.diag(np.append(voxel_sizes, 1.0))\n",
    "    new_affine[:3, 3] = target_origin\n",
    "\n",
    "    # Resample source_img to new voxel space\n",
    "    resampled_img = resample_from_to(source_img, (target_shape, new_affine))\n",
    "\n",
    "    return resampled_img\n",
    "\n",
    "# Usage\n",
    "\n",
    "path_3T = \"/host/verges/tank/data/daniel/3T7T/z/coregVols/PNE017_PX174/PX174a_0000.nii.gz\"\n",
    "path_7T = \"/data/mica3/BIDS_PNI/derivatives/micapipe_v0.2.0/sub-PNE017/ses-a1/anat/sub-PNE017_ses-a1_space-nativepro_T1w_brain.nii.gz\" # path to first volume\n",
    "path_3T_out = \"/host/verges/tank/data/daniel/3T7T/z/coregVols/PNE017_PX174/PX174a_0000_resampled.nii.gz\"\n",
    "\n",
    "source_img = nib.load(path_3T)\n",
    "target_img = nib.load(path_7T)\n",
    "resampled_img = resample_to_same_matrix_but_keep_resolution(source_img, target_img)\n",
    "\n",
    "# Save result if needed\n",
    "nib.save(resampled_img, path_3T_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define directories\n",
    "output_dir = \"/host/verges/tank/data/daniel/3T7T/z/outputs\"\n",
    "values_dir = \"values\"\n",
    "processed_output_dir = \"/host/verges/tank/data/daniel/3T7T/z/outputs/fig_stats\"\n",
    "\n",
    "\n",
    "# 3T-7T ID correspondence\n",
    "correps_IDs = {\n",
    "    \"path\": \"/host/verges/tank/data/daniel/3T7T/z/data/pt/IDs_ses_analyses_12Mar.csv\",\n",
    "    \"3T_ID\": \"3T_ID\",\n",
    "    \"7T_ID\": \"7T_ID\",\n",
    "    \"3T_SES\": \"3T_SES\",\n",
    "    \"7T_SES\": \"7T_SES\"\n",
    "}\n",
    "\n",
    "#id_corresp = pd.read_csv(corresp_ID)\n",
    "\n",
    "# Study names\n",
    "MICs = {\"name\": \"MICs\"}\n",
    "\n",
    "PNI = {\"name\": \"PNI\"}\n",
    "\n",
    "#studies = [\"MICs\", \"PNI\"]\n",
    "\n",
    "# zBrain analysis regions\n",
    "cortex = {\n",
    "    \"region\": \"cortex\",\n",
    "    \"surfaces\": [\"midthickness\", \"white\"],\n",
    "    \"resolution\": \"32k\",\n",
    "    \"features\": [\"ADC\", \"T1map\", \"volume\"], # (list) features to extract\n",
    "    #\"smoothing\": [10]\n",
    "    \"smoothing\": [2,5,10]\n",
    "}\n",
    "\n",
    "hippocampus = {\n",
    "    \"region\": \"hippocampus\",\n",
    "    \"surfaces\": [\"midthickness\"],\n",
    "    \"resolution\": \"0p5mm\",\n",
    "    \"features\": [\"ADC\", \"T1map\", \"volume\"], # (list) features to extract\n",
    "    #\"smoothing\": [5]\n",
    "    \"smoothing\": [1,2,5]\n",
    "}\n",
    "\n",
    "subcortex = {\n",
    "    \"region\": \"subcortex\",\n",
    "    \"features\": [\"ADC\", \"T1map\", \"volume\"],\n",
    "    \"smoothing\": [2,5,10]\n",
    "}\n",
    "\n",
    "regions = [cortex, hippocampus, subcortex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of corresponding 3T, 7T aggregate files\n",
    "files_lst = plots.corresp_paths(regions, MICs, PNI, output_dir, values_dir)\n",
    "#print(files_lst)\n",
    "shape = gen.lstOlst_shape(files_lst,print=False)\n",
    "print(f\"raw shape of files_lst (num files, num studies): {shape}\")\n",
    "\n",
    "# get missing files\n",
    "missing = plots.get_missingPths(files_lst)\n",
    "\n",
    "# remove missing files from list\n",
    "for m in missing:\n",
    "    files_lst.remove(m)\n",
    "\n",
    "shape = gen.lstOlst_shape(files_lst,print=False)\n",
    "print(f\"shape of files_lst (num files, num studies): {shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(files_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(vrtx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# summary statistics & prepare for group hists\n",
    "- All analysed PX vs all PNE for each file type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get summary stats for each file type\n",
    "df_summary = pd.DataFrame()\n",
    "clamp_files_lst = []\n",
    "for lst in files_lst:\n",
    "    clamp_lst = []\n",
    "    for file in lst:\n",
    "        print(os.path.basename(file))\n",
    "        df = vrtx.summaryStats(file)\n",
    "        df.insert(df.columns.get_loc(\"study\") + 1, \"region\", os.path.dirname(file).split('/')[-1])\n",
    "        \n",
    "        df_summary = pd.concat([df_summary, df])\n",
    "\n",
    "        # clamp values\n",
    "        df_clamped = vrtx.clamp(file)\n",
    "        \n",
    "        if (df[\"study\"] == \"MICs\").all():\n",
    "            study = \"MICs\"\n",
    "        elif (df[\"study\"] == \"PNI\").all():\n",
    "            study = \"PNI\"\n",
    "\n",
    "        \n",
    "        if (df[\"region\"] == cortex).all():\n",
    "            region = \"cortex\"\n",
    "        elif (df[\"region\"] == hippocampus).all():\n",
    "            region = \"hippocampus\"\n",
    "        elif (df[\"region\"] == subcortex).all():\n",
    "            region = \"subcortex\"\n",
    "        \n",
    "        clamp_name = os.path.basename(file).replace('.csv', '_clamp.csv')\n",
    "        clamp_pth = os.path.join(output_dir,\"values\", study, region, \"clamp\", clamp_name)\n",
    "\n",
    "        df_clamped.to_csv(clamp_pth, index=False)\n",
    "        print(f\"Clamped values saved to {clamp_pth}\")\n",
    "        clamp_lst.append(clamp_pth)\n",
    "    clamp_files_lst.append(clamp_lst)\n",
    "\n",
    "print(clamp_files_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_pth = os.path.join(processed_output_dir, f\"sumStats_{date}.csv\")\n",
    "df_summary.to_csv(out_pth, index=False)\n",
    "print  (f\"Summary stats saved to {out_pth}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_pth = os.path.join(processed_output_dir, f\"sumStats_{date}.csv\")\n",
    "df_summary.to_csv(out_pth, index=False)\n",
    "print  (f\"Summary stats saved to {out_pth}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group histograms\n",
    "\n",
    "for files in clamp_files_lst:\n",
    "    # check that base names are the same except for the study\n",
    "    histName_mics = os.path.basename(files[0])\n",
    "    histName_mics = histName_mics.split('_')\n",
    "\n",
    "    histName_pni = os.path.basename(files[1])\n",
    "    histName_pni = histName_pni.split('_')\n",
    "\n",
    "    if histName_mics[1:] != histName_pni[1:]:\n",
    "        print(\"Error: file names do not match. Skipping: \\n\\t%s\\n\\t%s\" %(histName_mics, histName_pni))\n",
    "        continue\n",
    "    else:\n",
    "        #print(\"File names match\")\n",
    "        pass\n",
    "\n",
    "    hemi = histName_mics[1].split('-')[1]\n",
    "    lbl = histName_mics[3].split('-')[1]\n",
    "    feat = histName_mics[4].split('-')[1]\n",
    "    smth = histName_mics[5].split('-')[1]\n",
    "    #print(f\"hemisphere: {hemi}, label: {lbl}, feature: {feat}, smoothing: {smth}\")\n",
    "\n",
    "    title = f\"{feat}, smoothing: {smth} ({hemi}, {lbl})\"\n",
    "    #print(title)\n",
    "\n",
    "\n",
    "    # plot histograms\n",
    "    save_path = \"/host/verges/tank/data/daniel/3T7T/z/outputs/fig_stats/hist_grp\"\n",
    "    save_name = f\"grpHist_{feat}_smth-{smth}_{hemi}_{lbl}_{date}.png\"\n",
    "    save = os.path.join(save_path, save_name)\n",
    "    fig = plots.group_hist(files, labels=[title, \"MICs\",\"PNI\"], save_path=save)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge plots (one line per participant)\n",
    "# plot histogram\n",
    "\n",
    "# check that file exists\n",
    "# if not, continue to next file\n",
    "for file in files_lst:\n",
    "        \n",
    "        \n",
    "        # read in data\n",
    "        df_mics = pd.read_csv(mics_file, index_col=False)\n",
    "        df_pni = pd.read_csv(pni_file, index_col=False)\n",
    "        # remove participants with Na in either df\n",
    "        # df_mics = df_mics.dropna()\n",
    "        # df_pni = df_pni.dropna()\n",
    "        \n",
    "        # keep only overlapping participants both dfs\n",
    "        \n",
    "        ## need to remap col names according to 3T-7T ID correspondence\n",
    "\n",
    "        ## keep only overlapping columns\n",
    "        # cols = df_mics.columns.intersection(df_pni.columns)\n",
    "        # df_mics = df_mics[cols]\n",
    "        # df_pni = df_pni[cols]\n",
    "\n",
    "        ## Take histogram for each participant\n",
    "\n",
    "        # print(df_mics.head())\n",
    "        break\n",
    "        # # construct histogram\n",
    "        # fig = plots.ridge(df_mics, matrix_df = df_mics)\n",
    "        # # show histogram\n",
    "        # fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        print(path)\n",
    "        \n",
    "        df = pd.read_csv(path)\n",
    "        fig = plots.histStack(df)\n",
    "        # display plot\n",
    "        fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tTsT_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
