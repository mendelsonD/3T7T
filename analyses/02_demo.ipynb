{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demographics\n",
    "\n",
    "a. Identify IDs with 3T and 7T  \n",
    "b. Extract clinical information for epilepsy patients  \n",
    "c. Extract demographic information for all participants  \n",
    "\n",
    "\n",
    "Note participant grouping:\n",
    "- CTRL : no known epilepsy (identified by ID code: 3T: HC; 7T: PNC, Pilot)\n",
    "\n",
    "- (m)TLE : temporal lobe epilepsy\n",
    "    - TLE_L : TLE left lateralized (including L>R)\n",
    "    - TLE_R : TLE right lateralized (including R>L)\n",
    "    - TLE_U : TLE with unknown lateralization\n",
    "    - mTLE_L : mTLE left lateralized\n",
    "    - mTLE_R : mTLE right lateralized\n",
    "- FLE : focal epilepsy that is not in temporal lobe\n",
    "    - FLE_L\n",
    "    - FLE_R\n",
    "- UKN : Focal epilepsy with focus in unknown lobe\n",
    "    - UKN_L : Unknown lobe with known L lateralization\n",
    "    - UKN_R : Unknown lobe with known R lateralization\n",
    "    - UKN_U : Unknown lobe with unknown lateralization\n",
    "- MFCL : multifocal epilepsy\n",
    "    - MFCL : multifocal epilepsy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"/Users/danielmendelson/Library/CloudStorage/OneDrive-McGillUniversity/Documents/PhD/Boris/code\")\n",
    "#sys.path.append(\"/host/verges/tank/data/daniel/\")\n",
    "from Utils import id, gen, t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group(df, ID_col=\"MICS_ID\", out_col=\"grp\", save_pth=None, MFCL_col=\"Epilepsy classification:Focal,Generalized\", lobe_col=\"Epileptogenic focus confirmed by the information of (sEEG/ site of surgical resection/ Ictal EEG abnormalities +/. MRI findings): FLE=forntal lobe epilepsy and cingulate epilepsy, CLE:central/midline epilepsy,ILE: insular epilepsy, mTLE=mesio.temporal lobe epilepsy, nTLE=neocortical lobe epilepsy, PQLE=posterior quadrant lobe epilepsy , multifocal epilepsy,IGE=ideopathic lobe epilepsy,unclear)\", lat_col=\"Lateralization of epileptogenic focus\"):\n",
    "    \"\"\"\n",
    "    Requires pandas as pd\n",
    "\n",
    "    Inputs:\n",
    "    df: str or pd.DataFrame\n",
    "        demographics data\n",
    "    ID_col: str\n",
    "        Column name for the ID\n",
    "    out_col: str\n",
    "        Column name for the output group classification. \n",
    "        Options: 'grp' returns high-level grouping. All other col_names return detailed grouping.\n",
    "    MFCL_col: str\n",
    "        Column name for the multifocal classification\n",
    "    lobe_col: str  \n",
    "        Column name for the lobe classification\n",
    "    lat_col: str\n",
    "        Column name for the lateralization classification\n",
    "\n",
    "    Outputs:\n",
    "    df: pd.DataFrame\n",
    "        DataFrame with the group classification added\n",
    "    \"\"\"\n",
    "\n",
    "    import pandas as pd\n",
    "    import os\n",
    "\n",
    "\n",
    "    print(\"[group] Identifying participant groups\")\n",
    "    # check if df is a string (path) or dataframe\n",
    "    if isinstance(df, str):\n",
    "        # check if file exists\n",
    "        if not os.path.isfile(df):\n",
    "            raise ValueError(f\"[group] Error: {df} does not exist.\")\n",
    "        \n",
    "        # read in file\n",
    "        df = pd.read_csv(df, dtype=str)\n",
    "    elif isinstance(df, pd.DataFrame):\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"[group] Error: df must be a string (path) or dataframe\")\n",
    "\n",
    "    df[out_col] = df.apply(\n",
    "        lambda row: f\"PATTERN NOT RECOGNIZED: lobe={row.get(lobe_col, None)}, lat={row.get(lat_col, None)}, MFCL={row.get(MFCL_col, None)}\",\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    if ID_col == \"MICS_ID\":\n",
    "        ctrl_ptrn = [\"HC\"]\n",
    "    elif ID_col == \"PNI_ID\":\n",
    "        ctrl_ptrn = [\"Pilot\", \"PNC\"]\n",
    "    else:\n",
    "        raise ValueError(\"[group] Error: ID_col must be 'MICS_ID' or 'PNI_ID'\")\n",
    "\n",
    "    \n",
    "    if out_col == \"grp\":\n",
    "        print(\"\\tReturning highlevel grouping to column: \", out_col)\n",
    "       \n",
    "        df.loc[df[ID_col].astype(str).str.contains('|'.join(ctrl_ptrn), na=False), out_col] = 'CTRL'\n",
    "        \n",
    "        # TLE and mTLE: L, left, R, right, unclear\n",
    "        df.loc[\n",
    "            (df[lobe_col].astype(str).str.lower().isin(['tle', 'mtle'])) & \n",
    "            (df[lat_col].astype(str).str.lower().str.contains('l|left|r|right|l>r|r>l|bl|bilateral|unclear', na=False)), \n",
    "            out_col\n",
    "        ] = 'TLE'\n",
    "        \n",
    "        # FLE: L, R, right, unclear\n",
    "        df.loc[\n",
    "            (df[lobe_col] == 'FLE') & \n",
    "            (df[lat_col].astype(str).str.contains('l|r|right|unclear', case=False, na=False)), \n",
    "            out_col\n",
    "        ] = 'FLE'\n",
    "        \n",
    "        # PLE: L, R, right, unclear\n",
    "        df.loc[\n",
    "            ((df[lobe_col] == 'PLE')) &\n",
    "            (df[lat_col].astype(str).str.contains('l|r|right|unclear', case=False, na=False)),\n",
    "            out_col\n",
    "        ] = 'PLE'\n",
    "\n",
    "        # PQLE: L, R, right, unclear\n",
    "        df.loc[\n",
    "            ((df[lobe_col] == 'PQLE')) &\n",
    "            (df[lat_col].astype(str).str.contains('l|r|right|unclear', case=False, na=False)),\n",
    "            out_col\n",
    "        ] = 'PQLE'\n",
    "\n",
    "        # Unclear: L, R, right, unclear\n",
    "        df.loc[(df[lobe_col].astype(str).str.contains('unclear', na=False)) & (df[lat_col] == 'L'), out_col] = 'UKN'\n",
    "        df.loc[(df[lobe_col].astype(str).str.contains('unclear', na=False)) & ((df[lat_col] == 'R') | (df[lat_col].astype(str).str.lower() == 'right')), out_col] = 'UKN'\n",
    "        df.loc[(df[lobe_col].astype(str).str.contains('unclear', na=False)) & (df[lat_col].astype(str).str.contains('unclear', na=False)), out_col] = 'UKN'\n",
    "        \n",
    "        # Multifocal\n",
    "        df.loc[(df[MFCL_col] == 'Multifocal'), out_col] = 'MFCL'\n",
    "    \n",
    "    else:  \n",
    "        print(\"\\tReturning detailed grouping to column: \", out_col) \n",
    "        \n",
    "        df.loc[df[ID_col].astype(str).str.contains('|'.join(ctrl_ptrn), na=False), out_col] = 'CTRL'\n",
    "        \n",
    "        # Combine TLE and mTLE, label as TLE\n",
    "        df.loc[\n",
    "            (df[lobe_col].astype(str).str.lower().isin(['tle', 'mtle'])) & (df[lat_col] == 'L'),\n",
    "            out_col\n",
    "        ] = 'TLE_L'\n",
    "        df.loc[\n",
    "            (df[lobe_col].astype(str).str.lower().isin(['tle', 'mtle'])) & ((df[lat_col] == 'R') | (df[lat_col].astype(str).str.lower() == 'right')),\n",
    "            out_col\n",
    "        ] = 'TLE_R'\n",
    "        df.loc[\n",
    "            (df[lobe_col].astype(str).str.lower().isin(['tle', 'mtle'])) & (df[lat_col].astype(str).str.contains('unclear', na=False)),\n",
    "            out_col\n",
    "        ] = 'TLE_U'\n",
    "\n",
    "        df.loc[(df[lobe_col] == 'FLE') & (df[lat_col] == 'L'), out_col] = 'FLE_L'\n",
    "        df.loc[(df[lobe_col] == 'FLE') & ((df[lat_col] == 'R') | (df[lat_col].astype(str).str.lower() == 'right')), out_col] = 'FLE_R'\n",
    "\n",
    "        df.loc[(df[lobe_col].astype(str).str.contains('unclear', na=False)) & (df[lat_col] == 'L'), out_col] = 'UKN_L'\n",
    "        df.loc[(df[lobe_col].astype(str).str.contains('unclear', na=False)) & ((df[lat_col] == 'R') | (df[lat_col].astype(str).str.lower() == 'right')), out_col] = 'UKN_R'\n",
    "        df.loc[(df[lobe_col].astype(str).str.contains('unclear', na=False)) & (df[lat_col].astype(str).str.contains('unclear', na=False)), out_col] = 'UKN_U'\n",
    "        \n",
    "        df.loc[(df[MFCL_col] == 'Multifocal'), out_col] = 'MFCL'\n",
    "        df.loc[(df[lobe_col] == 'TLE') & (df[lat_col] == 'L>R'), out_col] = 'TLE_L'\n",
    "        df.loc[(df[lobe_col] == 'TLE') & (df[lat_col] == 'R>L'), out_col] = 'TLE_R'\n",
    "        df.loc[(df[lobe_col] == 'TLE') & (df[lat_col] == 'BL'), out_col] = 'TLE_BL'\n",
    "    \n",
    "    return df\n",
    "\n",
    "def mergeCols(df):\n",
    "    \"\"\"\n",
    "    Merge columns with similar names (case-insensitive, ignoring whitespace) by prioritizing non-null values.\n",
    "    For each group of similar columns, select the column with the shortest name (or lowercase if tied),\n",
    "    and fill its missing values with values from the other columns in the group (row-wise).\n",
    "    Drops the other columns in the group.\n",
    "\n",
    "    Input:\n",
    "        df: DataFrame with potential duplicate/similar columns\n",
    "\n",
    "    Output:\n",
    "        df: DataFrame with merged columns\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize column names: lower case, strip whitespace\n",
    "    def norm(col):\n",
    "        return re.sub(r'\\s+', '', col).lower()\n",
    "\n",
    "    cols = list(df.columns)\n",
    "    norm_map = {}\n",
    "    for col in cols:\n",
    "        key = norm(col)\n",
    "        norm_map.setdefault(key, []).append(col)\n",
    "\n",
    "    for group in norm_map.values():\n",
    "        if len(group) > 1:\n",
    "            # Choose the column with shortest name, or lowercase if tied\n",
    "            group_sorted = sorted(group, key=lambda x: (len(x), x.lower()))\n",
    "            main_col = group_sorted[0]\n",
    "            other_cols = [c for c in group if c != main_col]\n",
    "            print(f\"[mergeCols] Merging columns: {group} -> '{main_col}'\")\n",
    "            # Fill missing values in main_col from other columns, row-wise\n",
    "            df[main_col] = df[group].bfill(axis=1).iloc[:, 0]\n",
    "            # Drop the other columns\n",
    "            df.drop(columns=other_cols, inplace=True)\n",
    "    return df\n",
    "\n",
    "def stdizeNA(df, missing_patterns=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Replace various missing value patterns in the DataFrame with np.nan.\n",
    "\n",
    "    Parameters:\n",
    "        df: pd.DataFrame\n",
    "        missing_patterns: list or None\n",
    "            List of patterns to treat as missing. If None, uses a default set.\n",
    "        verbose: bool\n",
    "            If True, prints detailed information about changes made.\n",
    "\n",
    "    Returns:\n",
    "        df: pd.DataFrame with standardized missing values\n",
    "    \"\"\"\n",
    "\n",
    "    # Default patterns for missing values\n",
    "    default_patterns = [\n",
    "        '', ' ', 'nan', 'NaN', 'NAN', 'null', 'NULL', '?', 'NA', 'na', 'n/a', 'N/A', '.', '-', '--', 'missing', 'MISSING'\n",
    "    ]\n",
    "    \n",
    "    if missing_patterns is not None:\n",
    "        patterns = set(default_patterns) | set(missing_patterns)\n",
    "    else:\n",
    "        patterns = set(default_patterns)\n",
    "\n",
    "    if verbose:\n",
    "        # Print every value that WOULD be changed (i.e., would be replaced with np.nan)\n",
    "        mask = df.isin(patterns)\n",
    "        changed = mask.stack()[lambda x: x].index.tolist()\n",
    "        if changed:\n",
    "            print(f\"[stdizeNA] The following values will be changed given match to any of: {patterns}\")\n",
    "            for idx, col in changed:\n",
    "                old_val = df.at[idx, col]\n",
    "                print(f\"\\t[MICSId={df.at[idx, 'MICS_ID'] if 'MICS_ID' in df.columns else 'NA'}={df.at[idx, 'PNI_ID'] if 'PNI_ID' in df.columns else 'NA'} study: {df.at[idx, 'study'] if 'study' in df.columns else 'NA'}-ses{df.at[idx, 'SES'] if 'SES' in df.columns else 'NA'}] {{{col[:10]}}}: {old_val} --> NaN\")\n",
    "\n",
    "    # Replace all matching patterns with np.nan\n",
    "    df = df.replace(list(patterns), np.nan)\n",
    "    num_changed = df.isna().sum().sum()\n",
    "    print(f\"[stdizeNA] Standardized {num_changed} missing values\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def carryVals(df, var):\n",
    "    \"\"\"\n",
    "    Carry forward and backward fill demographic variables for each participant.\n",
    "\n",
    "    For each participant (grouped by 'MICS_ID'), if multiple unique non-null values are found for the variable:\n",
    "      - Use the value with the shortest number of characters.\n",
    "      - If there is a tie, use the first value encountered.\n",
    "      - Print a warning indicating all found values and which one was used, including study and session number if available.\n",
    "\n",
    "    Input:\n",
    "        df: DataFrame containing demographic information\n",
    "        var: Column name to carry forward/backward fill\n",
    "\n",
    "    Output:\n",
    "        df: DataFrame with the variable filled per participant\n",
    "    \"\"\"\n",
    "    df[var] = df.groupby('MICS_ID')[var].transform(lambda x: x.ffill().bfill())\n",
    "    # Check for multiple unique values for the same ID\n",
    "    dupes = df.groupby('MICS_ID')[var].nunique()\n",
    "    multi_val_ids = dupes[dupes > 1].index\n",
    "    for mid in multi_val_ids:\n",
    "        vals = df[df['MICS_ID'] == mid][var].dropna().unique()\n",
    "        # Choose value with shortest number of characters, if tie, use first\n",
    "        vals_sorted = sorted(vals, key=lambda v: (len(str(v)), str(v)))\n",
    "        chosen = vals_sorted[0]\n",
    "        pni_id = df[df['MICS_ID'] == mid]['PNI_ID'].iloc[0] if 'PNI_ID' in df.columns else ''\n",
    "        study = df[df['MICS_ID'] == mid]['study'].iloc[0] if 'study' in df.columns else ''\n",
    "        ses = df[df['MICS_ID'] == mid]['SES'].iloc[0] if 'SES' in df.columns else ''\n",
    "        used_idx = list(vals).index(chosen) + 1\n",
    "        print(f\"\\t[carryVals] WARNING: [{mid}={pni_id} Study: {study}-ses{ses}] {var} : {' | '.join(map(str, vals))} --> {chosen}\")\n",
    "    \n",
    "    # Use the chosen value for each group\n",
    "    def choose_val(x):\n",
    "        non_nulls = x.dropna()\n",
    "        if len(non_nulls.unique()) > 1:\n",
    "            vals_sorted = sorted(non_nulls.unique(), key=lambda v: (len(str(v)), str(v)))\n",
    "            return vals_sorted[0]\n",
    "        elif len(non_nulls) > 0:\n",
    "            return non_nulls.iloc[0]\n",
    "        else:\n",
    "            return x\n",
    "    df[var] = df.groupby('MICS_ID')[var].transform(choose_val)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(sheets, save_pth=None, save_name=\"demo\"):\n",
    "    \"\"\"\n",
    "    Run all functions to generate demographic data for 3T-7T participants.\n",
    "\n",
    "    input:\n",
    "        sheets: list of dictionarys with source sheet information (path to sheet, key columns to extract\n",
    "        save_pth: path to save the output demographic file\n",
    "\n",
    "    outputs:\n",
    "        list of IDs with paired 3T-7T data\n",
    "        sheet with each row as separate session and with associated demographic information\n",
    "    \"\"\"\n",
    "\n",
    "    import os\n",
    "    sys.path.append(\"/Users/danielmendelson/Library/CloudStorage/OneDrive-McGillUniversity/Documents/PhD/Boris/code\") # on mac\n",
    "    #sys.path.append(\"/host/verges/tank/data/daniel/\") # on lab computer\n",
    "    from Utils import id, t1\n",
    "\n",
    "    import importlib\n",
    "    importlib.reload(id)\n",
    "    importlib.reload(t1)\n",
    "    \n",
    "    out = id.ids3T7T(sheets, save_pth=None) # determine participants with 3T and 7T data\n",
    "    id_cols = out.columns\n",
    "\n",
    "    out = id.id_visits(sheets, out, save_pth=None) # get all sessions for these participants\n",
    "    print(\"[main] There are \", out['MICS_ID'].nunique(), \" unique participants with a total of \", out.shape[0], \" sessions in input datasheet.\")\n",
    "\n",
    "    out = t1.demo(sheets, out, save_pth=None) # add demographic info\n",
    "\n",
    "    # remove rows with missing date. If only data from one study remains after this removal, then remove participant completely.\n",
    "    n_missing_dates = out['Date'].isna().sum()\n",
    "    if n_missing_dates != 0:\n",
    "        missing_dates = out[out['Date'].isna()]\n",
    "        count_7T = (missing_dates['study'] == '7T').sum()\n",
    "        count_3T = (missing_dates['study'] == '3T').sum()\n",
    "        print(f\"\\n[main] WARNING. {n_missing_dates} rows have missing scan sdates:\\n\\t{count_7T} from 7T\\n\\t{count_3T} from 3T\")\n",
    "        print(\"[main] Removing the following participants (study of the removed row):\")\n",
    "        missing_ids = missing_dates['MICS_ID'].unique()\n",
    "        out = out.dropna(subset=['Date'])\n",
    "        for mid in missing_ids:\n",
    "            # there should be at least one row with each study code\n",
    "            studies = out[out['MICS_ID'] == mid]['study'].unique() # remaining studies for that ID\n",
    "            if len(studies) < len(out['study'].unique()): # if this has ID has fewer unique studies than those present in the data, remove all rows for that ID\n",
    "                print(f\"\\t{mid} (removed study: {studies})\")\n",
    "                out = out[out['MICS_ID'] != mid]\n",
    "        print(\"[main] After cleaning for missing scan dates, there are \", out['MICS_ID'].nunique(), \" unique participants making with a total of \", out.shape[0], \" sessions in input datasheet.\")\n",
    "    \n",
    "    # save out\n",
    "    if save_pth is not None:\n",
    "        import datetime\n",
    "\n",
    "        save_name_tmp = f\"{save_pth}/ids3T7T_{datetime.datetime.now().strftime('%d%b%Y')}.csv\"\n",
    "        toSave = out[id_cols].drop_duplicates()\n",
    "        toSave.to_csv(save_name_tmp, index=False)\n",
    "        print(\"[main] Saved list of paired ids to: \", save_name_tmp)\n",
    "\n",
    "    dob_col = None\n",
    "    for sheet in sheets: # find DOB from sheet dictionary. If multiple sheets have DOB, use the first one found\n",
    "        if 'DOB' in sheet and sheet['DOB'] is not None:\n",
    "            dob_col = sheet['DOB']\n",
    "            #print(f\"[main] Found DOB column in sheet {sheet['NAME']}: {dob_col}\")\n",
    "            break\n",
    "\n",
    "    if dob_col is None:\n",
    "        print(\"[main] WARNING. No DOB column found in any input sheet. Age will not be computed.\")\n",
    "    \n",
    "    out = stdizeNA(out)\n",
    "    out = mergeCols(out)\n",
    "    print(\"[main] Merged columns with the same name. Current columns (sorted): \\n\\t\", sorted(out.columns.tolist(), key=lambda x: x.lower()))\n",
    "\n",
    "    # Fill missing demo variables for each participant, warn if multiple unique values exist\n",
    "    demo_vars = ['dob', 'sex', 'gender', 'handedness', 'ethnicity'] # these demo vars should be key values in the sheet dictionaries\n",
    "    print(\"[main] Filling missing demographic values for variables: \", demo_vars)\n",
    "\n",
    "    for var in demo_vars:\n",
    "        out = carryVals(out, var)\n",
    "\n",
    "    out = t1.dateDif(out, [dob_col, \"Date\"], \"age\", save=False) # compute age\n",
    "\n",
    "    out = group(out, out_col=\"grp\", ID_col=\"MICS_ID\", save_pth=None) # assign high level groups\n",
    "    out = group(out, out_col=\"grp_detailed\", ID_col=\"MICS_ID\", save_pth=None) # assign detailed groups\n",
    "\n",
    "    if save_pth is not None:\n",
    "        import datetime\n",
    "\n",
    "        save_name = f\"{save_pth}/{save_name}_{datetime.datetime.now().strftime('%d%b%Y')}.csv\"\n",
    "        out.to_csv(save_name, index=False)\n",
    "        print(\"[main] Saved to: \", save_name)\n",
    "    else:\n",
    "        print(\"[main] WARNING. Not saving demographics sheet to file. To save, please provide a path to save_pth\")\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Make participant demographics sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = \"/Users/danielmendelson/Library/CloudStorage/OneDrive-McGillUniversity/Documents/PhD/Boris/projects/PT/sources\" # path to directory with source pt sheets\n",
    "\n",
    "# For each sheet, must define NAME, PATH, SHEET, ID_7T, ID_3T. \n",
    "# All other keys are those to be extracted.\n",
    "# The same variables should have the same key names across sheets.\n",
    "\n",
    "PNI = {\n",
    "    'NAME': 'PNI',\n",
    "    'PATH': f'{src_dir}/MICA_PNI_27Aug2025.xlsx', # 7T controls\n",
    "    'SHEET': 'all', # name of sheet in file\n",
    "    'ID_7T': 'ID_PNI', \n",
    "    'ID_3T': 'ID_MICs',\n",
    "    'Ses_7T': 'session',\n",
    "    'Date_7T': 'scanDate',\n",
    "    'study': '7T',\n",
    "    'DOB': 'dob',\n",
    "    'Sex': 'sex',\n",
    "    'Gender': 'gender',\n",
    "    'Hand': 'handedness',\n",
    "    'Eth': 'ethnicity',\n",
    "    'Language': 'language',\n",
    "    'Job': 'employment',\n",
    "    'Edu': 'education',\n",
    "    'LastSz': 'lastSeizure'\n",
    "}\n",
    "\n",
    "MICs = {\n",
    "    'NAME': 'MICs',\n",
    "    'PATH': f'{src_dir}/MICA-MTL-3T_27Aug2025.xlsx', # 3T controls\n",
    "    'SHEET': 'Sheet1', # name of sheet in file\n",
    "    'ID_7T': None, \n",
    "    'ID_3T': 'Study_name',\n",
    "    'Ses_3T': 'Visit',\n",
    "    'Date_3T': 'Scan_Date (D.M.Y)',\n",
    "    'study': '3T',\n",
    "    'Hand': 'Handed', \n",
    "    'Sex': 'AssignedSex',\n",
    "    'Gender': 'GenderIdentity',\n",
    "    'Height': 'HeightApprox',\n",
    "    'Weight': 'WeightApprox',\n",
    "    'Eth': 'Ethnicity',\n",
    "    'Job': 'Employ',\n",
    "    'Edu': 'YoE',\n",
    "    'LastSz': 'Last seizure'\n",
    "}\n",
    "\n",
    "Clin = {\n",
    "    'NAME': 'Clin',\n",
    "    'PATH': f'{src_dir}/Clinical_27Aug2025.xlsx',\n",
    "    'SHEET': 'clinical-database-detailed', # name of sheet in file\n",
    "    'ID_7T': None, \n",
    "    'ID_3T': 'participant_id',\n",
    "    'Date_3T': None,\n",
    "    'Gender': 'Gender',\n",
    "    'Hand': 'Handedness',\n",
    "    'Language': 'Language',\n",
    "    'Job': 'Employment',\n",
    "    'Edu': 'Education',\n",
    "    'EpilepsyDxILAE': 'Epilepsy diagnosis based on ILAE',\n",
    "    'EpilepsyClass': 'Epilepsy classification:Focal,Generalized',\n",
    "    'FocusLat': 'Lateralization of epileptogenic focus',\n",
    "    'FocusConfirmed': 'Epileptogenic focus confirmed by the information of (sEEG/ site of surgical resection/ Ictal EEG abnormalities +/. MRI findings): FLE=forntal lobe epilepsy and cingulate epilepsy, CLE:central/midline epilepsy,ILE: insular epilepsy, mTLE=mesio.temporal lobe epilepsy, nTLE=neocortical lobe epilepsy, PQLE=posterior quadrant lobe epilepsy , multifocal epilepsy,IGE=ideopathic lobe epilepsy,unclear)',\n",
    "    'EMUDischargeDx': 'Dx at EMU discharge ',\n",
    "    'EMUAdmissionDate': 'EMU admission date(dd-mm-yy)',\n",
    "    'AdmissionDuration': 'Duration of admission',\n",
    "    'EpilepsyRiskFactors': 'Risk factors for epilepsy',\n",
    "    'SeizureOnsetYr': 'Seizure onset (yr)',\n",
    "    'DrugResistant': 'Drug resistant epilepsy at time of EMU admission',\n",
    "    'NumASMsPrior': '# of ASMs prior current EMU admission',\n",
    "    'PrevASMs': 'Previous ASMs (name and doses (mg/d)) if applicable prior the current EMU admission',\n",
    "    'NumASMOnAdmission': '# of ASM on admission',\n",
    "    'ASMsOnAdmission': 'ASMs  on admission (name, doses (mg per day)',\n",
    "    'GeneticTest': 'Genetic test (year,results)',\n",
    "    'FDGPET': 'FDG.PET',\n",
    "    'BaselineMRI': 'Baseline MRI (year,results)',\n",
    "    'InvasiveExplorations': 'Invasive explorations (Y/N)',\n",
    "    'NumSurgicalResections': '# of surgical resection/thermocoagulatin',\n",
    "    'SurgicalResectionDateSite': 'Surgical resection date and site',\n",
    "    'Histopathology': 'Histopatholgy',\n",
    "    'Engel6mo': 'Engel classification (seizure outcomes at the 6 month )',\n",
    "    'Engel1yr': 'Engel classification (seizure outcomes after 1 year from surgical resection)',\n",
    "    'ILAEOutcome1yr': 'ILAE outcome after surgical resection by 1 yr',\n",
    "    'NeuromodDevices': 'Neuromodulation devices'\n",
    "    }\n",
    "\n",
    "\n",
    "sheets = [PNI, MICs, Clin]\n",
    "correspSheets = [PNI]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ID_3T7T] Extracting IDs for participants with both 3T and 7T IDs\n",
      "\tLoading: PNI\n",
      "\tSkipping: MICs (missing ID_7T or ID_3T key)\n",
      "\tSkipping: Clin (missing ID_7T or ID_3T key)\n",
      "[id_visits] Extracting visits for IDs in list\n",
      "\tSkipping: Clin (missing Ses_7T and Ses_3T key)\n",
      "\n",
      "\n",
      "\tPNI\n",
      "\t\tExtracting cols: ['ID_MICs', 'ID_PNI', 'scanDate', 'session']\n",
      "\tMICs\n",
      "\t\tExtracting cols: ['Study_name', 'Scan_Date (D.M.Y)', 'Visit']\n",
      "\t\tFinding corresponding ID_7T\n",
      "[main] There are  63  unique participants with a total of  203  sessions in input datasheet.\n",
      "[demo] Retrieving demographics data.\n",
      "\tOverlapping: ['Sex', 'Gender', 'Hand', 'Eth', 'Language', 'Job', 'Edu', 'LastSz', 'Date_3T']\n",
      "\tPNI\n",
      "\t\tmerge_keys: ['ID_7T', 'ID_3T', 'Ses_7T']\n",
      "\tMICs\n",
      "\t\tmerge_keys: ['ID_3T', 'Ses_3T']\n",
      "\tClin\n",
      "\t\tmerge_keys: ['ID_3T']\n",
      "\tRemoving empty columns: ['YoE', 'WeightApprox', 'AssignedSex', 'Handed', 'Employ', 'Scan_Date (D.M.Y)', 'GenderIdentity', 'HeightApprox', 'Ethnicity'] [main] Saved list of paired ids to:  /Users/danielmendelson/Library/CloudStorage/OneDrive-McGillUniversity/Documents/PhD/Boris/projects/3T7T/data/outputs/ids3T7T_27Aug2025.csv\n",
      "[stdizeNA] The following values will be changed given match to any of: {'', 'none', 'N/A', 'n/a', 'NULL', 'NA', 'MISSING', '?', '-', '--', 'missing', 'NAN', ' ', 'NaN', 'null', '.', 'nan', 'None', 'na'}\n",
      "\t[MICSId=PX071=PNE001 study: 7T-ses01] {Previous A}: none --> NaN\n",
      "\t[MICSId=PX071=PNE001 study: 7T-ses01] {Previous A}: none --> NaN\n",
      "\t[MICSId=PX071=PNE001 study: 7T-ses01] {Previous A}: none --> NaN\n",
      "\t[MICSId=PX071=PNE001 study: 7T-ses01] {Previous A}: none --> NaN\n",
      "\t[MICSId=PX176=PNE004 study: 7T-sesa1] {Education}: ? --> NaN\n",
      "\t[MICSId=PX176=PNE004 study: 7T-sesa1] {Employment}: ? --> NaN\n",
      "\t[MICSId=PX158=PNE007 study: 7T-sesa1] {Previous A}: none --> NaN\n",
      "\t[MICSId=PX158=PNE007 study: 7T-sesa2] {Previous A}: none --> NaN\n",
      "\t[MICSId=PX153=PNE009 study: 7T-sesa1] {Risk facto}: none --> NaN\n",
      "\t[MICSId=PX153=PNE009 study: 7T-sesa2] {Risk facto}: none --> NaN\n",
      "\t[MICSId=PX168=PNE010 study: 7T-sesa1] {Previous A}: none --> NaN\n",
      "\t[MICSId=PX168=PNE010 study: 7T-sesa1] {Risk facto}: none --> NaN\n",
      "\t[MICSId=PX189=PNE012 study: 7T-sesa2] {Previous A}: none --> NaN\n",
      "\t[MICSId=PX189=PNE012 study: 7T-sesa1] {Previous A}: none --> NaN\n",
      "\t[MICSId=PX148=PNE013 study: 7T-sesa1] {Previous A}: none --> NaN\n",
      "\t[MICSId=PX200=PNE015 study: 7T-sesa1] {Previous A}: none --> NaN\n",
      "\t[MICSId=PX119=PNE016 study: 7T-sesa1] {ASMs  on a}: none --> NaN\n",
      "\t[MICSId=PX174=PNE017 study: 7T-sesa1] {Risk facto}: none --> NaN\n",
      "\t[MICSId=PX216=PNE021 study: 7T-sesa1] {Risk facto}: none --> NaN\n",
      "\t[MICSId=PX038=PNE023 study: 7T-sesa1] {Previous A}: none --> NaN\n",
      "\t[MICSId=PX175=PNE026 study: 7T-sesa1] {Previous A}: none --> NaN\n",
      "\t[MICSId=PX200=PNE015 study: 7T-sesa2] {Previous A}: none --> NaN\n",
      "\t[MICSId=PX131=PNE040 study: 7T-sesa1] {Education}: missing --> NaN\n",
      "\t[MICSId=PX064=PNE041 study: 7T-sesa1] {Previous A}: none --> NaN\n",
      "\t[MICSId=PX064=PNE041 study: 7T-sesa1] {Risk facto}: none --> NaN\n",
      "\t[MICSId=PX192=PNE042 study: 7T-sesa1] {Previous A}: none --> NaN\n",
      "\t[MICSId=PX192=PNE042 study: 7T-sesa1] {Risk facto}: none --> NaN\n",
      "\t[MICSId=PX038=PNE023 study: 3T-ses01] {Previous A}: none --> NaN\n",
      "\t[MICSId=PX064=PNE041 study: 3T-ses01] {Previous A}: none --> NaN\n",
      "\t[MICSId=PX064=PNE041 study: 3T-ses01] {Risk facto}: none --> NaN\n",
      "\t[MICSId=PX071=PNE001 study: 3T-ses01] {Previous A}: none --> NaN\n",
      "\t[MICSId=PX071=PNE001 study: 3T-ses01] {Previous A}: none --> NaN\n",
      "\t[MICSId=PX071=PNE001 study: 3T-ses02] {Previous A}: none --> NaN\n",
      "\t[MICSId=PX071=PNE001 study: 3T-ses03] {Previous A}: none --> NaN\n",
      "\t[MICSId=PX119=PNE016 study: 3T-ses01] {ASMs  on a}: none --> NaN\n",
      "\t[MICSId=PX071=PNE001 study: 3T-ses04] {Previous A}: none --> NaN\n",
      "\t[MICSId=PX131=PNE040 study: 3T-ses01] {Education}: missing --> NaN\n",
      "\t[MICSId=PX148=PNE013 study: 3T-ses01] {Previous A}: none --> NaN\n",
      "\t[MICSId=PX153=PNE009 study: 3T-ses01] {Risk facto}: none --> NaN\n",
      "\t[MICSId=PX158=PNE007 study: 3T-ses01] {Previous A}: none --> NaN\n",
      "\t[MICSId=PX168=PNE010 study: 3T-ses01] {Previous A}: none --> NaN\n",
      "\t[MICSId=PX168=PNE010 study: 3T-ses01] {Risk facto}: none --> NaN\n",
      "\t[MICSId=PX064=PNE041 study: 3T-ses02] {Previous A}: none --> NaN\n",
      "\t[MICSId=PX064=PNE041 study: 3T-ses02] {Risk facto}: none --> NaN\n",
      "\t[MICSId=PX174=PNE017 study: 3T-ses01] {Risk facto}: none --> NaN\n",
      "\t[MICSId=PX175=PNE026 study: 3T-ses01] {Previous A}: none --> NaN\n",
      "\t[MICSId=PX176=PNE004 study: 3T-ses01] {Education}: ? --> NaN\n",
      "\t[MICSId=PX176=PNE004 study: 3T-ses01] {Employment}: ? --> NaN\n",
      "\t[MICSId=PX189=PNE012 study: 3T-ses01] {Previous A}: none --> NaN\n",
      "\t[MICSId=PX192=PNE042 study: 3T-ses01] {Previous A}: none --> NaN\n",
      "\t[MICSId=PX192=PNE042 study: 3T-ses01] {Risk facto}: none --> NaN\n",
      "\t[MICSId=PX200=PNE015 study: 3T-ses01] {Previous A}: none --> NaN\n",
      "\t[MICSId=PX064=PNE041 study: 3T-ses03] {Previous A}: none --> NaN\n",
      "\t[MICSId=PX064=PNE041 study: 3T-ses03] {Risk facto}: none --> NaN\n",
      "\t[MICSId=PX216=PNE021 study: 3T-ses01] {Risk facto}: none --> NaN\n",
      "\t[MICSId=PX174=PNE017 study: 3T-ses02] {Risk facto}: none --> NaN\n",
      "[stdizeNA] Standardized 4466 missing values\n",
      "[mergeCols] Merging columns: ['handedness', 'Handedness'] -> 'handedness'\n",
      "[mergeCols] Merging columns: ['language', 'Language'] -> 'language'\n",
      "[mergeCols] Merging columns: ['employment', 'Employment'] -> 'employment'\n",
      "[mergeCols] Merging columns: ['gender', 'Gender'] -> 'gender'\n",
      "[mergeCols] Merging columns: ['education', 'Education'] -> 'education'\n",
      "[main] Merged columns with the same name. Current columns (sorted): \n",
      "\t ['# of ASM on admission', '# of ASMs prior current EMU admission', 'ASMs  on admission (name, doses (mg per day)', 'Baseline MRI (year,results)', 'Date', 'dob', 'Drug resistant epilepsy at time of EMU admission', 'Duration of admission', 'Dx at EMU discharge ', 'education', 'employment', 'EMU admission date(dd-mm-yy)', 'Engel classification (seizure outcomes after 1 year from surgical resection)', 'Engel classification (seizure outcomes at the 6 month )', 'Epilepsy classification:Focal,Generalized', 'Epilepsy diagnosis based on ILAE', 'Epileptogenic focus confirmed by the information of (sEEG/ site of surgical resection/ Ictal EEG abnormalities +/. MRI findings): FLE=forntal lobe epilepsy and cingulate epilepsy, CLE:central/midline epilepsy,ILE: insular epilepsy, mTLE=mesio.temporal lobe epilepsy, nTLE=neocortical lobe epilepsy, PQLE=posterior quadrant lobe epilepsy , multifocal epilepsy,IGE=ideopathic lobe epilepsy,unclear)', 'ethnicity', 'FDG.PET', 'gender', 'Genetic test (year,results)', 'handedness', 'Histopatholgy', 'ILAE outcome after surgical resection by 1 yr', 'Invasive explorations (Y/N)', 'language', 'lastSeizure', 'Lateralization of epileptogenic focus', 'MICS_ID', 'Neuromodulation devices', 'PNI_ID', 'Previous ASMs (name and doses (mg/d)) if applicable prior the current EMU admission', 'Risk factors for epilepsy', 'scanDate', 'Seizure onset (yr)', 'SES', 'sex', 'study', 'Surgical resection date and site']\n",
      "[main] Filling missing demographic values for variables:  ['dob', 'sex', 'gender', 'handedness', 'ethnicity']\n",
      "\t[carryVals] WARNING: [HC130=PNC026 Study: 7T-ses01] gender : F | Woman --> F\n",
      "\t[carryVals] WARNING: [HC140=PNC038 Study: 7T-sesa1] gender : Man | M --> M\n",
      "\t[carryVals] WARNING: [HC152=PNC039 Study: 7T-sesa1] gender : man | M --> M\n",
      "\t[carryVals] WARNING: [PX038=PNE023 Study: 7T-sesa1] gender : Man | M --> M\n",
      "\t[carryVals] WARNING: [PX100=PNE022 Study: 7T-sesa1] gender : Man | M --> M\n",
      "\t[carryVals] WARNING: [PX223=PNE025 Study: 7T-sesa1] gender : Woman | F --> F\n",
      "\t[carryVals] WARNING: [PX228=PNE035 Study: 7T-sesa1] gender : Man | M --> M\n",
      "\t[carryVals] WARNING: [PX236=PNE034 Study: 7T-sesa1] gender : Woman | F --> F\n",
      "\t[carryVals] WARNING: [PX144=PNE038 Study: 7T-sesa1] handedness : L | R --> L\n",
      "\t[carryVals] WARNING: [PX183=PNE002 Study: 7T-sesa1] handedness : L | R --> L\n",
      "\t[carryVals] WARNING: [HC082=PNC003 Study: 7T-ses01] ethnicity : Japanese  | Japanese --> Japanese\n",
      "\t[carryVals] WARNING: [HC128=PNC025 Study: 7T-sesa1] ethnicity : Chinese | Chlnese --> Chinese\n",
      "[dateDif] Unparseable values in dob: [nan, nan]\n",
      "[dateDif] Column `age` created.\n",
      "[group] Identifying participant groups\n",
      "\tReturning highlevel grouping to column:  grp\n",
      "[group] Identifying participant groups\n",
      "\tReturning detailed grouping to column:  grp_detailed\n",
      "[main] Saved to:  /Users/danielmendelson/Library/CloudStorage/OneDrive-McGillUniversity/Documents/PhD/Boris/projects/3T7T/data/outputs/demo_27Aug2025.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jz/knj2s5ld08xb9qtkljpdcl9r0000gn/T/ipykernel_78804/708700703.py:225: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[var] = df.groupby('MICS_ID')[var].transform(lambda x: x.ffill().bfill())\n",
      "/var/folders/jz/knj2s5ld08xb9qtkljpdcl9r0000gn/T/ipykernel_78804/708700703.py:225: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[var] = df.groupby('MICS_ID')[var].transform(lambda x: x.ffill().bfill())\n",
      "/var/folders/jz/knj2s5ld08xb9qtkljpdcl9r0000gn/T/ipykernel_78804/708700703.py:225: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[var] = df.groupby('MICS_ID')[var].transform(lambda x: x.ffill().bfill())\n",
      "/var/folders/jz/knj2s5ld08xb9qtkljpdcl9r0000gn/T/ipykernel_78804/708700703.py:225: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[var] = df.groupby('MICS_ID')[var].transform(lambda x: x.ffill().bfill())\n"
     ]
    }
   ],
   "source": [
    "save_pth = \"/Users/danielmendelson/Library/CloudStorage/OneDrive-McGillUniversity/Documents/PhD/Boris/projects/3T7T/data/outputs\"\n",
    "demo = main(sheets, save_pth=save_pth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!! ENSURE THAT GROUPING OCCURED PROPERLY !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MICS_ID</th>\n",
       "      <th>PNI_ID</th>\n",
       "      <th>study</th>\n",
       "      <th>SES</th>\n",
       "      <th>Date</th>\n",
       "      <th>lastSeizure</th>\n",
       "      <th>handedness</th>\n",
       "      <th>dob</th>\n",
       "      <th>language</th>\n",
       "      <th>employment</th>\n",
       "      <th>gender</th>\n",
       "      <th>sex</th>\n",
       "      <th>education</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>scanDate</th>\n",
       "      <th>Genetic test (year,results)</th>\n",
       "      <th>ASMs  on admission (name, doses (mg per day)</th>\n",
       "      <th>Dx at EMU discharge</th>\n",
       "      <th>ILAE outcome after surgical resection by 1 yr</th>\n",
       "      <th>Epileptogenic focus confirmed by the information of (sEEG/ site of surgical resection/ Ictal EEG abnormalities +/. MRI findings): FLE=forntal lobe epilepsy and cingulate epilepsy, CLE:central/midline epilepsy,ILE: insular epilepsy, mTLE=mesio.temporal lobe epilepsy, nTLE=neocortical lobe epilepsy, PQLE=posterior quadrant lobe epilepsy , multifocal epilepsy,IGE=ideopathic lobe epilepsy,unclear)</th>\n",
       "      <th>Previous ASMs (name and doses (mg/d)) if applicable prior the current EMU admission</th>\n",
       "      <th>Seizure onset (yr)</th>\n",
       "      <th># of ASM on admission</th>\n",
       "      <th>Neuromodulation devices</th>\n",
       "      <th>Epilepsy diagnosis based on ILAE</th>\n",
       "      <th>Histopatholgy</th>\n",
       "      <th>Duration of admission</th>\n",
       "      <th># of ASMs prior current EMU admission</th>\n",
       "      <th>Engel classification (seizure outcomes at the 6 month )</th>\n",
       "      <th>Drug resistant epilepsy at time of EMU admission</th>\n",
       "      <th>Surgical resection date and site</th>\n",
       "      <th>Baseline MRI (year,results)</th>\n",
       "      <th>Lateralization of epileptogenic focus</th>\n",
       "      <th>EMU admission date(dd-mm-yy)</th>\n",
       "      <th>Engel classification (seizure outcomes after 1 year from surgical resection)</th>\n",
       "      <th>Invasive explorations (Y/N)</th>\n",
       "      <th>Risk factors for epilepsy</th>\n",
       "      <th>FDG.PET</th>\n",
       "      <th>Epilepsy classification:Focal,Generalized</th>\n",
       "      <th>age</th>\n",
       "      <th>grp</th>\n",
       "      <th>grp_detailed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HC129</td>\n",
       "      <td>Pilot013</td>\n",
       "      <td>7T</td>\n",
       "      <td>05</td>\n",
       "      <td>18.04.2024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>L</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>Full time student</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>Master Studnet</td>\n",
       "      <td>Canadian</td>\n",
       "      <td>18.04.2024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CTRL</td>\n",
       "      <td>CTRL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HC082</td>\n",
       "      <td>PNC003</td>\n",
       "      <td>7T</td>\n",
       "      <td>01</td>\n",
       "      <td>06.05.2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R</td>\n",
       "      <td>17.09.1997</td>\n",
       "      <td>English</td>\n",
       "      <td>Full time student</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>PhD Student</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>06.05.2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.632444</td>\n",
       "      <td>CTRL</td>\n",
       "      <td>CTRL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HC082</td>\n",
       "      <td>PNC003</td>\n",
       "      <td>7T</td>\n",
       "      <td>03</td>\n",
       "      <td>13.03.2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R</td>\n",
       "      <td>17.09.1997</td>\n",
       "      <td>English</td>\n",
       "      <td>Full time student</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>PhD Student</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>13.03.2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.483915</td>\n",
       "      <td>CTRL</td>\n",
       "      <td>CTRL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HC082</td>\n",
       "      <td>PNC003</td>\n",
       "      <td>7T</td>\n",
       "      <td>02</td>\n",
       "      <td>13.06.2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R</td>\n",
       "      <td>17.09.1997</td>\n",
       "      <td>English</td>\n",
       "      <td>Full time student</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>PhD Student</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>13.06.2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.736482</td>\n",
       "      <td>CTRL</td>\n",
       "      <td>CTRL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HC082</td>\n",
       "      <td>PNC003</td>\n",
       "      <td>7T</td>\n",
       "      <td>04</td>\n",
       "      <td>24.10.2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R</td>\n",
       "      <td>17.09.1997</td>\n",
       "      <td>English</td>\n",
       "      <td>Full time student</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>PhD Student</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>24.10.2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.099932</td>\n",
       "      <td>CTRL</td>\n",
       "      <td>CTRL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>PX235</td>\n",
       "      <td>PNE033</td>\n",
       "      <td>3T</td>\n",
       "      <td>01</td>\n",
       "      <td>29.07.2025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R</td>\n",
       "      <td>10.01.1984</td>\n",
       "      <td>en</td>\n",
       "      <td>employee</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>high school diploma</td>\n",
       "      <td>Canadian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>definite</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Focal</td>\n",
       "      <td>41.549624</td>\n",
       "      <td>FLE</td>\n",
       "      <td>FLE_R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>PX236</td>\n",
       "      <td>PNE034</td>\n",
       "      <td>3T</td>\n",
       "      <td>01</td>\n",
       "      <td>31.07.2025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R</td>\n",
       "      <td>12.07.2007</td>\n",
       "      <td>fr</td>\n",
       "      <td>Etudiante au CEGEP (sciences de la nature)</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>Secondaire</td>\n",
       "      <td>Cameroonian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>definite</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Focal</td>\n",
       "      <td>18.053388</td>\n",
       "      <td>PATTERN NOT RECOGNIZED: lobe=nan, lat=nan, MFCL=Focal</td>\n",
       "      <td>PATTERN NOT RECOGNIZED: lobe=nan, lat=nan, MFCL=Focal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>PX240</td>\n",
       "      <td>PNE037</td>\n",
       "      <td>3T</td>\n",
       "      <td>01</td>\n",
       "      <td>12.08.2025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R</td>\n",
       "      <td>11.11.2001</td>\n",
       "      <td>fr</td>\n",
       "      <td>Hospital Cleaner</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>Almost finished CEGEP</td>\n",
       "      <td>Algerian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Followed at the CHUM</td>\n",
       "      <td>Followed at the CHUM</td>\n",
       "      <td>periventricular nodular heterotopia-R fronto/fronto-temporal epilepsy</td>\n",
       "      <td>Followed at the CHUM</td>\n",
       "      <td>unclear</td>\n",
       "      <td>Followed at the CHUM</td>\n",
       "      <td>Followed at the CHUM</td>\n",
       "      <td>Followed at the CHUM</td>\n",
       "      <td>Followed at the CHUM</td>\n",
       "      <td>definite</td>\n",
       "      <td>Followed at the CHUM</td>\n",
       "      <td>Followed at the CHUM</td>\n",
       "      <td>Followed at the CHUM</td>\n",
       "      <td>Followed at the CHUM</td>\n",
       "      <td>Followed at the CHUM</td>\n",
       "      <td>Followed at the CHUM</td>\n",
       "      <td>Followed at the CHUM</td>\n",
       "      <td>unclear</td>\n",
       "      <td>Followed at the CHUM</td>\n",
       "      <td>Followed at the CHUM</td>\n",
       "      <td>Followed at the CHUM</td>\n",
       "      <td>Followed at the CHUM</td>\n",
       "      <td>Followed at the CHUM</td>\n",
       "      <td>Focal</td>\n",
       "      <td>23.750856</td>\n",
       "      <td>UKN</td>\n",
       "      <td>UKN_U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>PX242</td>\n",
       "      <td>PNE039</td>\n",
       "      <td>3T</td>\n",
       "      <td>01</td>\n",
       "      <td>20.08.2025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R</td>\n",
       "      <td>07.09.1947</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Canadian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77.952088</td>\n",
       "      <td>PATTERN NOT RECOGNIZED: lobe=nan, lat=nan, MFCL=nan</td>\n",
       "      <td>PATTERN NOT RECOGNIZED: lobe=nan, lat=nan, MFCL=nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>PX174</td>\n",
       "      <td>PNE017</td>\n",
       "      <td>3T</td>\n",
       "      <td>02</td>\n",
       "      <td>21.08.2025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R</td>\n",
       "      <td>06.12.1993</td>\n",
       "      <td>en</td>\n",
       "      <td>Taxes, Loans stuff</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>Master's from Spain</td>\n",
       "      <td>Mexican</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LCM 350/d</td>\n",
       "      <td>left nocturnal temporal lobe epilepsy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TLE</td>\n",
       "      <td>LEV</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>definite</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8 days</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024:nonlesional</td>\n",
       "      <td>L</td>\n",
       "      <td>21.08.2024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Focal</td>\n",
       "      <td>31.707050</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_L</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>212 rows  42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    MICS_ID    PNI_ID study SES        Date lastSeizure handedness  \\\n",
       "0     HC129  Pilot013    7T  05  18.04.2024         NaN          L   \n",
       "1     HC082    PNC003    7T  01  06.05.2022         NaN          R   \n",
       "2     HC082    PNC003    7T  03  13.03.2023         NaN          R   \n",
       "3     HC082    PNC003    7T  02  13.06.2022         NaN          R   \n",
       "4     HC082    PNC003    7T  04  24.10.2023         NaN          R   \n",
       "..      ...       ...   ...  ..         ...         ...        ...   \n",
       "207   PX235    PNE033    3T  01  29.07.2025         NaN          R   \n",
       "208   PX236    PNE034    3T  01  31.07.2025         NaN          R   \n",
       "209   PX240    PNE037    3T  01  12.08.2025         NaN          R   \n",
       "210   PX242    PNE039    3T  01  20.08.2025         NaN          R   \n",
       "211   PX174    PNE017    3T  02  21.08.2025         NaN          R   \n",
       "\n",
       "            dob language                                  employment gender  \\\n",
       "0           NaN       en                          Full time student       F   \n",
       "1    17.09.1997  English                           Full time student      F   \n",
       "2    17.09.1997  English                           Full time student      F   \n",
       "3    17.09.1997  English                           Full time student      F   \n",
       "4    17.09.1997  English                           Full time student      F   \n",
       "..          ...      ...                                         ...    ...   \n",
       "207  10.01.1984       en                                    employee      F   \n",
       "208  12.07.2007       fr  Etudiante au CEGEP (sciences de la nature)      F   \n",
       "209  11.11.2001       fr                            Hospital Cleaner      F   \n",
       "210  07.09.1947      NaN                                         NaN    NaN   \n",
       "211  06.12.1993       en                          Taxes, Loans stuff      M   \n",
       "\n",
       "    sex              education    ethnicity     scanDate  \\\n",
       "0     F        Master Studnet      Canadian   18.04.2024   \n",
       "1     F           PhD Student      Japanese  06.05.2022    \n",
       "2     F           PhD Student      Japanese   13.03.2023   \n",
       "3     F           PhD Student      Japanese   13.06.2022   \n",
       "4     F           PhD Student      Japanese   24.10.2023   \n",
       "..   ..                    ...          ...          ...   \n",
       "207   F    high school diploma     Canadian          NaN   \n",
       "208   F             Secondaire  Cameroonian          NaN   \n",
       "209   F  Almost finished CEGEP     Algerian          NaN   \n",
       "210   M                    NaN     Canadian          NaN   \n",
       "211   M    Master's from Spain      Mexican          NaN   \n",
       "\n",
       "    Genetic test (year,results) ASMs  on admission (name, doses (mg per day)  \\\n",
       "0                           NaN                                          NaN   \n",
       "1                           NaN                                          NaN   \n",
       "2                           NaN                                          NaN   \n",
       "3                           NaN                                          NaN   \n",
       "4                           NaN                                          NaN   \n",
       "..                          ...                                          ...   \n",
       "207                         NaN                                          NaN   \n",
       "208                         NaN                                          NaN   \n",
       "209        Followed at the CHUM                         Followed at the CHUM   \n",
       "210                         NaN                                          NaN   \n",
       "211                         NaN                                    LCM 350/d   \n",
       "\n",
       "                                                      Dx at EMU discharge   \\\n",
       "0                                                                      NaN   \n",
       "1                                                                      NaN   \n",
       "2                                                                      NaN   \n",
       "3                                                                      NaN   \n",
       "4                                                                      NaN   \n",
       "..                                                                     ...   \n",
       "207                                                                    NaN   \n",
       "208                                                                    NaN   \n",
       "209  periventricular nodular heterotopia-R fronto/fronto-temporal epilepsy   \n",
       "210                                                                    NaN   \n",
       "211                                  left nocturnal temporal lobe epilepsy   \n",
       "\n",
       "    ILAE outcome after surgical resection by 1 yr  \\\n",
       "0                                             NaN   \n",
       "1                                             NaN   \n",
       "2                                             NaN   \n",
       "3                                             NaN   \n",
       "4                                             NaN   \n",
       "..                                            ...   \n",
       "207                                           NaN   \n",
       "208                                           NaN   \n",
       "209                          Followed at the CHUM   \n",
       "210                                           NaN   \n",
       "211                                           NaN   \n",
       "\n",
       "    Epileptogenic focus confirmed by the information of (sEEG/ site of surgical resection/ Ictal EEG abnormalities +/. MRI findings): FLE=forntal lobe epilepsy and cingulate epilepsy, CLE:central/midline epilepsy,ILE: insular epilepsy, mTLE=mesio.temporal lobe epilepsy, nTLE=neocortical lobe epilepsy, PQLE=posterior quadrant lobe epilepsy , multifocal epilepsy,IGE=ideopathic lobe epilepsy,unclear)  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                            NaN   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                            NaN   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                            NaN   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                            NaN   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                            NaN   \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                           ...   \n",
       "207                                                                                                                                                                                                                                                                                                                                                                                                          FLE   \n",
       "208                                                                                                                                                                                                                                                                                                                                                                                                          NaN   \n",
       "209                                                                                                                                                                                                                                                                                                                                                                                                      unclear   \n",
       "210                                                                                                                                                                                                                                                                                                                                                                                                          NaN   \n",
       "211                                                                                                                                                                                                                                                                                                                                                                                                          TLE   \n",
       "\n",
       "    Previous ASMs (name and doses (mg/d)) if applicable prior the current EMU admission  \\\n",
       "0                                                                                   NaN   \n",
       "1                                                                                   NaN   \n",
       "2                                                                                   NaN   \n",
       "3                                                                                   NaN   \n",
       "4                                                                                   NaN   \n",
       "..                                                                                  ...   \n",
       "207                                                                                 NaN   \n",
       "208                                                                                 NaN   \n",
       "209                                                                Followed at the CHUM   \n",
       "210                                                                                 NaN   \n",
       "211                                                                                 LEV   \n",
       "\n",
       "       Seizure onset (yr) # of ASM on admission Neuromodulation devices  \\\n",
       "0                     NaN                   NaN                     NaN   \n",
       "1                     NaN                   NaN                     NaN   \n",
       "2                     NaN                   NaN                     NaN   \n",
       "3                     NaN                   NaN                     NaN   \n",
       "4                     NaN                   NaN                     NaN   \n",
       "..                    ...                   ...                     ...   \n",
       "207                   NaN                   NaN                     NaN   \n",
       "208                   NaN                   NaN                     NaN   \n",
       "209  Followed at the CHUM  Followed at the CHUM    Followed at the CHUM   \n",
       "210                   NaN                   NaN                     NaN   \n",
       "211                    26                     1                     NaN   \n",
       "\n",
       "    Epilepsy diagnosis based on ILAE         Histopatholgy  \\\n",
       "0                                NaN                   NaN   \n",
       "1                                NaN                   NaN   \n",
       "2                                NaN                   NaN   \n",
       "3                                NaN                   NaN   \n",
       "4                                NaN                   NaN   \n",
       "..                               ...                   ...   \n",
       "207                        definite                    NaN   \n",
       "208                        definite                    NaN   \n",
       "209                        definite   Followed at the CHUM   \n",
       "210                              NaN                   NaN   \n",
       "211                        definite                    NaN   \n",
       "\n",
       "    Duration of admission # of ASMs prior current EMU admission  \\\n",
       "0                     NaN                                   NaN   \n",
       "1                     NaN                                   NaN   \n",
       "2                     NaN                                   NaN   \n",
       "3                     NaN                                   NaN   \n",
       "4                     NaN                                   NaN   \n",
       "..                    ...                                   ...   \n",
       "207                   NaN                                   NaN   \n",
       "208                   NaN                                   NaN   \n",
       "209  Followed at the CHUM                  Followed at the CHUM   \n",
       "210                   NaN                                   NaN   \n",
       "211                8 days                                     1   \n",
       "\n",
       "    Engel classification (seizure outcomes at the 6 month )  \\\n",
       "0                                                       NaN   \n",
       "1                                                       NaN   \n",
       "2                                                       NaN   \n",
       "3                                                       NaN   \n",
       "4                                                       NaN   \n",
       "..                                                      ...   \n",
       "207                                                     NaN   \n",
       "208                                                     NaN   \n",
       "209                                    Followed at the CHUM   \n",
       "210                                                     NaN   \n",
       "211                                                     NaN   \n",
       "\n",
       "    Drug resistant epilepsy at time of EMU admission  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "..                                               ...   \n",
       "207                                              NaN   \n",
       "208                                              NaN   \n",
       "209                             Followed at the CHUM   \n",
       "210                                              NaN   \n",
       "211                                                Y   \n",
       "\n",
       "    Surgical resection date and site Baseline MRI (year,results)  \\\n",
       "0                                NaN                         NaN   \n",
       "1                                NaN                         NaN   \n",
       "2                                NaN                         NaN   \n",
       "3                                NaN                         NaN   \n",
       "4                                NaN                         NaN   \n",
       "..                               ...                         ...   \n",
       "207                              NaN                         NaN   \n",
       "208                              NaN                         NaN   \n",
       "209             Followed at the CHUM        Followed at the CHUM   \n",
       "210                              NaN                         NaN   \n",
       "211                              NaN            2024:nonlesional   \n",
       "\n",
       "    Lateralization of epileptogenic focus EMU admission date(dd-mm-yy)  \\\n",
       "0                                     NaN                          NaN   \n",
       "1                                     NaN                          NaN   \n",
       "2                                     NaN                          NaN   \n",
       "3                                     NaN                          NaN   \n",
       "4                                     NaN                          NaN   \n",
       "..                                    ...                          ...   \n",
       "207                                     R                          NaN   \n",
       "208                                   NaN                          NaN   \n",
       "209                               unclear         Followed at the CHUM   \n",
       "210                                   NaN                          NaN   \n",
       "211                                     L                   21.08.2024   \n",
       "\n",
       "    Engel classification (seizure outcomes after 1 year from surgical resection)  \\\n",
       "0                                                                            NaN   \n",
       "1                                                                            NaN   \n",
       "2                                                                            NaN   \n",
       "3                                                                            NaN   \n",
       "4                                                                            NaN   \n",
       "..                                                                           ...   \n",
       "207                                                                          NaN   \n",
       "208                                                                          NaN   \n",
       "209                                                         Followed at the CHUM   \n",
       "210                                                                          NaN   \n",
       "211                                                                          NaN   \n",
       "\n",
       "    Invasive explorations (Y/N) Risk factors for epilepsy  \\\n",
       "0                           NaN                       NaN   \n",
       "1                           NaN                       NaN   \n",
       "2                           NaN                       NaN   \n",
       "3                           NaN                       NaN   \n",
       "4                           NaN                       NaN   \n",
       "..                          ...                       ...   \n",
       "207                         NaN                       NaN   \n",
       "208                         NaN                       NaN   \n",
       "209        Followed at the CHUM      Followed at the CHUM   \n",
       "210                         NaN                       NaN   \n",
       "211                         NaN                       NaN   \n",
       "\n",
       "                  FDG.PET Epilepsy classification:Focal,Generalized  \\\n",
       "0                     NaN                                       NaN   \n",
       "1                     NaN                                       NaN   \n",
       "2                     NaN                                       NaN   \n",
       "3                     NaN                                       NaN   \n",
       "4                     NaN                                       NaN   \n",
       "..                    ...                                       ...   \n",
       "207                   NaN                                     Focal   \n",
       "208                   NaN                                     Focal   \n",
       "209  Followed at the CHUM                                     Focal   \n",
       "210                   NaN                                       NaN   \n",
       "211                   NaN                                     Focal   \n",
       "\n",
       "           age                                                    grp  \\\n",
       "0          NaN                                                   CTRL   \n",
       "1    24.632444                                                   CTRL   \n",
       "2    25.483915                                                   CTRL   \n",
       "3    24.736482                                                   CTRL   \n",
       "4    26.099932                                                   CTRL   \n",
       "..         ...                                                    ...   \n",
       "207  41.549624                                                    FLE   \n",
       "208  18.053388  PATTERN NOT RECOGNIZED: lobe=nan, lat=nan, MFCL=Focal   \n",
       "209  23.750856                                                    UKN   \n",
       "210  77.952088    PATTERN NOT RECOGNIZED: lobe=nan, lat=nan, MFCL=nan   \n",
       "211  31.707050                                                    TLE   \n",
       "\n",
       "                                              grp_detailed  \n",
       "0                                                     CTRL  \n",
       "1                                                     CTRL  \n",
       "2                                                     CTRL  \n",
       "3                                                     CTRL  \n",
       "4                                                     CTRL  \n",
       "..                                                     ...  \n",
       "207                                                  FLE_R  \n",
       "208  PATTERN NOT RECOGNIZED: lobe=nan, lat=nan, MFCL=Focal  \n",
       "209                                                  UKN_U  \n",
       "210    PATTERN NOT RECOGNIZED: lobe=nan, lat=nan, MFCL=nan  \n",
       "211                                                  TLE_L  \n",
       "\n",
       "[212 rows x 42 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo[[\"MICS_ID\", \"PNI_ID\", \"SES\", \"grp\", \"grp_detailed\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All assigned a group\n"
     ]
    }
   ],
   "source": [
    "# print all cases with missing group\n",
    "missing = demo[(demo[\"grp\"] == '') | (demo[\"grp_detailed\"] == '')]\n",
    "if len(missing) > 0:\n",
    "    missing[\n",
    "    [\"MICS_ID\", \"PNI_ID\", \"SES\", \"grp\", \"grp_detailed\", \n",
    "        \"Epileptogenic focus confirmed by the information of (sEEG/ site of surgical resection/ Ictal EEG abnormalities +/. MRI findings): FLE=forntal lobe epilepsy and cingulate epilepsy, CLE:central/midline epilepsy,ILE: insular epilepsy, mTLE=mesio.temporal lobe epilepsy, nTLE=neocortical lobe epilepsy, PQLE=posterior quadrant lobe epilepsy , multifocal epilepsy,IGE=ideopathic lobe epilepsy,unclear)\", \n",
    "        \"Lateralization of epileptogenic focus\"]\n",
    "        ]\n",
    "else:\n",
    "    print(\"All assigned a group\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Get summary statistics by category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grp\n",
       "TLE                                                      19\n",
       "CTRL                                                     16\n",
       "UKN                                                       7\n",
       "FLE                                                       6\n",
       "PATTERN NOT RECOGNIZED: lobe=nan, lat=nan, MFCL=Focal     2\n",
       "MFCL                                                      1\n",
       "PATTERN NOT RECOGNIZED: lobe=nan, lat=nan, MFCL=nan       1\n",
       "PLE                                                       1\n",
       "PQLE                                                      1\n",
       "Name: MICS_ID, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print number per group, median age, number of 3T and 7T sessions in each group\n",
    "#demo['grp'].value_counts()\n",
    "# count unique participants per group\n",
    "demo.groupby('grp')['MICS_ID'].nunique().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grp_detailed\n",
       "CTRL                                                                                       16\n",
       "TLE_R                                                                                       8\n",
       "TLE_L                                                                                       6\n",
       "UKN_U                                                                                       4\n",
       "FLE_L                                                                                       3\n",
       "FLE_R                                                                                       3\n",
       "TLE_BL                                                                                      3\n",
       "PATTERN NOT RECOGNIZED: lobe=nan, lat=nan, MFCL=Focal                                       2\n",
       "TLE_U                                                                                       2\n",
       "UKN_L                                                                                       2\n",
       "MFCL                                                                                        1\n",
       "PATTERN NOT RECOGNIZED: lobe=PLE, lat=Lateralization of epileptogenic focus, MFCL=Focal     1\n",
       "PATTERN NOT RECOGNIZED: lobe=PQLE, lat=R, MFCL=Focal                                        1\n",
       "PATTERN NOT RECOGNIZED: lobe=nan, lat=nan, MFCL=nan                                         1\n",
       "UKN_R                                                                                       1\n",
       "Name: MICS_ID, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.groupby('grp_detailed')['MICS_ID'].nunique().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MICS_ID</th>\n",
       "      <th>PNI_ID</th>\n",
       "      <th>SES</th>\n",
       "      <th>grp</th>\n",
       "      <th>grp_detailed</th>\n",
       "      <th>Lateralization of epileptogenic focus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>PX153</td>\n",
       "      <td>PNE009</td>\n",
       "      <td>a1</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_BL</td>\n",
       "      <td>BL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>PX153</td>\n",
       "      <td>PNE009</td>\n",
       "      <td>a2</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_BL</td>\n",
       "      <td>BL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>PX153</td>\n",
       "      <td>PNE009</td>\n",
       "      <td>01</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_BL</td>\n",
       "      <td>BL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>PX176</td>\n",
       "      <td>PNE004</td>\n",
       "      <td>a1</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_BL</td>\n",
       "      <td>BL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>PX176</td>\n",
       "      <td>PNE004</td>\n",
       "      <td>01</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_BL</td>\n",
       "      <td>BL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>PX200</td>\n",
       "      <td>PNE015</td>\n",
       "      <td>a1</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_BL</td>\n",
       "      <td>BL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>PX200</td>\n",
       "      <td>PNE015</td>\n",
       "      <td>a2</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_BL</td>\n",
       "      <td>BL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>PX200</td>\n",
       "      <td>PNE015</td>\n",
       "      <td>01</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_BL</td>\n",
       "      <td>BL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>PX168</td>\n",
       "      <td>PNE010</td>\n",
       "      <td>a1</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_L</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>PX168</td>\n",
       "      <td>PNE010</td>\n",
       "      <td>01</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_L</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>PX174</td>\n",
       "      <td>PNE017</td>\n",
       "      <td>a1</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_L</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>PX174</td>\n",
       "      <td>PNE017</td>\n",
       "      <td>01</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_L</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>PX174</td>\n",
       "      <td>PNE017</td>\n",
       "      <td>02</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_L</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>PX189</td>\n",
       "      <td>PNE012</td>\n",
       "      <td>a2</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_L</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>PX189</td>\n",
       "      <td>PNE012</td>\n",
       "      <td>a1</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_L</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>PX189</td>\n",
       "      <td>PNE012</td>\n",
       "      <td>01</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_L</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>PX198</td>\n",
       "      <td>PNE018</td>\n",
       "      <td>a1</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_L</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>PX198</td>\n",
       "      <td>PNE018</td>\n",
       "      <td>01</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_L</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>PX204</td>\n",
       "      <td>PNE019</td>\n",
       "      <td>a1</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_L</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>PX204</td>\n",
       "      <td>PNE019</td>\n",
       "      <td>01</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_L</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>PX229</td>\n",
       "      <td>PNE030</td>\n",
       "      <td>a1</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_L</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>PX229</td>\n",
       "      <td>PNE030</td>\n",
       "      <td>01</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_L</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>PX043</td>\n",
       "      <td>PNE031</td>\n",
       "      <td>a1</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_R</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>PX043</td>\n",
       "      <td>PNE031</td>\n",
       "      <td>01</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_R</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>PX043</td>\n",
       "      <td>PNE031</td>\n",
       "      <td>02</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_R</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>PX050</td>\n",
       "      <td>PNE029</td>\n",
       "      <td>a1</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_R</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>PX050</td>\n",
       "      <td>PNE029</td>\n",
       "      <td>01</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_R</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>PX050</td>\n",
       "      <td>PNE029</td>\n",
       "      <td>02</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_R</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>PX050</td>\n",
       "      <td>PNE029</td>\n",
       "      <td>03</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_R</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>PX050</td>\n",
       "      <td>PNE029</td>\n",
       "      <td>04</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_R</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>PX144</td>\n",
       "      <td>PNE038</td>\n",
       "      <td>a1</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_R</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>PX144</td>\n",
       "      <td>PNE038</td>\n",
       "      <td>01</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_R</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>PX144</td>\n",
       "      <td>PNE038</td>\n",
       "      <td>02</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_R</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>PX148</td>\n",
       "      <td>PNE013</td>\n",
       "      <td>a1</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_R</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>PX148</td>\n",
       "      <td>PNE013</td>\n",
       "      <td>01</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_R</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>PX173</td>\n",
       "      <td>PNE006</td>\n",
       "      <td>a1</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_R</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>PX173</td>\n",
       "      <td>PNE006</td>\n",
       "      <td>01</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_R</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>PX216</td>\n",
       "      <td>PNE021</td>\n",
       "      <td>a1</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_R</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>PX216</td>\n",
       "      <td>PNE021</td>\n",
       "      <td>01</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_R</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>PX224</td>\n",
       "      <td>PNE024</td>\n",
       "      <td>a1</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_R</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>PX224</td>\n",
       "      <td>PNE024</td>\n",
       "      <td>01</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_R</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>PX225</td>\n",
       "      <td>PNE025</td>\n",
       "      <td>a1</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_R</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>PX225</td>\n",
       "      <td>PNE025</td>\n",
       "      <td>01</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_R</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>PX071</td>\n",
       "      <td>PNE001</td>\n",
       "      <td>01</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_U</td>\n",
       "      <td>unclear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>PX071</td>\n",
       "      <td>PNE001</td>\n",
       "      <td>02</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_U</td>\n",
       "      <td>unclear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>PX071</td>\n",
       "      <td>PNE001</td>\n",
       "      <td>03</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_U</td>\n",
       "      <td>unclear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>PX071</td>\n",
       "      <td>PNE001</td>\n",
       "      <td>04</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_U</td>\n",
       "      <td>unclear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>PX190</td>\n",
       "      <td>PNE011</td>\n",
       "      <td>a1</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_U</td>\n",
       "      <td>unclear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>PX190</td>\n",
       "      <td>PNE011</td>\n",
       "      <td>01</td>\n",
       "      <td>TLE</td>\n",
       "      <td>TLE_U</td>\n",
       "      <td>unclear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    MICS_ID  PNI_ID SES  grp grp_detailed  \\\n",
       "63    PX153  PNE009  a1  TLE       TLE_BL   \n",
       "64    PX153  PNE009  a2  TLE       TLE_BL   \n",
       "141   PX153  PNE009  01  TLE       TLE_BL   \n",
       "57    PX176  PNE004  a1  TLE       TLE_BL   \n",
       "151   PX176  PNE004  01  TLE       TLE_BL   \n",
       "72    PX200  PNE015  a1  TLE       TLE_BL   \n",
       "94    PX200  PNE015  a2  TLE       TLE_BL   \n",
       "160   PX200  PNE015  01  TLE       TLE_BL   \n",
       "65    PX168  PNE010  a1  TLE        TLE_L   \n",
       "146   PX168  PNE010  01  TLE        TLE_L   \n",
       "74    PX174  PNE017  a1  TLE        TLE_L   \n",
       "149   PX174  PNE017  01  TLE        TLE_L   \n",
       "186   PX174  PNE017  02  TLE        TLE_L   \n",
       "67    PX189  PNE012  a2  TLE        TLE_L   \n",
       "68    PX189  PNE012  a1  TLE        TLE_L   \n",
       "154   PX189  PNE012  01  TLE        TLE_L   \n",
       "75    PX198  PNE018  a1  TLE        TLE_L   \n",
       "158   PX198  PNE018  01  TLE        TLE_L   \n",
       "76    PX204  PNE019  a1  TLE        TLE_L   \n",
       "161   PX204  PNE019  01  TLE        TLE_L   \n",
       "86    PX229  PNE030  a1  TLE        TLE_L   \n",
       "179   PX229  PNE030  01  TLE        TLE_L   \n",
       "87    PX043  PNE031  a1  TLE        TLE_R   \n",
       "105   PX043  PNE031  01  TLE        TLE_R   \n",
       "128   PX043  PNE031  02  TLE        TLE_R   \n",
       "84    PX050  PNE029  a1  TLE        TLE_R   \n",
       "106   PX050  PNE029  01  TLE        TLE_R   \n",
       "109   PX050  PNE029  02  TLE        TLE_R   \n",
       "123   PX050  PNE029  03  TLE        TLE_R   \n",
       "135   PX050  PNE029  04  TLE        TLE_R   \n",
       "95    PX144  PNE038  a1  TLE        TLE_R   \n",
       "138   PX144  PNE038  01  TLE        TLE_R   \n",
       "168   PX144  PNE038  02  TLE        TLE_R   \n",
       "69    PX148  PNE013  a1  TLE        TLE_R   \n",
       "139   PX148  PNE013  01  TLE        TLE_R   \n",
       "59    PX173  PNE006  a1  TLE        TLE_R   \n",
       "148   PX173  PNE006  01  TLE        TLE_R   \n",
       "78    PX216  PNE021  a1  TLE        TLE_R   \n",
       "167   PX216  PNE021  01  TLE        TLE_R   \n",
       "80    PX224  PNE024  a1  TLE        TLE_R   \n",
       "174   PX224  PNE024  01  TLE        TLE_R   \n",
       "81    PX225  PNE025  a1  TLE        TLE_R   \n",
       "175   PX225  PNE025  01  TLE        TLE_R   \n",
       "51    PX071  PNE001  01  TLE        TLE_U   \n",
       "125   PX071  PNE001  02  TLE        TLE_U   \n",
       "126   PX071  PNE001  03  TLE        TLE_U   \n",
       "131   PX071  PNE001  04  TLE        TLE_U   \n",
       "66    PX190  PNE011  a1  TLE        TLE_U   \n",
       "155   PX190  PNE011  01  TLE        TLE_U   \n",
       "\n",
       "    Lateralization of epileptogenic focus  \n",
       "63                                     BL  \n",
       "64                                     BL  \n",
       "141                                    BL  \n",
       "57                                     BL  \n",
       "151                                    BL  \n",
       "72                                     BL  \n",
       "94                                     BL  \n",
       "160                                    BL  \n",
       "65                                      L  \n",
       "146                                     L  \n",
       "74                                      L  \n",
       "149                                     L  \n",
       "186                                     L  \n",
       "67                                      L  \n",
       "68                                      L  \n",
       "154                                     L  \n",
       "75                                      L  \n",
       "158                                     L  \n",
       "76                                      L  \n",
       "161                                     L  \n",
       "86                                      L  \n",
       "179                                     L  \n",
       "87                                      R  \n",
       "105                                     R  \n",
       "128                                     R  \n",
       "84                                      R  \n",
       "106                                     R  \n",
       "109                                     R  \n",
       "123                                     R  \n",
       "135                                     R  \n",
       "95                                      R  \n",
       "138                                     R  \n",
       "168                                     R  \n",
       "69                                      R  \n",
       "139                                     R  \n",
       "59                                      R  \n",
       "148                                     R  \n",
       "78                                  right  \n",
       "167                                 right  \n",
       "80                                      R  \n",
       "174                                     R  \n",
       "81                                      R  \n",
       "175                                     R  \n",
       "51                                unclear  \n",
       "125                               unclear  \n",
       "126                               unclear  \n",
       "131                               unclear  \n",
       "66                               unclear   \n",
       "155                              unclear   "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print all grp=TLE with laterlaization\n",
    "demo[demo['grp'] == 'TLE'][['MICS_ID', 'PNI_ID', 'SES', 'grp', 'grp_detailed', 'Lateralization of epileptogenic focus']].drop_duplicates().sort_values(['grp', 'grp_detailed', 'MICS_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grp</th>\n",
       "      <th>study</th>\n",
       "      <th>SES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td></td>\n",
       "      <td>3T</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td></td>\n",
       "      <td>7T</td>\n",
       "      <td>a1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>CTRL</td>\n",
       "      <td>3T</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>CTRL</td>\n",
       "      <td>3T</td>\n",
       "      <td>02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>CTRL</td>\n",
       "      <td>3T</td>\n",
       "      <td>03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>CTRL</td>\n",
       "      <td>3T</td>\n",
       "      <td>04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CTRL</td>\n",
       "      <td>7T</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CTRL</td>\n",
       "      <td>7T</td>\n",
       "      <td>02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CTRL</td>\n",
       "      <td>7T</td>\n",
       "      <td>03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CTRL</td>\n",
       "      <td>7T</td>\n",
       "      <td>04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CTRL</td>\n",
       "      <td>7T</td>\n",
       "      <td>05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CTRL</td>\n",
       "      <td>7T</td>\n",
       "      <td>a1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>CTRL</td>\n",
       "      <td>7T</td>\n",
       "      <td>a2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>CTRL</td>\n",
       "      <td>7T</td>\n",
       "      <td>a3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>FLE</td>\n",
       "      <td>3T</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>FLE</td>\n",
       "      <td>7T</td>\n",
       "      <td>a1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>FLE</td>\n",
       "      <td>7T</td>\n",
       "      <td>a2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>MFCL</td>\n",
       "      <td>3T</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>MFCL</td>\n",
       "      <td>7T</td>\n",
       "      <td>a1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>PLE</td>\n",
       "      <td>3T</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>PLE</td>\n",
       "      <td>7T</td>\n",
       "      <td>a1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>PQLE</td>\n",
       "      <td>3T</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>PQLE</td>\n",
       "      <td>7T</td>\n",
       "      <td>a1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>TLE</td>\n",
       "      <td>3T</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>TLE</td>\n",
       "      <td>3T</td>\n",
       "      <td>02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>TLE</td>\n",
       "      <td>3T</td>\n",
       "      <td>03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>TLE</td>\n",
       "      <td>3T</td>\n",
       "      <td>04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>TLE</td>\n",
       "      <td>7T</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>TLE</td>\n",
       "      <td>7T</td>\n",
       "      <td>a1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>TLE</td>\n",
       "      <td>7T</td>\n",
       "      <td>a2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>UKN</td>\n",
       "      <td>3T</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>UKN</td>\n",
       "      <td>3T</td>\n",
       "      <td>02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>UKN</td>\n",
       "      <td>7T</td>\n",
       "      <td>a1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>UKN</td>\n",
       "      <td>7T</td>\n",
       "      <td>a2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      grp study SES\n",
       "178          3T  01\n",
       "90           7T  a1\n",
       "101  CTRL    3T  01\n",
       "104  CTRL    3T  02\n",
       "115  CTRL    3T  03\n",
       "170  CTRL    3T  04\n",
       "1    CTRL    7T  01\n",
       "3    CTRL    7T  02\n",
       "2    CTRL    7T  03\n",
       "4    CTRL    7T  04\n",
       "0    CTRL    7T  05\n",
       "14   CTRL    7T  a1\n",
       "33   CTRL    7T  a2\n",
       "36   CTRL    7T  a3\n",
       "130   FLE    3T  01\n",
       "56    FLE    7T  a1\n",
       "61    FLE    7T  a2\n",
       "152  MFCL    3T  01\n",
       "55   MFCL    7T  a1\n",
       "173   PLE    3T  01\n",
       "92    PLE    7T  a1\n",
       "102  PQLE    3T  01\n",
       "79   PQLE    7T  a1\n",
       "105   TLE    3T  01\n",
       "109   TLE    3T  02\n",
       "123   TLE    3T  03\n",
       "131   TLE    3T  04\n",
       "51    TLE    7T  01\n",
       "57    TLE    7T  a1\n",
       "64    TLE    7T  a2\n",
       "140   UKN    3T  01\n",
       "176   UKN    3T  02\n",
       "58    UKN    7T  a1\n",
       "71    UKN    7T  a2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo[['grp', 'study', 'SES']].drop_duplicates().sort_values(['grp', 'study', 'SES'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grp   study\n",
       "CTRL  3T       28.284736\n",
       "      7T       28.861054\n",
       "FLE   3T       33.519507\n",
       "      7T       28.969199\n",
       "MFCL  3T       34.135524\n",
       "      7T       34.214921\n",
       "TLE   3T       30.970568\n",
       "      7T       30.781656\n",
       "UKN   3T       31.735797\n",
       "      7T       31.786448\n",
       "Name: age, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get median age per study in each group\n",
    "demo.groupby(['grp', 'study'])['age'].median().sort_index(level='grp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'grp_highLvl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'grp_highLvl'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m     plt\u001b[38;5;241m.\u001b[39mtight_layout()\n\u001b[1;32m     33\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 35\u001b[0m plot_histogram(demo, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrp_highLvl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAge Distribution by Group\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 22\u001b[0m, in \u001b[0;36mplot_histogram\u001b[0;34m(data, group_col, age_col, title)\u001b[0m\n\u001b[1;32m     19\u001b[0m ages \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(data[age_col], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Overlay histograms for each group\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m groups \u001b[38;5;241m=\u001b[39m data[group_col]\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m grp \u001b[38;5;129;01min\u001b[39;00m groups:\n\u001b[1;32m     24\u001b[0m     subset \u001b[38;5;241m=\u001b[39m data[data[group_col] \u001b[38;5;241m==\u001b[39m grp]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'grp_highLvl'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show histograms for each group\n",
    "\n",
    "def plot_histogram(data, group_col, age_col, title):\n",
    "    \"\"\"\n",
    "    Overlay histograms of age distribution for each group in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): DataFrame containing the data.\n",
    "    group_col (str): Column name for the group classification.\n",
    "    age_col (str): Column name for the age values.\n",
    "    title (str): Title for the plot.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Convert age column to numeric, coerce errors to NaN\n",
    "    ages = pd.to_numeric(data[age_col], errors='coerce')\n",
    "\n",
    "    # Overlay histograms for each group\n",
    "    groups = data[group_col].unique()\n",
    "    for grp in groups:\n",
    "        subset = data[data[group_col] == grp]\n",
    "        age_vals = pd.to_numeric(subset[age_col], errors='coerce').dropna()\n",
    "        plt.hist(age_vals, bins=10, alpha=0.5, label=grp)\n",
    "\n",
    "    plt.xlabel('Age')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_histogram(demo, 'grp_highLvl', 'age', 'Age Distribution by Group')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create table 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_pth = \"/Users/danielmendelson/Library/CloudStorage/OneDrive-McGillUniversity/Documents/PhD/Boris/projects/3T7T/data/demo_23May2025.csv\"\n",
    "df = pd.read_csv(df_pth, dtype=str)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['MICS_ID', 'PNI_ID', 'study', 'SES', 'Date', 'handedness', 'education', 'gender', 'ethnicity', 'employment', 'sex', 'age', 'grp', 'Lateralization of epileptogenic focus', 'Dx at EMU discharge ', 'grp_detailed']\n",
    "df_short = df[columns_to_keep]\n",
    "print(df_short.shape)\n",
    "df_short['age'] = pd.to_numeric(df_short['age'], errors='coerce')\n",
    "df_short.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print rows for HC154\n",
    "df_short[df_short['MICS_ID'] == 'HC154']\n",
    "# remove rows with MICS_ID == 'HC154'\n",
    "df_short = df_short[df_short['MICS_ID'] != 'HC154']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# for repeated rows, values in columns other than Date, age and study should be the same. If NaN then replace with the first non-NaN value in the group\n",
    "df_short = (\n",
    "    df_short.groupby('MICS_ID', group_keys=False)\n",
    "    .apply(lambda x: x.ffill().bfill())\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "df_short = df_short.infer_objects(copy=False)\n",
    "\n",
    "# keep most recent session only (via Date column)\n",
    "df_3T = df_short[df_short['study'] == '3T']\n",
    "df_3T = df_3T.sort_values(by='Date').drop_duplicates(subset='MICS_ID', keep='last')\n",
    "df_3T = df_3T.reset_index(drop=True)\n",
    "df_3T = df_3T.sort_values(by='MICS_ID')\n",
    "\n",
    "\n",
    "df_7T = df_short[df_short['study'] == '7T']\n",
    "df_7T = df_7T.sort_values(by='Date').drop_duplicates(subset='MICS_ID', keep='last')\n",
    "df_7T = df_7T.reset_index(drop=True)\n",
    "df_7T = df_7T.sort_values(by='MICS_ID')\n",
    "\n",
    "print(f\"3T shape: {df_3T.shape}\")\n",
    "print(f\"7T shape: {df_7T.shape}\")\n",
    "print(f\"Unique MICS_IDs in 3T: {df_3T['MICS_ID'].nunique()} (IDs: {df_3T['MICS_ID'].unique()})\")\n",
    "print(f\"Unique MICS_IDs in 7T: {df_7T['MICS_ID'].nunique()} (IDs: {df_7T['MICS_ID'].unique()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_7T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pd.concat([df_3T, df_7T], ignore_index=True)\n",
    "df_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute date difference between 3T and 7T scans by individual\n",
    "df_clean['Date'] = pd.to_datetime(df_clean['Date'], errors='coerce')\n",
    "df_clean['Time between scans (mths)'] = np.nan\n",
    "# for each unique MICS_ID, compute the difference between the 3T and 7T dates. Return this value to both rows\n",
    "for mics_id in df_clean['MICS_ID'].unique():\n",
    "    dates = df_clean[df_clean['MICS_ID'] == mics_id]['Date']\n",
    "    \n",
    "    date_diff_months = abs((dates.max() - dates.min()).days / 30.44)  # average days per month, absolute value\n",
    "    df_clean.loc[df_clean['MICS_ID'] == mics_id, 'Time between scans (mths)'] = date_diff_months\n",
    "\n",
    "df_clean.sort_values(by='MICS_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make histogram of time between scans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_clean['Time between scans (mths)'].dropna(), bins=30, kde=True)\n",
    "plt.title('Time Between 3T and 7T Scans')\n",
    "\n",
    "mean_val = df_clean['Time between scans (mths)'].mean()\n",
    "median_val = df_clean['Time between scans (mths)'].median()\n",
    "\n",
    "plt.axvline(mean_val, color='red', linestyle='--', label=f'Mean: {mean_val:.2f}')\n",
    "plt.axvline(median_val, color='green', linestyle='-.', label=f'Median: {median_val:.2f}')\n",
    "\n",
    "plt.xlabel('Time (months)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_table(df, stat='mdn', grp='grp', age='age', sex='sex', scan_dif='Time between scans (mths)', edu='education', eth='ethnicity', hand='handedness'):\n",
    "    \"\"\"\n",
    "    Create a summary statistics table for the given DataFrame.\n",
    "    \"\"\"\n",
    "    table = pd.DataFrame(columns=df[grp].unique())\n",
    "\n",
    "    grp_values = list(df[grp].unique()) + ['ALL PX']\n",
    "    for grp_value in grp_values:\n",
    "        if grp_value == 'ALL PX':\n",
    "            sub_df = df[df[grp] != 'CTRL'].copy()\n",
    "        else:\n",
    "            sub_df = df[df[grp] == grp_value].copy()\n",
    "\n",
    "        # counts\n",
    "        n_individuals = sub_df['MICS_ID'].nunique()\n",
    "        table.loc['n', grp_value] = n_individuals\n",
    "\n",
    "        # Ensure numeric for stats\n",
    "        sub_df[age] = pd.to_numeric(sub_df[age], errors='coerce')\n",
    "        sub_df[scan_dif] = pd.to_numeric(sub_df[scan_dif], errors='coerce')\n",
    "\n",
    "        # Numerical: age (per study)\n",
    "        for study in sub_df['study'].unique():\n",
    "            study_mask = sub_df['study'] == study\n",
    "            if stat == 'mdn':\n",
    "                age_mdn = sub_df.loc[study_mask, age].median()\n",
    "                age_iqr = sub_df.loc[study_mask, age].quantile(0.75) - sub_df.loc[study_mask, age].quantile(0.25)\n",
    "                row_label = f'age_{study} (mdn (IQR))'\n",
    "                table.loc[row_label, grp_value] = f\"{age_mdn:.1f} ({age_iqr:.1f})\"\n",
    "            elif stat == 'mean':\n",
    "                age_mean = sub_df.loc[study_mask, age].mean()\n",
    "                age_std = sub_df.loc[study_mask, age].std()\n",
    "                row_label = f'age_{study} (mean (std))'\n",
    "                table.loc[row_label, grp_value] = f\"{age_mean:.1f} ({age_std:.0f})\"\n",
    "\n",
    "        # remove repeated rows based on MICS_ID\n",
    "        sub_df = sub_df.drop_duplicates(subset='MICS_ID')\n",
    "\n",
    "        # Numerical: Time between scans (mths)\n",
    "        if stat == 'mdn':\n",
    "            tbs_mdn = sub_df[scan_dif].median()\n",
    "            tbs_iqr = sub_df[scan_dif].quantile(0.75) - sub_df[scan_dif].quantile(0.25)\n",
    "            row_label = 'Time between scans (mths) (mdn (IQR))'\n",
    "            table.loc[row_label, grp_value] = f\"{tbs_mdn:.1f} ({tbs_iqr:.1f})\"\n",
    "        elif stat == 'mean':\n",
    "            tbs_mean = sub_df[scan_dif].mean()\n",
    "            tbs_std = sub_df[scan_dif].std()\n",
    "            row_label = 'Time between scans (mths) (mean (std))'\n",
    "            table.loc[row_label, grp_value] = f\"{tbs_mean:.1f} ({tbs_std:.1f})\"\n",
    "\n",
    "        # Cat: sex\n",
    "        sub_df.loc[:, sex] = sub_df[sex].str.lower().map({'f': 'female', 'm': 'male'})\n",
    "        n_females = (sub_df[sex] == 'female').sum()\n",
    "        n_males = (sub_df[sex] == 'male').sum()\n",
    "        n_total = sub_df[sex].notna().sum()\n",
    "        pct_females = (n_females / n_total * 100) if n_total > 0 else 0\n",
    "        row_label = 'sex (F:M (% F))'\n",
    "        row_label_count = 'sex (F:M (%F))'\n",
    "        table.loc[row_label_count, grp_value] = f\"{n_females}:{n_males} ({pct_females:.0f}%)\"\n",
    "\n",
    "        # Cat: education\n",
    "        edu_counts = sub_df[edu].value_counts(dropna=False)\n",
    "        edu_str = ', '.join([f\"{k}:{v}\" for k, v in edu_counts.items()])\n",
    "        row_label = 'education (count)'\n",
    "        table.loc[row_label, grp_value] = edu_str\n",
    "\n",
    "        # Cat: Handedness\n",
    "        hand_counts = sub_df[hand].value_counts(dropna=False)\n",
    "        n_left = hand_counts.get('L', 0)\n",
    "        n_right = hand_counts.get('R', 0)\n",
    "        hand_str = f\"{n_left}:{n_right}\"\n",
    "        row_label = 'handedness (L:R)'\n",
    "        table.loc[row_label, grp_value] = hand_str\n",
    "\n",
    "        # Cat: lateralization of epileptogenic focus\n",
    "        if grp_value != 'CTRL':\n",
    "            lat_counts = sub_df['Lateralization of epileptogenic focus'].value_counts(dropna=False)\n",
    "            n_tot = lat_counts.sum()\n",
    "            # Report as L:R:unknown\n",
    "            n_left = lat_counts.get('L', 0)\n",
    "            n_left += lat_counts.get('L ', 0)  # Handle trailing space\n",
    "            n_right = lat_counts.get('R', 0)\n",
    "            n_right += lat_counts.get('R ', 0)  # Handle trailing space\n",
    "            n_right += lat_counts.get('right', 0)  # Handle 'right' case\n",
    "            n_left += lat_counts.get('left', 0)  # Handle 'left' case\n",
    "            # Count L>R or R>L and add to L or R accordingly\n",
    "            n_unclear = lat_counts.get('unclear', 0)\n",
    "            n_unclear += lat_counts.get('unclear ', 0)\n",
    "            \n",
    "            print(f\"Tot: {n_tot}, sum: {n_left + n_right + n_unclear}\")\n",
    "            \n",
    "            if n_left + n_right + n_unclear != n_tot:\n",
    "                print(f\"\\t {n_left + n_right + n_unclear} != {n_tot} for {grp_value}\")\n",
    "                n_bl = lat_counts.get('BL', 0)\n",
    "\n",
    "                if n_bl > 0:\n",
    "                    print(\"BL found in Lateralization of epileptogenic focus\")\n",
    "                    print(f\"R, L counts: {n_right}, {n_left}\")\n",
    "                    # Subset rows with 'BL' in 'Lateralization of epileptogenic focus'\n",
    "                    bl_rows = sub_df[sub_df['Lateralization of epileptogenic focus'] == 'BL']\n",
    "                    # Search 'Dx at EMU discharge ' for 'L>' or 'R>' and add to counts\n",
    "                    n_bl_l = bl_rows['Dx at EMU discharge '].astype(str).str.contains('L>').sum()\n",
    "                    n_bl_r = bl_rows['Dx at EMU discharge '].astype(str).str.contains('R>').sum()\n",
    "                    n_left += n_bl_l\n",
    "                    n_right += n_bl_r\n",
    "                    # Count all other BL cases that are not clearly L> or R>\n",
    "                    n_bl_u = n_bl - (n_bl_l + n_bl_r)\n",
    "                    print(f\"R, L counts: {n_right}, {n_left}\")\n",
    "            else:\n",
    "                n_bl_u = 0\n",
    "\n",
    "            lat_str = f\"{n_left}:{n_right}:{n_bl_u}:{n_unclear}\"\n",
    "        \n",
    "        else:\n",
    "            lat_str = ''\n",
    "        \n",
    "        row_label = 'Lateralization (L:R:BL (unclear dominance):unclear)'\n",
    "        table.loc[row_label, grp_value] = lat_str\n",
    "\n",
    "    return table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = make_table(df_clean, stat='mean', grp='grp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', None)\n",
    "t1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
